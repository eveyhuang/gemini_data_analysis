{
    "meeting_annotations": [
        {
            "speaker": "Stephen Fried",
            "timestamp": "00:00-01:19",
            "transcript": "you think that like as an experimentalist who works on protein folding, I would say that we're just really excited. Like we think this is an amazing tool, like we obviously weren't competing with Google to do this and now we're just really glad that we can be beneficiaries of a what I think is a truly revolutionary technology. I think that in terms of the concern, I think that when people say like the protein folding problem, it's actually kind of like a semantic issue where like there really isn't one protein folding problem. I think when Christian Anfinsen kind of first used that phrase, he did in Google's defense, he did actually use it to mean structure prediction. Of course, like what's happened over the last four or five decades is that we now know that protein folding is interesting for many other reasons other than being able to potentially predict structures. And so I think the term has kind of taken on more meanings than it originally had when it was first articulated. So that was just something I kind of wanted to clarify on. But I mean, just to kind of like update a little bit because this is a space that we do a lot of reading and thinking on, they just came out with a preprint two or three days ago to predict complexes as well, like multimers, which apparently works pretty well. So it's pretty remarkable the rate of progress.",
            "speaking duration": 79,
            "interuption": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "other": "The speaker uses frequent and expressive hand gestures to emphasize his points."
        },
        {
            "speaker": "Stephen Yi",
            "timestamp": "01:20-03:52",
            "transcript": "Yeah, just want to follow up on that. That was exactly what I was trying to say because we work on protein-protein interactions and the network for a long time now. So exactly when first came out AlphaFold, the first question is can they really predict protein-protein interaction interfaces? The answer is no. And that's okay. And then when you were talking to people more and more, there are many things that AlphaFold didn't really yet produce and generate for us. Yes, it's a very amazing platform, there are no doubts. So we came up with this paper like almost like 10 years ago. So that was the first time when people predict that the protein interaction network generated by computational prediction can be of even higher quality compared to experimentally generated data set. They have proof of that, they have evidence, they have a lot of things, and it was really high quality and things that a lot of times because of experimental failures, noise, batch effect and all of that. So apparently, of course, you cannot really compare to a carefully designed computational algorithm. So definitely see a huge application and potential that we can benefit from AlphaFold. But at the same time, as Stephen Fried just mentioned now, that a couple of days ago, maybe last month, this new paper came out saying that hey, they were able to modify this algorithm to predict protein-protein interaction complexes. And my student has been working on that to see if we can benefit by integrating with PDB that I know Abhishek has been has mentioned there too, to see if you can somehow use the prediction to broad extend to many different protein interactions and then figure out, I think the key thing that we want to do is to see the mutational effect, how that localized to the protein structure, the interaction interface. AlphaFold is perfect, you know, in that regard. We can we can really leverage that information and then model our network to really generate potential effect of mutations and then maybe also model a patient level where you have a set of five, six driver mutations, combination of them, what will be the effect. I think it's just the beginning of many exciting things.",
            "speaking duration": 152,
            "interuption": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "other": "The speaker is using a virtual background of a university campus."
        },
        {
            "speaker": "Stephen Fried",
            "timestamp": "03:52-04:29",
            "transcript": "So I want to actually ask Stephen you about that because that was something that I was wondering. So like if you have like let's say a protein that you know, there is a mutation that causes a disease and the main way that that mutation causes the disease is by destabilizing the protein to the point that it doesn't properly fold or gets degraded or something to that effect. So am I understanding you properly and what you're saying that you think that if you then typed into AlphaFold, you know, here's the exact same sequence, I'm just going to mutate this one phenylalanine to a valine, that it would then properly predict that the whole thing falls apart?",
            "speaking duration": 37,
            "interuption": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "other": "None"
        },
        {
            "speaker": "Stephen Yi",
            "timestamp": "04:29-06:04",
            "transcript": "Yes, so that's we've been working on that. That's not published, but for the past I would say three to five months now. So you know, exactly because excitement by AlphaFold. So at the beginning, you know, when you first check the mutant and wild type form, so yeah, there's no change, almost the same. But actually, for people who are very familiar with PDB, I think Abhishek may know this more than me, you're able to do generate a score, a quantitative score that compare the sequence similarity and sequence or structural similarity or structural differences. You know, when you rotate the structure from the current position 90 degrees rotating, you say this is different. No, they are the same. So you have to have a way to quantify. So there is a way that we can somehow see that single point mutation can really do generate the structural differences. I think you can look at that more for structural differences, folding and other things. We we just started, but I think there are for sure some interesting things that AlphaFold can offer even in terms of protein folding too. At least we can see some sort of thing on the protein surfaces because we are interested more in interactions. And then you see some sort of structural surface changes obviously. And then by the structural alignment score, you can say yes, it is really a real change.",
            "speaking duration": 95,
            "interuption": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "other": "The speaker smiles and appears very engaged and excited by the question, indicating it is a key area of his research."
        },
        {
            "speaker": "Rigoberto Hernandez",
            "timestamp": "06:04-06:08",
            "transcript": "So Abhishek, you've been called out a couple of times by them. You want to...",
            "speaking duration": 4,
            "interuption": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "other": "The speaker acts as a moderator, inviting another participant into the conversation."
        },
        {
            "speaker": "Abhishek SINGHAROY",
            "timestamp": "06:08-10:00",
            "transcript": "I have very strong feelings on AlphaFold. It's fantastic actually, it's very, very good. So actually, my feeling is a bit different. I would not like to think about the 10 years in future with AlphaFold. I would like to think about the 10 years in future with Google, with what the CEO of Google is thinking might be potentially once he has gotten AlphaFold out. So now if we think in that direction, where did AlphaFold come from? The reason we really like this because we essentially have a statistical mechanics kind of a mindset which really, really kind of gets us into neural networks and and this sort of a multi-sequence alignment. So essentially as a community, we were already highly biased on which this 500 million dollars investment is now being, it's a we are the return on investment. So in a way, that's my way of thinking. Now, let's think from a different side. Where did machine learning the way we are thinking over the last five years come from? The first wave was in 1960s where already Boltzmann motors, not Boltzmann generator, but Boltzmann motors were generated. There's a second wave in 1980s and this is the third wave of machine learning holding the hands of GPUs. So from where I like to think is algorithms biology. There are seven different entry points of machine learning. Anything and everything that we are doing right now is the machine learning of neural networks, brain. How does the brain work? That is one way and what we are trying to do is we are trying to cast all our problems essentially in terms of how our brain thinks. There are six other ways. I've forgotten the names of the six others. One of them is quorum sensing. There are, if you review, there are reviews in algorithms biology. Of these seven ways, three of them doesn't even have mathematical theorems derived, let go of, it hasn't even reached computer science. It's in the applied math level. So I think of machine learning, it's heavily biased by our interpretation of things. It's a virtual machine. So our interpretation is statistical mechanical, which goes very nicely with this neural gas and and so this this is very, very good. And therefore, okay, now now coming down to to AlphaFold, I think therefore the next, if we follow this track, I would invite everyone to look into the six other tracks. But if we follow this track, I think can we can we look into dynamics, which is what I am very, very interested in. In fact, if you go to AlphaFold, they have a similar kind of work on dynamics on the website. They have been only able to train on three particle dynamics. They haven't been able to go beyond that. And not only that, one thing we forget, the training happens on TPUs, not GPUs, very, very highly sophisticated hardware which is not available on any public supercomputer in the country. It's only available with as a part of Google Colab, otherwise not. So I think dynamics would be extremely important to learn. The question is, should we learn dynamics with neural networks, with the way we are thinking about our brain thinks about things? Is that the ansatz to follow? Or should we think of other ways of machine learning altogether? And I think that's where I think we we need to as biologists, we can contribute because we know from from this biological side, how does nature use seven algorithms? If we can treat this problem these seven algorithms that way, then possibly it's going to be easier. I know the reason of doing neural networks, LSTMs, GANs, the different kinds of objects, also because now they are available. There's a lot of implementation, but can we think of it beyond that? I would say that...",
            "speaking duration": 232,
            "interuption": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "other": "The speaker is highly animated, using hand gestures and a wide range of facial expressions to convey his passion and deep knowledge of the topic."
        }
    ]
}