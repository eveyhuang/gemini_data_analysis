{
    "meeting_annotations": [
        {
            "speaker": "Jin Zhang",
            "timestamp": "00:00-00:03",
            "transcript": "and um we want to discuss a little bit more about that.",
            "speaking duration": 3,
            "nods_others": 0,
            "smile_self": 50.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Matthew Lovett-Barron",
            "timestamp": "00:03-00:34",
            "transcript": "Yeah, I put that in. I I agree. I put that into the shared Google Doc because I thought that did seem like a common uh approach. Yeah, so is that does that end up being more of a discussion of how to design um a microscope that integrates its hardware with with uh like a machine learning online approach or or just a user guided approach. I mean, I I I don't know if uh the others have ideas about that.",
            "speaking duration": 31,
            "nods_others": 1,
            "smile_self": 32.25,
            "smile_other": 3.22,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "00:34-01:34",
            "transcript": "I mean, for what I mentioned, I think you do need an automated sort of machine learning guided approach. A user guided approach is usually too time consuming and you know, um low throughput again. Um and so you need something that um, you know, can integrate hardware with like recognition of what the image is telling you to guide the hardware um to the right field of view to the right focal plane. Um and change between objectives, right? Going from low magnification, low NA to high mag, high NA. Um and and then, you know, focusing and zooming into the right spot and changing modalities of imaging.",
            "speaking duration": 60,
            "nods_others": 1,
            "smile_self": 15.0,
            "smile_other": 1.67,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "01:34-01:34",
            "transcript": "I'm back.",
            "speaking duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Arnold Hayer",
            "timestamp": "01:37-02:02",
            "transcript": "I guess the feature detection is something that one could not develop as a universal tool because the features might very context and question be question dependent. So one would have to develop algorithms of being able to detect those rare events or you know, structural arrangements of signals and uh feed that back into into a response of the microscope.",
            "speaking duration": 25,
            "nods_others": 1,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "02:03-02:17",
            "transcript": "Yeah, may be very application dependent, right? But maybe could be something you could train, right? For your specific application.",
            "speaking duration": 14,
            "nods_others": 0,
            "smile_self": 50.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Matthew Lovett-Barron",
            "timestamp": "02:19-02:21",
            "transcript": "Is this is Oh sorry, go ahead please.",
            "speaking duration": 2,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "Yes",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Aseema Mohanty",
            "timestamp": "02:22-02:41",
            "transcript": "Is there any value in um kind of like sparsely randomly checking super high resolution and then, you know, you get a feel for what's going on on the large scale. I mean, I'm not sure for your applications, but that could be an approach as well.",
            "speaking duration": 19,
            "nods_others": 0,
            "smile_self": 78.95,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "02:42-04:14",
            "transcript": "So, I mean, I guess just to give you an idea, right? When we do high resolution imaging because we're using 100x objective, um our field of view is very limited, right? Um, and so the throughput is very low. We image one cell at a time. Um, now with a CMOS camera is again very increasing a bit the field of view and the throughput, um but it's a couple of cells at a time. And that um, you know, limits you in terms of um, you know, if you have rare populations in your sample, will you ever sample them um in your imaging if you're only imaging 10 cells in and again it's a slow imaging modality, you image one cell in every uh, you know, 20 minutes, 15 minutes, um something like that. And so, um, you know, one approach is again like make it automated so that you can image um thousands of cells um automatically. Um, and again randomly sample or maybe a more sort of intelligent approach would be uh maybe there are low resolution features that, you know, um mark those rare populations that you can um find um with a higher throughput approach then you can assume it to those with high resolution uh right rather than just randomly sampling and hoping you will find one of those cells in your in your image. I don't know if that uh answers your question.",
            "speaking duration": 92,
            "nods_others": 0,
            "smile_self": 10.87,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Aseema Mohanty",
            "timestamp": "04:14-04:14",
            "transcript": "Yeah, absolutely.",
            "speaking duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Doug Shepherd",
            "timestamp": "04:18-05:07",
            "transcript": "So, one thing I'm curious about for these applications is we just had the a bit of the label free discussion. And so these tend to be much less phototoxic, but you lack specificity. You don't have molecular labeling. So, how hard is it to know if the event in your case, I guess specifically, I know generalizing is difficult, um that the event would have started occurring if you had a label free measurement that was lower resolution and then you could switch over to doing some sort of super res. I mean that requires integrating across data modalities, but it's it's pretty easy to integrate some of these label free methods in an opposing arm on the microscope. And so then you could try and image that way and then switch over. So, but the question is, will you actually know it's happening with a label free method if you don't have the molecular readout and I don't know if that's the case or not.",
            "speaking duration": 49,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Matthew Lovett-Barron",
            "timestamp": "05:11-05:30",
            "transcript": "Couldn't you in principle just um, I mean if you're training a model to be able to identify these things, you could just collect enough data where you could predict at least with some reasonable degree of accuracy, you could predict something from a label free method based on fluorescence detection as well and then use that to guide further experiments.",
            "speaking duration": 19,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Doug Shepherd",
            "timestamp": "05:30-06:07",
            "transcript": "Yeah, I think I in principle I agree with that, but let's say that you're interested in, you know, transcription factor searching, right? You're never going to know if like a certain transcription factor has been shuttled to the nucleus most of the time from a label free measurement from a stimuli. So I I think there's cases it'll work, there's cases it won't work and I guess this is where I'm trying to figure out kind of, you know, where the value in it is.",
            "speaking duration": 37,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "06:08-07:02",
            "transcript": "I think that may actually um, you know, perhaps this is where we could go into the multi, you know, modality um discussion, right? Uh for a label free based modality uh and um optical and some of the other modalities uh can they be um integrated uh and if so um computationally or experimentally uh do we, you know, have um, you know, sort of a registration or internal reference uh or do we need to have that uh to connect and integrate imaging data from different modalities. And from live cells versus fixed cells as well. Um this is a question.",
            "speaking duration": 54,
            "nods_others": 0,
            "smile_self": 33.33,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Matthew Lovett-Barron",
            "timestamp": "07:05-07:50",
            "transcript": "this is yeah, as I mentioned earlier, this has worked really well for me to move from live tissue where we look at neural activity during behavior to fixed samples where we can do single molecule fish even in different microscopes. It's as long as we can find the same cells, we can register back and forth between the two data sets. And some of that is by virtue of the fact that the larval zebrafish is small and it's easy to get gross level registration and then um the fine cellular level registration isn't such a problem when things are broadly overlapped.",
            "speaking duration": 45,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Arnold Hayer",
            "timestamp": "07:50-08:57",
            "transcript": "So one challenge that we face and uh that is related to using biosensors reporters for subcellular activities in migrating cells is that usually we can't do more than one or two reporters um at the same time. But perhaps you would like to know what five or 10 different things are doing in a specific event. So what we've come up with is is try to make cells behave in a very stereotypic way. For example, move along a a track of extracellular matrix which then has a turn. So you can have a cell that migrates along that track and then it has to turn and then you can especially temporarily analyze what's happening during this specific turning process. And then you have different cells, you you don't use the same cell but you use different cells in a stereotypical behavior um to register the different molecular events uh over time. So that's that's something that, you know, we're we're trying to um establish more and there there certainly with with microfabrication there's there ways of of uh forcing cells into specific behaviors and and helping with that issue. goes back to this multiplexing uh problem of spatial temporal analysis.",
            "speaking duration": 67,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Matthew Lovett-Barron",
            "timestamp": "09:08-09:39",
            "transcript": "Could you uh could you also have something where, you know, you have a bunch of different sensors at once in the same cell, but they may be somewhat broadly distributed and then they are each tagged with a barcode and even if they're all in the same color, then afterwards you could fix the sample and do some kind of uh fixed tissue labeling or multi round fixed tissue labeling to identify based on the barcode what what what sensor it was even though they were all, you know, green at the time or something like that.",
            "speaking duration": 31,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Arnold Hayer",
            "timestamp": "09:39-09:56",
            "transcript": "Okay, okay. Yeah, yeah, I see I see what you mean. So you would have you would multiplex the uh the acquisition and then later basically deconvolve and decide who was who was who in the end. Yeah, that's an interesting idea. We haven't we haven't thought about that yet, but that could could definitely work.",
            "speaking duration": 17,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "09:56-10:01",
            "transcript": "I guess you need to sort of link that that read out to that barcode.",
            "speaking duration": 5,
            "nods_others": 0,
            "smile_self": 40.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        }
    ]
}