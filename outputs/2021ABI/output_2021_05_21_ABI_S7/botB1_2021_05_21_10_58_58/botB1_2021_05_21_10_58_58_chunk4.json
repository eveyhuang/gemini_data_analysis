{
    "meeting_annotations": [
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "00:00-00:31",
            "transcript": "cameras frame rate. And so then the question is like, can you correct for that motion, right? Um, because when you're tracking your protein, you're going to have to um, um, find a way to to subtract the motion of the cilium, uh, from the motion of the protein itself. Um, is that motion of the cilium very stereotypical, like can you sort of characterize and and and correct for it?",
            "speaking duration": 31,
            "nods_others": 0,
            "smile_self": 10,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "00:32-01:13",
            "transcript": "I believe I I can't personally. I think the computational folks can. Um, certainly, I think that they can do it. Um, and but but I don't I can't do it. So that would be that that's a barrier right there is that trying to, you know, I think correct and straighten would be one way to do it so that you could, you know, take that curve waveform, turn it into a linear rod and then correct for it. So that's a that's an interesting idea. Um, so that would definitely be a computational approach. And then with the C stuff, that yeah, I use a a prime 95B scientific C and the frame rates aren't the issue. It's it is definitely getting the the signal for the protein of interest.",
            "speaking duration": 41,
            "nods_others": 0,
            "smile_self": 33,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "01:14-01:19",
            "transcript": "Is there any way to have a non fluorescence contrast agent against some of these proteins of interest?",
            "speaking duration": 5,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "01:20-01:44",
            "transcript": "Good thought. I don't know. That's a good thought. Um, you know, one thing that potentially maybe, I don't know. I don't know much about this quantitative phase imaging. But QPI might be one way to to kill two birds with one stone and that just avoid we wouldn't have molecular specificity, but even tracking one of the granules moving within that structure, maybe that would be a good QPI type approach.",
            "speaking duration": 24,
            "nods_others": 0,
            "smile_self": 25,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Doug Shepherd",
            "timestamp": "01:45-02:58",
            "transcript": "So one issue with a lot of QPIs is it's multiple images. I mean there are ones that aren't, but um, typically you need to introduce some sort of diversity in the phase so then you can extract, you know, what the refracted index was. So you need to look at the image somehow in with multiple views. So this can be pretty low though for certain techniques and so the frame rate can still get pretty high. Um, the you know, one the issue is how much is it moving in 3D in like one sort of time step, right? So that that also so let's say you needed minimum three views, I'm just guessing, you know, you could make some technique, you know, how far is it going to displace between each of those three shots or do you need to come up with some sort of simultaneous multifocal technique, which exists. But you keep every time you do that you split the light, so you're really going to need transmitted light measurements, right where your photon, your excitation photons are doing the work, not your emission photons. Um, so I do think the QPA stuff is is definitely a really promising way to go there, but it does typically require some sort of re computational recombination of multiple views.",
            "speaking duration": 73,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "02:59-03:11",
            "transcript": "No, that makes sense. And so yeah, there there is also it's not a totally planar waveform. There is a three-dimensional rotary aspect to it as well. Um, the the most substantial translation is is planar, but then there is this little twist along with it.",
            "speaking duration": 12,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "03:11-03:22",
            "transcript": "And so the maybe the multifocal approach where I assume then you can use optics to get multiple focal planes simultaneously on the same camera or do you need different cameras?",
            "speaking duration": 11,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Doug Shepherd",
            "timestamp": "03:23-03:57",
            "transcript": "There's there's multiple approaches. Um, the most rigorously optically corrected way would would sort of do it with some tricks in 4A space and then you would get different views on the same camera. You can also do a port a less sophisticated version where you literally just use a couple cameras and displace where the image is being formed out of each of them, but this has some aberration cost. And so there's a couple ways to do this. Um, and so, you know, the simplest one to build is this one where you just displace a couple cheap cameras, you know, so that they're looking at different positions. So,",
            "speaking duration": 34,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "03:58-04:17",
            "transcript": "What about um, uh, I used to do a little bit of light field microscopy where you put a lenslet array, uh, so that you can get kind of multiple views in the same camera frame. But the issue is then it's really computationally expensive to deconvolve into an image and resolution is only so so.",
            "speaking duration": 19,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Doug Shepherd",
            "timestamp": "04:19-04:44",
            "transcript": "I mean, I think those methods are getting a lot better. Um, they're very similar in spirit to the also different views for QPI. So, so either way you're talking about somehow combining something that has a different view of the image to then try and reconstruct it in 3D, right? The nice part about the QPI is it gets the refractive index and that's still a bit tricky to get with a light field setup. So, um, yeah.",
            "speaking duration": 25,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Arnold Hayer",
            "timestamp": "04:49-05:00",
            "transcript": "Can I ask a question about the the transport phenomenon that you're interested in looking at inside the cilia? Is that motor based transport vesicular or free diffusion? That might also be important for for the speed that you need to have in order to pick up the movement.",
            "speaking duration": 11,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "05:05-05:11",
            "transcript": "Yeah, it's microtubial motor based and so it's thought to occur around the micron per second type range.",
            "speaking duration": 6,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "05:11-06:17",
            "transcript": "And it's it's it's kind of fascinating. So cryo EM has actually resolved. So there's two microtubules. There's there's 18 microtubules within the cilium and they're arranged in this really beautiful structure called an axoneme. And what people have figured out with cryo EM with just static snapshots of frozen cilia is that one set of motors walk up the A microtubule and another set of motors walk down the B microtubule. Um, and so that's what we have at the electron microscopy level is that there's actually a highway and you know, the plus end directed ones are going on the A tubule and the minus end directed ones are going down the B tubule.",
            "speaking duration": 66,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "06:17-06:43",
            "transcript": "Can I ask a quick question? Um, so you had mentioned previously that a lot of your issue is um, kind of being photon starved and not being able to get enough light in the end of the day. Is that because of the labeling or is it because of the optical system and somewhere you're throwing out a lot of the light? What would what is the kind of cause?",
            "speaking duration": 26,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "06:44-07:27",
            "transcript": "Yeah, so I mean it's a good I I I don't quite know the answer. My assumption is is that it's a little bit of both. And so like people have only really tried to do this because with with wide field, right? I think it's it's generally like a wide field approach because we do want the most number of photons and we want speed. So camera based wide field analysis is kind of the standard approach. And so, you know, we're we're we could label brighter, we could try to, you know, you know, I guess wide field with deconvolution would probably be the next step. So just to do simple deconvolution would probably be the next step, but beyond that, I I don't know where the photons. I don't know, you know, we can try just going brighter. We're using typical FPs like GFP and and neon, which is pretty bright, but so could be a combination of both.",
            "speaking duration": 43,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "07:27-07:44",
            "transcript": "Maybe I'll add the other factor is you can't just blast it with more light because it's not good for the cell or the, you know, so you have the issues there. It's the tradeoff of illumination without damage, collection of signal very fast, so you're limiting the time that you can capture those photons coming out.",
            "speaking duration": 17,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "07:45-09:52",
            "transcript": "So it's like we almost had a case study, right? Nick's uh um Celia tracking and uh molecular molecule or or granular tracking within the Celia. Uh I wanted to um go back to there's a little bit of a shared theme between uh Arnold uh and Melika's um introduction, right? In both cases, I think um they talked about for example, Arnold talk about um uh temporarily, you know, you do slow imaging and then uh zoom in. You you you know, get something you're interested and you do fast imaging focusing on uh the processes that you really want to study in detail. And Melika talked about spatially, uh maybe lower resolution imaging and then you find something interesting and zoom in do uh super resolution that really high resolution imaging. I found that that shared theme very interesting. Um, do we want to um as a group talk a little bit about that what are the challenges um and and um there's a little bit of a right? uh AI guided uh automatic zooming in uh that's already been done what what's people uh experience?",
            "speaking duration": 127,
            "nods_others": 0,
            "smile_self": 75,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        }
    ]
}