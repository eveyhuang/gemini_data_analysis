{
    "meeting_annotations": [
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "00:00-00:22",
            "transcript": "at this higher threshold value. So I mean I I I I I I think you know these these are valid concerns but at the same time I think they're concerns that can be to some extent um sort of um um softened by, you know, seeing how how robust is your result to the exact analysis or parameters that you're picking in your in your analysis.",
            "speaking duration": 22,
            "nods_others": 2,
            "smile_self": 20,
            "smile_other": 30,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan McCreedy",
            "timestamp": "00:22-01:45",
            "transcript": "So is one thing that I wanted to discuss is is how do we bridge kind of the gap between people that have the qualitative images and people that do that quantitative analysis. You know, is there a solution where you send your images in and you say this is the feature I'm interested in, you know, what data can be derived from those features. Um and then that way it sort of takes out sort of the the subjective sort of input into things. I'm just trying to think because there's been a lot of talk about activation energy and I I'd share that same issue where I think about deep learning and I have no idea where to start. And and so we can do some image analysis. We've repurposed uh tractography in a DTI based method for fluorescent intensity and so we can do tractography on fluorescent based images. So that's kind of one approach we've been able to take multimodality analysis and use it in different ways. But if you were to ask me to do deep learning, I would immediately go to somebody else and say, here are my images, what can you do with them? So how do we how do we sort of bridge that gap, reduce that activation energy while still trying to increase reproducibility.",
            "speaking duration": 83,
            "nods_others": 3,
            "smile_self": 10,
            "smile_other": 10,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "01:45-01:57",
            "transcript": "Are there people who so you know how there are people now who no longer have wet labs and they literally just take data sets from single cell seek and they they do reanalysis and they sort of so there are people who do this for imaging, right?",
            "speaking duration": 12,
            "nods_others": 1,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan McCreedy",
            "timestamp": "01:57-02:01",
            "transcript": "I think Shannon's raising his hand is one of those people.",
            "speaking duration": 4,
            "nods_others": 0,
            "smile_self": 50,
            "smile_other": 50,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "02:01-02:03",
            "transcript": "Okay.",
            "speaking duration": 2,
            "nods_others": 0,
            "smile_self": 100,
            "smile_other": 100,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "02:04-05:16",
            "transcript": "Um so there is in open science a concept of um study pre-registration uh which is mostly known within the context of wet lab bench work where you sort of lay out all the steps of your protocol that and that's what's peer reviewed before you've ever done a single experiment. And this idea is starting to get adopted by the more computational community as well where it's like here are all of the steps that we're going to do before we've ever even done any of it and so not only does it take out researcher degrees of freedom that idea of, you know, parameter scanning um to figure out what works, um but it also in theory improves reproducibility to the point where like literally anybody could else could do this and tell you your results before you've even done it yourself. Um of course the problem with that is incentive uh is is one of incentive where you you as an academic gets get no reward for doing this other than the thrill of having your work be reproducible. Um and so there has been an effort to try to there are some entities that will publish the results of your pre-registration regardless of the outcome assuming that the pre-registration passes peer review. Um and that seems like that might be a pretty effective way at least on the computational end of helping out with reproducibility. Now once you get into things like deep learning models and you know, this kind of probabilistic training procedure, we are still kind of grasping for ideas because now you're talking about a model whose behavior is not entirely deterministic. Um and so simply pressing play um given, you know, a guy a series of steps is not by definition always going to give you the same result. Um and so that's kind of still an outstanding issue.",
            "speaking duration": 192,
            "nods_others": 2,
            "smile_self": 10,
            "smile_other": 10,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "05:17-05:21",
            "transcript": "Yeah, we go back to image J.",
            "speaking duration": 4,
            "nods_others": 0,
            "smile_self": 100,
            "smile_other": 100,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "05:22-05:26",
            "transcript": "Um but I think that's the way you get over the activation energy, right? As you look at the ecosystem that's currently in use and you ask how can I bring quantitative imaging or whatever platform it is that is needed to this workflow.",
            "speaking duration": 4,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "05:26-05:53",
            "transcript": "Absolutely. I think the super resolution field is a good example of this um you know because our data actually not even intensity based. Um it's points in space and um you know we've gone through this like um uh phase where we were um um trying to to convert our images into intensity based images um but there a lot of problems with that and you know like the quantification again was very dependent on how you converted it into an intensity based image etc.",
            "speaking duration": 27,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "05:53-05:55",
            "transcript": "we can go ahead from you. Perfect time to jump in.",
            "speaking duration": 2,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "05:55-05:56",
            "transcript": "Yes, go ahead, Katie.",
            "speaking duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "05:56-06:00",
            "transcript": "Okay, uh so I was going to say um once you have these points.",
            "speaking duration": 4,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Nick Galati",
            "timestamp": "06:00-06:28",
            "transcript": "I I I I think that's a really good idea and and I think with the image J stuff, I think one thing that ends up happening is that there's a few plugins that everybody uses. Like I think the colocalization plugin and image J is very well done. I think a lot of people tend like so that's just an example of one thing that I think some people use. But then there's the proliferation of like tweak plugins that take something that instead of funneling everybody through the same thing, then it starts proliferating laterally and that where that's where like I think a lot of the variability pops up. So I think it would be great if we could kind of funnel people through the most useful plugins and and not necessarily leave them static, still improve them, but then not branch off and go laterally. It seems like that's something that happens a lot.",
            "speaking duration": 28,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "06:28-07:01",
            "transcript": "I I I feel like that touches on this first point of how do we get the most information out of an image or set of images is because it's going to be very application specific, right? And so you're describing this plugin and and at least this is my speculation but the reason for that lateral expansion is because everybody has a slightly different application which requires a slightly a slightly tweaked version of that plugin in order to get the most information they can out of those images. Um and yeah, I I don't have a good answer for that.",
            "speaking duration": 33,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "07:02-07:32",
            "transcript": "Absolutely. I think the super resolution field is a good example of this um you know because our data actually not even intensity based. Um it's points in space and um you know we've gone through this like um uh phase where we were um um trying to to convert our images into intensity based images um but there a lot of problems with that and you know like the quantification again was very dependent on how you converted it into an intensity based image etc.",
            "speaking duration": 30,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "07:32-07:35",
            "transcript": "Katie, we want to hear from you. Perfect time to jump in.",
            "speaking duration": 3,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "07:37-07:37",
            "transcript": "Yes, go ahead, Katie.",
            "speaking duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "07:39-07:54",
            "transcript": "Okay, uh so I was going to say um once you have these points, can you tell us a little bit more about like what the quantitative aspect that follows is or for somebody who's like thresholding, what comes next in the process.",
            "speaking duration": 15,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "07:55-08:53",
            "transcript": "Yeah, all of these are mainly for segmenting things things that are interesting. And so we have been trying as a field to sort of converge and you know we have now good ways like software and some of it is imageJ plugins where you go from the raw data which is your, you know, single molecule data for example to point data, right? So those single molecules are localized and you get your point um patterns. Uh but then going from those point patterns to something biologically meaningful and quantitate is is you know, so like it it's it's a wide open field and we don't have any kind of um, you know, um workflow or combined like algorithm that does everybody still has to write their own Python or whatever MATLAB code for it. Um and so yeah, I feel like there are some fields that are mature that maybe like there's a workflow, you apply it, everybody uses it and it works and then there are other fields where you have to um like tweak it and and and and and make it work for your own application.",
            "speaking duration": 58,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "08:53-09:04",
            "transcript": "Do you think that come just comes down to like standardizing protocols and normalizing data and now all of a sudden the same workflow will work for everybody? Or do you think it's more complicated than that?",
            "speaking duration": 11,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "09:05-09:32",
            "transcript": "I think it's more complicated than that because we're looking things that are very different. Some people are looking at tiny sort of clusters of receptors on the membrane. Other people are looking at complex like uh microtubular arrays and you know that that have very different um, you know, structure. And so how you analyze all that data is dependent on what your data actually looks like.",
            "speaking duration": 27,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        }
    ]
}