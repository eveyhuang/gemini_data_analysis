{
    "meeting_annotations": [
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "00:00-00:25",
            "transcript": "brain last week in our group and the data generated just enormous and how do you quantify that? How do you process it? Um so it's a challenge that everybody faces every day. Um and can you just give us an idea of your thoughts in this area and what you think.",
            "speaking duration": 25,
            "nods_others": 3,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Crystal Rogers (UC Davis)",
            "timestamp": "00:25-00:59",
            "transcript": "I have a lot of really basic thoughts, but I'll start with basically, um there are so many different pipelines for image analysis and currently, we don't have normalized or consistent processes so that image analysis actually ends up so that the data you get is the same across even users in the same lab, much less across labs and I think that that makes it really difficult to compare apples to apples uh when you're doing research to try to quantify things with images.",
            "speaking duration": 34,
            "nods_others": 2,
            "smile_self": 0.09,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn, University of Georgia (he/him)",
            "timestamp": "01:00-01:08",
            "transcript": "When you say normalization, do you mean in terms of the experimental protocols or the computational pre-processing or everything?",
            "speaking duration": 8,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Crystal Rogers (UC Davis)",
            "timestamp": "01:08-01:37",
            "transcript": "Well, okay, so that's that gets into it. Yes, obviously there are also experimental protocols that differ, but even if you hand people the same image, how they process or pre-process that before. So for example, say you're using ImageJ and you're just going to threshold or you're going to filter or you know, something like that could be different um across users and so then you end up getting different data and so it's really hard to then what's real? Like define what's real.",
            "speaking duration": 29,
            "nods_others": 0,
            "smile_self": 0.1,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Ferdinand Schweser, SUNY UB",
            "timestamp": "01:38-02:10",
            "transcript": "I just like to comment that in the MRI world, we distinguish between quantitative imaging and quantitative analysis. So you can do a quantitative analysis on a qualitative image. Right so you just get an anatomical brain scan and you measure the volume of a certain region, you get a quantitative metric but the imaging technique is not quantitative. But you can you can have a thermometry technique that gives you a number for the temperature that would be a quantitative technique.",
            "speaking duration": 32,
            "nods_others": 1,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Crystal Rogers (UC Davis)",
            "timestamp": "02:11-02:26",
            "transcript": "That's a good point. So separating the actual quantitative imaging from quantitative analysis of the imaging, whatever type of imaging you're looking at. That's I think that's something we need to clarify.",
            "speaking duration": 15,
            "nods_others": 0,
            "smile_self": 0.2,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Nick Galati",
            "timestamp": "02:27-03:25",
            "transcript": "Yeah, I think Crystal, I I agree with you. I mean as somebody that was really into ImageJ because I'm not sophisticated enough to use deep learning and that kind of stuff. You know, just having the human be the one that sets the threshold is to me this this this problem that I think is pervasive and like you said, you can give somebody the same image. So I I I I don't know the answer to it. I think it probably involves deep learning or something. But I think that, you know, and I had a discussion about this yesterday with somebody that's in the deep learning field and and their take is that ImageJ is now already obsolete and that all of this is going into the deep learning realm and people that like me that use ImageJ are going to get left behind. And I was sad to hear that, but I think it just seems like it's true. So I I'm very curious then from the deep learning folks, you know, what can be done to lower the activation energy to getting into that so that I don't have to write Python code, which I don't know how to do.",
            "speaking duration": 58,
            "nods_others": 0,
            "smile_self": 0.2,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Crystal Rogers (UC Davis)",
            "timestamp": "03:25-03:25",
            "transcript": "Take.",
            "speaking duration": 0,
            "nods_others": 0,
            "smile_self": 1.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn, University of Georgia (he/him)",
            "timestamp": "03:25-03:45",
            "transcript": "Well, I I would push back on that for exactly the reason that you said. The activation energy is way too high to get into deep learning right now unless you're a deep learning practitioner. And as I have never heard that ImageJ is on the way out. That still seems like kind of the de facto standard for non computer science people.",
            "speaking duration": 20,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan Burnette- Vanderbilt",
            "timestamp": "03:46-03:49",
            "transcript": "Someone mentioned that yesterday too and I just let it slide.",
            "speaking duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn, University of Georgia (he/him)",
            "timestamp": "03:50-03:51",
            "transcript": "Yeah, I don't.",
            "speaking duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan Burnette- Vanderbilt",
            "timestamp": "03:51-03:56",
            "transcript": "I mean we use all kind of tools in my lab and ImageJ is a perfectly fine tool to use for.",
            "speaking duration": 5,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn, University of Georgia (he/him)",
            "timestamp": "03:56-03:56",
            "transcript": "Yeah.",
            "speaking duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Ferdinand Schweser, SUNY UB",
            "timestamp": "03:58-04:03",
            "transcript": "And you can have custom models in ImageJ, so they'll probably even machine learning AI models now that you can just load.",
            "speaking duration": 5,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan Burnette- Vanderbilt",
            "timestamp": "04:03-04:47",
            "transcript": "And and if and if you're writing software like we do to that we want other people to use, if you make it an ImageJ plugin, it's more likely to be used. And so a lot of this like like uh I don't like this um uh imaging snobbery that can pop up very quickly. Uh and it's not just imagers, you know other scientists, you know, biochemists can be snobs too with their techniques. But when it when it comes down to it, you know, a lot of great research is being done with a a 10x objective in the field.",
            "speaking duration": 44,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Nick Galati",
            "timestamp": "04:47-05:07",
            "transcript": "I I I'm I'm glad to hear that because I've got probably tens of thousands of lines of ImageJ macro code that that like my life depends on. And it's not elegant, but I mean it does what it needs to do. But again, like Crystal was saying, somebody else might have a different set of 10,000 lines and that leads into this different answer or subtly different answer and so I I I'd love to hear people chime in.",
            "speaking duration": 20,
            "nods_others": 0,
            "smile_self": 0.2,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan Burnette- Vanderbilt",
            "timestamp": "05:07-05:10",
            "transcript": "So the the first thing is how do you segment out the data?",
            "speaking duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan Burnette- Vanderbilt",
            "timestamp": "05:10-06:10",
            "transcript": "Because that's really the first step of this is is how do you segment out and you don't have to go full AI and be all mathematical about it. Uh there are some very uh easy to use tools like elastic for example, where you can use machine learning so you're guiding the process. And the end result of that is you're guiding the the the computer to make make decisions, but the end result is a standard that and and and we typically use this. We in our lab is like someone threshold holds her images and they give the other person the actual elastic parameters and they can threshold their images exactly the same way. And it's easy because yeah, as I said already pointed out the the threshold has to be low enough so that people can actually access the technology.",
            "speaking duration": 60,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn, University of Georgia (he/him)",
            "timestamp": "06:10-06:10",
            "transcript": "Or if you really want to go crazy, put it into a docker image.",
            "speaking duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Ferdinand Schweser, SUNY UB",
            "timestamp": "06:11-06:27",
            "transcript": "Right, it seems to be more a reporting issue because I mean you you could upload your ImageJ scripts to GitHub and then reference them in your manuscripts or uh like create a video while you while you do it and just publish it somewhere so people could reproduce it exactly the same way.",
            "speaking duration": 16,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn, University of Georgia (he/him)",
            "timestamp": "06:29-07:14",
            "transcript": "Sorry, reproducible open science is a big um interest of mine. So yeah, I could go off on tangents on this, but there are and again, this is kind of where I come from, but there are technological answers to the issue of reproducibility, especially if it comes from a place of kind of tweaking knobs on ImageJ. Um there are a lot of scripting answers to that in particular, yeah, let's build out this docker image where I can just push a button and get the exact same results that you did. Um but that is obviously not an answer for everybody citing the aforementioned activation energy.",
            "speaking duration": 45,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "07:14-07:41",
            "transcript": "So is is there really a challenging problem then because imaging is data as you know. Um and we see a lot of beautiful pictures every day. But the question is what do they really mean? And have you faced similar challenges and how do you deal with it and are there opportunities for us to solve big problems in that area.",
            "speaking duration": 27,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Crystal Rogers (UC Davis)",
            "timestamp": "07:46-08:37",
            "transcript": "Feeling very stressed about that question because it goes back to the original question which is how did you do the experiment? What are you looking at and what are you using to capture these data because we are all doing different things and I'm realizing that across cell types, across systems, across, you know, mechanisms, like if you're using immunohistochemistry versus um you know, immunofluorescence versus a reporter, you're going to get a different answer even using the exact same system and processing the images the same way even if you're looking at the same protein and that's very stressful because then you get back to this issue of transparency and if I repeat your experiment with a slightly different method and then I get a different answer and the the numbers I get out of it are different then like what are we even doing here?",
            "speaking duration": 51,
            "nods_others": 0,
            "smile_self": 0.1,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Mimi Sammarco, Tulane",
            "timestamp": "08:38-10:00",
            "transcript": "One thing I thought like you and I probably do the same thing when you're looking at IHC or you know, histochemistry right in a two-dimensional space and so you're going for depth through say like in my case it's going to be a limb, right? And if we took two limbs off the same animal and I hit it more lateral and you more medial, right? That signal can be very different. Independent of the fact that I might normalize against Daby or like cell count and you might normalize against area. But it seems like in that sort of something that would be predictive because we've tried you don't want to sacrifice the entire sample but to get a sampling across it because I'm sitting in these groups and trying to discuss like three-dimensional anything deeper than like, you know, 20 micrometers basically which is essentially a section becomes very difficult.",
            "speaking duration": 82,
            "nods_others": 0,
            "smile_self": 0.1,
            "smile_other": 0.0,
            "distracted_others": 1,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        }
    ]
}