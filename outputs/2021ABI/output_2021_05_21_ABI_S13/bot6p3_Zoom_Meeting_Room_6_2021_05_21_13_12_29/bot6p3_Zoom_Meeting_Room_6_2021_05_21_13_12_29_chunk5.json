{
    "meeting_annotations": [
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "00:00-00:42",
            "transcript": "your image, right? So, for example, in the super resolution field, one thing we do with the point pattern analysis is use density based clustering or, you know, things that we borrowed from other fields. Um, Voronoi, um, tessellation, uh, to make Voronoi polygons around the points and say, okay, small polygons correspond to dense regions that actually have signal. And so I'm going to zoom into those regions, um, and and segment them out of background, um, information and then maybe interrogate their size, um, you know, um, do they form domains? How big are those domains, etc.",
            "speaking duration": 42,
            "nods_others": 2,
            "smile_self": 10,
            "smile_other": 30,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "00:43-01:07",
            "transcript": "And would you say at this point that um like it's a physics informed or biology informed or uh like is there a model that's been proposed that you're applying and testing or um are these sort of um I would say like more data analysis, like looking for the the patterns. Um.",
            "speaking duration": 24,
            "nods_others": 1,
            "smile_self": 12,
            "smile_other": 25,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "01:07-01:19",
            "transcript": "It's more the latter looking for the patterns in the data, right? And um trying to sort of make a biological sort of um interpretation of those patterns.",
            "speaking duration": 12,
            "nods_others": 1,
            "smile_self": 33,
            "smile_other": 33,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "01:19-01:46",
            "transcript": "that is is the problem there in terms of reproducibility really the analysis of the final image or the acquisition of the image? Because yeah, you can set a slightly different threshold but you would probably still detect if something goes up or goes down, which is usually enough for the hypothesis.",
            "speaking duration": 27,
            "nods_others": 1,
            "smile_self": 15,
            "smile_other": 30,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "01:46-01:55",
            "transcript": "I mean for confirming your hypothesis that whatever adding some chemical reduces the number of cells or what I mean you don't need to be quantitative there so much. a little bit but if you're 50% off compared to another lab it probably doesn't matter much in most applications.",
            "speaking duration": 9,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "01:56-02:34",
            "transcript": "Yeah, I mean, reproducibility is more, I mean, I, you know, I don't mean to sort of go back to the topic of reproducibility, but it was more like, you know, these are not, like, I can't just apply the image processing tools that exist for, you know, confocal images to my data. It just doesn't work because the data is not the same. So, um, the field has to sort of come up with new ideas about how do you, how do you extract information from this point data, um, that is not like pixel and intensity based.",
            "speaking duration": 38,
            "nods_others": 0,
            "smile_self": 13,
            "smile_other": 13,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "02:43-03:47",
            "transcript": "All right. That that sounds really exciting discussion about the how we can at least be able to um segment our data is critical steps and and how it can be retrieved down the line in terms of quantification. But then there is this huge problem of looking at different systems that different people are using. Um, and then you want to you have different special scales that you still want to be able to merge together. Um, do you see any challenges in that form of um collecting data at different time points with different systems, yet you want to match them into um a system that data set that's interpretable by different groups.",
            "speaking duration": 64,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "03:48-04:29",
            "transcript": "Oh yeah, there was this comment that somebody yesterday had said that machine learning will take the place of image J. Uh, and I disagree with that. But when I read this question about like stitching and matching things, I do think there's some opportunities there. Um, for machine learning, uh, just to uh do feature extraction for comparison across some of these. Um, but I'm curious, I mean that that's kind of what we would turn to in our group if we were going to solve this, but I'm curious if other people have ideas or strategies where they would start if they want to cross modalities.",
            "speaking duration": 41,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "04:29-05:55",
            "transcript": "I don't know about crossing modalities, but to implement machine learning, um, my student is actively training different softwares to mirror the the in person like they're doing I have two groups counting or quantifying the same images, one teaching the machine like these different softwares elastic and then whatever else he's using. I don't know. I'm sorry. Cell profiler, some other things. Um, and then you have the students manually doing it using image day or other methods and I think that first, it's really necessary to sort of every lab would have to normalize these data within the lab within the same sample before crossing modalities. So I think that that's one of the the hard parts is that this um activation energy is really high even within a single lab to change mechanisms of quantitation and and and quantitative analysis and then to then be able to translate that. So if I'm looking at so this is actually something had had brought up is looking at um going from 2D to 3D, right? And and trying to figure out how you would take information from our 2D system and then sort of thinking about that within 3D and then how would you quantify that and analyze that. Um, even though you're looking at the same cells, same markers, whatever it is, that in and of itself is a really difficult thing to do.",
            "speaking duration": 86,
            "nods_others": 0,
            "smile_self": 10,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "06:15-07:24",
            "transcript": "I have seen some really interesting but probably not science ready machine learning work where they're able to you're able to train a model that quite literally maps your image data from one modality to another. So like if your image data is recorded under fluorescence, here's what it would look like under differential image contrast interference contrast or you know, some other transformation. Um, and even though it's not anywhere near perfect, um, it does provide an interesting way forward at least in terms of how you would stitch together different modalities.",
            "speaking duration": 69,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan McCreedy",
            "timestamp": "07:25-08:01",
            "transcript": "Yeah, I've seen uh labs that are collaborating with Google and they're taking fluorescent images of cells and then taking phase contrast and they've gotten to a point now where they can just give it phase contrast and it will predict the fluorescent labeling. They can then do the fluorescent labeling and compare them and there's a high fidelity between the two. And so it is yeah, I mean even then though you're you're still in an optical, you know, you're you're fairly similar modalities, but I do think it could extend beyond that as long as you have points of registration or some common map that you can put everything on.",
            "speaking duration": 36,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "08:02-08:29",
            "transcript": "Yeah, right now you would at least need a baseline of here's my I recorded this same data under these two different modalities and I'm going to train this model to learn how to tell the difference between them. Um, part of previous discussions is how we could get to that point without needing to generate those whole data sets on your own.",
            "speaking duration": 27,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "08:30-09:59",
            "transcript": "we still we still don't have a a a way of taking the same modality at different magnifications and comparing those data sets. So switching from DIC or phase to fluorescence and back. Now there are some limitations in there that I can go on for a while because we've been thinking about this for a while too and we're interested in in nanoscale kind of protein protein interactions at this point and phase isn't going to do that for us. Uh so there are there are some limitations but even if you are doing a a low mag image of the same of of obvious structure like say mitochondria or the ER at different magnifications, you have to use different algorithms to quantify those. So not just switching from modalities but the actual magnifications of the same modality are not standardized even in the same lab. So if if if if we're switching to a new modality before we fix the the magnification problem in the same modality. I think Crystal really laid out well that if you have the exact same equipment, you should be able to reproduce the data. But none of us have the exact same equipment. And so that only applies if I have the exact same camera you have. And then you tell me exactly what you did and I could probably get close to reproducing it. But I bought the camera that was the cheapest that did the what I wanted it to do. So I don't know some of you probably have my cheap camera too, but most of you have better cameras um for for for imaging and so we have this magnification problem before before we switch modalities. magnification is key.",
            "speaking duration": 89,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        }
    ]
}