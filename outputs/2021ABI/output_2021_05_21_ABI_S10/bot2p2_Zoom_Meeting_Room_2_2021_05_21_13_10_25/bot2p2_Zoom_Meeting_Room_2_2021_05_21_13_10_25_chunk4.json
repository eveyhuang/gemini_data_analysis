{
    "meeting_annotations": [
        {
            "speaker": "Vivian Qian Liu_McGill",
            "timestamp": "00:00-00:20",
            "transcript": "So what I think the so the challenge here is in a in a cloud data set, we don't know uh we don't know which spot which dots are related to each other. So we don't have an algorithm to really section them.",
            "speaking duration": 20,
            "nods_others": 0,
            "smile_self": 10,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "00:20-00:27",
            "transcript": "So so you need that at at a lower resolution to guide you to the specific areas?",
            "speaking duration": 7,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Raising Hand",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Vivian Qian Liu_McGill",
            "timestamp": "00:27-01:28",
            "transcript": "Sort of. So we have have we have to have some previous experience, either it's a lower resolution imaging or either you know the biology. Either you know that these two proteins somehow interacted. So we don't we can't directly see it only by looking at those uh dots on the imaging or or the um so so that that brings me to another challenge. We have so many dots, we can't like with human eye, it is very easy to tell these two are clustering together. like these two species of proteins are related. But with the computer, when I wanted to generate uh get more data from it, like see uh how close they are, like how um how how and the the uh how much they interact. It's very difficult to have the computer do that for me.",
            "speaking duration": 61,
            "nods_others": 0,
            "smile_self": 10,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "01:29-01:49",
            "transcript": "Well, that's where you're asking how can we get the most information out of an image or set of image if you're not quite sure where to go within that image. So you need some kind of lower resolution direction to the right spot on your high resolution to get the most information out.",
            "speaking duration": 20,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Vivian Qian Liu_McGill",
            "timestamp": "01:50-02:06",
            "transcript": "Yes, it's kind of mask first and then you like focus on this spot. But like the the the the the details and then you'll look uh you look for more information. So that was that was the big challenge.",
            "speaking duration": 16,
            "nods_others": 0,
            "smile_self": 31,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "02:06-02:32",
            "transcript": "So once we hear from um San Pun, I'm gonna and I think I've heard from everyone else, we should let this go to try to um satisfy these three bullets, which it seems like we've we've looked at them differently because of our different fields, which makes sense.",
            "speaking duration": 26,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "02:32-02:32",
            "transcript": "So?",
            "speaking duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Sapun Parekh (UT Austin)",
            "timestamp": "02:32-02:35",
            "transcript": "I think yeah, I think we're discussing those two questions.",
            "speaking duration": 3,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Sapun Parekh (UT Austin)",
            "timestamp": "02:35-03:41",
            "transcript": "But I think the at least for where I come from, I mean, I think the the major bottlenecks are I think very much what what Dave said, which is data sharing and being able to like if I get some data and then one of you get some data and somebody else gets some data and we did the exact same sample, do we get the same answer? And I think that's a pretty big bottleneck for quantitative imaging because I got to be honest and I think the answer is a resounding no for 90% of the cases. And I think that that's kind of sad, but like that's that's the truth, right? Like somebody has a little special trick there or a little trick there that they process it with and it's not always available. And I find that that that limits a lot of progress because it doesn't allow us to really standardize things across different institutions or instruments or setups and so I've seen some databases go up that try to provide that information. I've participated in some of them as well to just give away everything, all the metadata, all the code, everything you have to try to make it transferable, but I I don't know to me that's the bottleneck to doing really quantitative imaging.",
            "speaking duration": 66,
            "nods_others": 0,
            "smile_self": 11,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "03:41-03:56",
            "transcript": "Right. And for and for what um uh Matt Lou wrote down here would need also uh the normalization and um actually across multiple stages. So Vivian, you had your hand up? Did you want to comment on that?",
            "speaking duration": 15,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Vivian Qian Liu_McGill",
            "timestamp": "03:57-04:55",
            "transcript": "Yes. Um so that that's also another challenge I have. One is validation, like I got I got my algorithm. I analyze my data and I got this result and I can't compare it to other people. And also because right now, um for a lot of the images like processing uh at least uh my collaborator and me, we're still kind of using human eyes and or or like previous experience to guide the development of algorithm. But the previous experience may be not true. Uh so that's why we ask for AI or computation to do that without without any uh what is it? Um judgment. So without previous judgment, that way we get more um uh we get we get uh unbiased.",
            "speaking duration": 58,
            "nods_others": 0,
            "smile_self": 10,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Gokul Upadhyayula (UC Berkeley)",
            "timestamp": "04:55-04:55",
            "transcript": "Unbiased.",
            "speaking duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Vivian Qian Liu_McGill",
            "timestamp": "04:55-04:58",
            "transcript": "Yeah, yeah. Unbiased unbiased interpretation.",
            "speaking duration": 3,
            "nods_others": 0,
            "smile_self": 66,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "David Van Valen - Caltech",
            "timestamp": "05:00-05:03",
            "transcript": "Yeah, if I could if I could add.",
            "speaking duration": 3,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "05:03-05:03",
            "transcript": "Okay, David and then Fanny.",
            "speaking duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "David Van Valen - Caltech",
            "timestamp": "05:03-06:01",
            "transcript": "Yeah, if I could add a thought that was um sparked by what Vivian had to say, which is uh people don't really understand like the full value of reference data sets. Um so when you're generating algorithms, you have no way of knowing what's an actual advance um and what isn't without a reference data set to benchmark things on. Um, you know, we just experienced this uh in the last like couple in the last like year or so. Um we've put together a reference data set for doing whole cell and nuclear segmentation human tissues.",
            "speaking duration": 58,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Sapun Parekh (UT Austin)",
            "timestamp": "06:01-06:14",
            "transcript": "Yeah, we put out a data set very similar to that in the last year doing exactly that segmenting nuclei and and cells for people who want to develop algorithms because we are not good at that to actually try to do that.",
            "speaking duration": 13,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "06:15-06:16",
            "transcript": "Okay, so I'm going to go to Fanny.",
            "speaking duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "06:16-06:24",
            "transcript": "I just want to point out um from these comments being um made, I think that's great that the research groups are putting those data sets out there.",
            "speaking duration": 8,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "06:24-06:27",
            "transcript": "Um and I am in one way.",
            "speaking duration": 3,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "06:27-07:07",
            "transcript": "medical imaging is extremely organized because people use DICOM and we can get images from around the world and there's some it's part of it is very but people don't share as well. It seems here with the microscopy folks that are it seems that um you have data sets and you're trying to share them but the format from one institution, not even, you know, even if you go before the staining before the scanning, you've got the actual just good old fashioned file format isn't unified. So it's like we got to take these two fields and put them together. But Fanny, you've been wait waiting patiently.",
            "speaking duration": 40,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Fanny Chapelin, UKentucky",
            "timestamp": "07:09-07:51",
            "transcript": "Sure. Thanks. Um it's a bit of a sort of crazy idea but um and I'm not really a specialist in computers and algorithms at all. But I'm just picturing when sometimes we analyze data on just image J and they have those, you know, window levels where you can go from zero to 255 and intensity. I'm wondering if on the computational side, it's possible to maybe sort of develop something where you can reformat every single image acquired worldwide onto this fixed scale somehow so that we could really compare one to the other.",
            "speaking duration": 42,
            "nods_others": 0,
            "smile_self": 14,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Fanny Chapelin, UKentucky",
            "timestamp": "07:51-07:52",
            "transcript": "Is that something completely crazy?",
            "speaking duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "07:53-08:54",
            "transcript": "Well, I I I don't think it's crazy. I think it's very um uh difficult because um you might lose imaging aspects um in making making them um conform to that. I I know even going having everything um going into say a deep net, they often require certain matrix size and bit. You lose it or not? Well, maybe if you have enough cases, it might be okay, but um um sometimes the formats can be exactly the same, but small things like uh timing could um change the acquisition such as in dynamic contrast enhanced MRI. You know, one country was at 92nd intervals and another one's 60 and cause big 10 changes in the data.",
            "speaking duration": 61,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "08:54-08:55",
            "transcript": "But um, I don't know.",
            "speaking duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "08:55-08:55",
            "transcript": "Uh.",
            "speaking duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "08:55-08:57",
            "transcript": "Oh, we have two hands up. I'm sorry.",
            "speaking duration": 2,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "08:57-08:58",
            "transcript": "Arnold does have his hand up.",
            "speaking duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "08:58-08:58",
            "transcript": "Good.",
            "speaking duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger UChicago",
            "timestamp": "08:58-08:59",
            "transcript": "And then Matt Lou.",
            "speaking duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        }
    ]
}