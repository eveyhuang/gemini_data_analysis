{
    "meeting_annotations": [
        {
            "speaker": "Dylan Burnette",
            "timestamp": "00:00-00:59",
            "transcript": "So I would say that this this this question goes well beyond what I think about normally. Um, because we're trying to get to structural resolution. And if you do that in an animal, it would be too much data. We don't have big enough computers for that. And so this is a very interesting, you know, concept because when we want to look at an animal, we just cut off a piece of our zebra fish and take that tissue and we call that an animal or an organism. Um, but I I think this is a very interesting conversation, but what will we do with that data if we got super res from an entire animal. Like I'm not even sure what we do with it. And that's what I'm actually scared about that part. I guess when we have a single cell, we're we're also doing expansion microscopy and we're doing structural elimination or single molecule imaging on top of that. And I'm calculating resolution and it's scaring me already.",
            "speaking duration": 59,
            "nods_others": 0,
            "smile_self": 15.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "00:59-01:04",
            "transcript": "I'm I'm I'm just it's exciting but also scary at the same time.",
            "speaking duration": 5,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Uzay Emir",
            "timestamp": "01:14-01:27",
            "transcript": "Can I uh, so similar to your question actually, your question is originating from a problem actually going back to whole animal or big organism will be bring a lot of data.",
            "speaking duration": 13,
            "nods_others": 1,
            "smile_self": 15.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Uzay Emir",
            "timestamp": "01:27-01:28",
            "transcript": "Now the question is,",
            "speaking duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Uzay Emir",
            "timestamp": "01:30-02:07",
            "transcript": "how from my point of view, I'm a microscopic person and even our data is big, but uh it's not as complicated as yours. Uh, but it has its own difficulty. How you are seeing all you guys are optical imaging and super resolution compared to myself to make it really translatable to real life or big animal or live animal uh thing. So what is your pathway to bring those techniques to this and considering your concern about the size of the data.",
            "speaking duration": 37,
            "nods_others": 1,
            "smile_self": 18.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "02:10-02:17",
            "transcript": "So is it possible to to combine structured light and uh scattering at the same time? Is that too complicated?",
            "speaking duration": 7,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "02:17-02:20",
            "transcript": "Sorry, I'm not I'm not a physicist, but",
            "speaking duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "02:21-03:13",
            "transcript": "we are using structured light uh through the line of slide sheet and in and and it's this um, you know, basic run of the mill technologies. Um, but can you collect scattered light from that sort of information? Because that's already up and running of in many, many labs around the country and world right now. Uh, would being able to combine something like the light of slide sheet which penetrates into tissues pretty decently, let's not say great, but decently. Now it's good as 400 tongue wood. But um, uh, but we uh, but that also has already structured information with it. Does that complicate getting scattered light or not? Because that's kind of a fascinating idea. Uh, that I haven't really thought too much about. And at least three of you have thought a lot about it. So that's pretty cool.",
            "speaking duration": 52,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Josh Brake",
            "timestamp": "03:18-05:06",
            "transcript": "I guess my two cents on this is by de facto as optical engineers, we try to throw out the scattered light. That's the conventional wisdom. We always have done that. And so every optical system you pick up whether it's your iPhone or a microscope is designed to throw that information out. And that's the that I think is the kernel of the revolutionary idea here is like let's remove that constraint and go back to the drawing board and think differently. And so that I think that that's one thing. The other thing I was thinking about related to this conversation of so much data and resolution is you need the resolution but not everywhere. And maybe we don't need the super resolution at all points. So you can think at a native structure like I want to try to have, you know, again as optical designers, conventionally, we think about isotropic resolution everywhere. But in a more if we remove that constraint and think about, well maybe I just care about super resolution here and here and here and here in these specific parts of the cell and everywhere else, I don't care. It's fine if I have millimeter resolution maybe. I just want to be able to zoom into these certain areas. And so maybe there's I think if we think about more interesting ways to we have a limited number of pixels or voxels or bits or however you want to whatever fundamental quanta of information you want to think about that we can distribute along our in our sample. And so can we think about innovative ways to redistribute that in a in a way that's efficient but maybe isn't, you know, interpretable without a algorithm to understand or reconstruct or give us some meaningful information about what's going on.",
            "speaking duration": 108,
            "nods_others": 0,
            "smile_self": 30.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Matt Lew",
            "timestamp": "05:07-05:07",
            "transcript": "Yeah, Josh, I I'd like to to build off of that.",
            "speaking duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Matt Lew",
            "timestamp": "05:07-06:45",
            "transcript": "So I think you made a great point about the uh the the sort of paradigm of of at least microscopy uh and ballistic imaging that we throw away all the scattered light. Then we're kind of forgetting our friends in the diffuse optics regime. So let's think of DOT diffuse optical tomography and people sort of um uh there's there's actually a colleague here at Washu who builds the whole brain uh uh imaging device, right? And and looks at uh sort of function uh um by collecting diffuse light off of there. So maybe um maybe uh a regime of pushing the resolution there might be helpful. Uh, right now it's sort of spatially resolved based upon the average number of scattering events. It takes light longer to travel further or or uh uh and and you can sort of resolve some things in depth that way. So maybe that's one way we can think about it. I think maybe another question is um like if we take our existing technologies that can penetrate deep but somehow are deficient in some other axis. So let's just say pet for example, right? The gold standard in in specifically detecting something uh um within within the body and organism. Uh, is there something that optical imaging if engineered the right way, maybe with the the fancy uh fluorescent based reporters that we saw uh earlier in the keynote. Um, maybe there's something that we can do to to to solve a more targeted problem as a as a as a as a prototype for for for uh for solving the the bigger question that that was posed. Um, just a couple ideas there.",
            "speaking duration": 98,
            "nods_others": 0,
            "smile_self": 10.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Aseema Mohanty",
            "timestamp": "06:46-08:14",
            "transcript": "I'd like to I'd like to kind of jump on that as well. Um, I I feel like one thing that I I and this might just be my inexperience with the super resolution field, but like um, one thing we don't really talk about much is what and and similar to what Matt was talking about is um, what could we kind of learn from, you know, we're kind of used to it a certain type of imaging with an objective with a very high NA, you know, lens and and I feel like that's kind of a bulk of the problem and I'd like to understand from people who are doing microscopy and and like uh, you know, what are kind of the limitations um from people who are actually using super resolution um with those objective lenses that that you guys use um in terms of I guess field of view and like the amount of power that you need to be able to illuminate an entire section.",
            "speaking duration": 88,
            "nods_others": 0,
            "smile_self": 30.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "08:15-09:01",
            "transcript": "So I will say that from because I'm the I'm I'm I'm the uh I'm the honer here not the forger. I don't uh I don't uh I I developed one technique and no one ever used it. So it was called bomb because back in the day anything with single markers had to have acronym. Uh, and I learned through that that I should not develop techniques. I should just take other people's and",
            "speaking duration": 46,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "09:01-09:55",
            "transcript": "Um, but I'll say as a as a user, and I'm we're pretty advanced users, uh that we are pretty much addicted to the high NA lens because it's what's available. It's what's commercially available, it's what we can order. We're not going to build our own lenses. And that is why we use them. There's no one has come up with a better way that I can purchase to do this and it's very limiting because high NA is usually uh uh for the most of our of our work for super res which is single molecule or structured light.",
            "speaking duration": 54,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "09:55-10:00",
            "transcript": "Um, and I generally put those into just two categories. I know that some of you are have four categories in super res.",
            "speaking duration": 5,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "10:00-10:14",
            "transcript": "Um, but when you think about it from that way, it's very limiting as far as Z depth, so how far you can go, so your axial dimension that you can penetrate your sample is limited.",
            "speaking duration": 14,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        }
    ]
}