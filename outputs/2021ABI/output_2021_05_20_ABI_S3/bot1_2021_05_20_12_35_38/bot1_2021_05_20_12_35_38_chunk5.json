{
    "meeting_annotations": [
        {
            "speaker": "Kristen Maitland",
            "timestamp": "00:00-00:35",
            "transcript": "through your filters, that might be one aspect. Um, so I think um it it would take a big picture look at what are you trying to do, what are is your imaging system or your microscope that you're trying to use and um how is scattering affecting your ability to image what you're trying to see or image at a depth, you know, that maybe you can't reach. Um, and then taking into into consideration your sample that the light is traveling through and how does that influence or impact your ability to image whether it be resolution or depth or field of view.",
            "speaking duration": 35,
            "nods_others": 1,
            "smile_self": 10,
            "smile_other": 10,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Matt Lew",
            "timestamp": "00:35-02:05",
            "transcript": "Yeah, maybe to to jump off that and and bridge back to what uh what Sian was talking about at the beginning. Like if we're in the fluorescence and uh some of the light from your floor for gets multiply scattered, there's kind of two ways that we might think of detecting it. One is just waiting long enough because those multiply scattered photons will take longer to get to you. So you you kind of wait long enough and see where they landed and maybe those were your scattered photons. The other thing that I think Josh is really better expert at is um if you change your imaging instrument in some way because that light was determinate that scattering was deterministic, like the cell boundaries that the light scattered off of were at certain some place relative to your imaging system. If you change the instrument a little bit to better leverage those scattering patterns and get more light out now and get more light in, then you have a signature for for for fixing your image in the bit. So the the the the imaging problem becomes harder now because it's no longer uh uh optically engineered perfect objective that does your job for you. Now you've got to reoptimize the imaging system on the fly for your specific sample and then you got to make sure that whatever you thought the scattering was from wherever it was from and whenever it was from like that's actually what you're detecting because it in principle at least with elastically scattered light, there's no way to identify it any other way. Like the photon energy is going to be the same, right? And uh and so yeah.",
            "speaking duration": 90,
            "nods_others": 2,
            "smile_self": 15,
            "smile_other": 10,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Josh Brake",
            "timestamp": "02:05-03:40",
            "transcript": "There's also some interesting things with scattering connecting back to our conversation about numerical aperture, which counter, you know, counterintuitively scattering can actually help you because at the fundamental level, high NA is about high spatial frequencies is about high angle illumination and so scattering has a way of creating this for you because of the, you know, in strongly scattering tissue after you, you know, scatter enough, you're at lambda over two. Um, so that I think is something else too that you make your you make your your enemy your friend in some sense by, you know, taking advantage of what's happening there and then maybe you maybe you decouple from the traditional connection with lenses between field of view and working depth and NA and the geometrical piece of lens design, there's only so many parameters you have to to tweak, but if you can think about your lens as a and I think Sian will be, you know, with Laura, she always talks about like your lens is a Laura Waller, your lens is a matrix basically, which I love that like picture thinking about um, you know, and in the wavefront shaping community, we think of like replacing a lens with a cube of salt or something like, you know, cube of sugar or something and just sending light through it and if you know what the you know, the matrix is, mathematically there may be some interesting properties there that you can leverage that may actually help you and not hurt you.",
            "speaking duration": 95,
            "nods_others": 1,
            "smile_self": 20,
            "smile_other": 10,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Sian You",
            "timestamp": "03:40-04:00",
            "transcript": "That's a great point. Um, I was also doing post in Laura Waller's lab and uh I really like her perspective that uh lens is just a phase mask, right? So whatever whatever point spread function you want in the end, you can somehow engineer the lens you want.",
            "speaking duration": 20,
            "nods_others": 0,
            "smile_self": 50,
            "smile_other": 10,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Sian You",
            "timestamp": "04:00-05:31",
            "transcript": "And uh back to Dylan's question, that's actually really um uh I think it's also a very good point to start framing off. Um, so to solve the problem, I think we can start from two. Uh one is detection, how can you reassign photons based on different properties, based on different characteristics of photons you capture and then how can you reassign it. And then second way is uh second way, how can you form the beam for for example, for light sheet microscopy, it's very hard to get a nice light sheet after scattering. So how can you use wavefront engineering or scattering compensation to get a nice illumination in the beginning. So you don't worry that much about detection later. Um, my question is um like what is the fundamental limit in either direction. So if we try all these methods and we push our depths like 10% more, is it worth it? Uh, and this applies for both um detection and illumination. Detection, you have photo photon starvation, you have noise issue and for illumination, um no matter how much you compensate, at some point you lose the correlation between photons and you lost uh at one point it's just random walk. So how much more can we push and what is the fundamental challenge in the in the field right now.",
            "speaking duration": 91,
            "nods_others": 0,
            "smile_self": 20,
            "smile_other": 10,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "05:31-05:36",
            "transcript": "And Luke, can we hear from you as well?",
            "speaking duration": 5,
            "nods_others": 0,
            "smile_self": 100,
            "smile_other": 10,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Luke Mortensen",
            "timestamp": "05:36-06:15",
            "transcript": "Um, yeah, sure. So I I think I mean, I think that one of the challenges that I see is is like how much deeper can you go? How much further can you go is a major problem. But I think a a bigger problem that we've noticed is how long does it take you to get there? You know, because you can do a decent job of understanding what's happening to the light and like recreating a focus and then detecting whatever signal you get out.",
            "speaking duration": 39,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Luke Mortensen",
            "timestamp": "06:15-07:35",
            "transcript": "But you're really going to have to detect for a really long time to get enough photons out and overcome your SNR problems or it's going to take you quite a while to do the correction factor and even you know, current best in class it's pretty, you know, for a whole organism or, you know, whether it's like the whole organism or just a whole organism, in both cases you're looking at problems with, you know, signal and movement and time. And um, you know, those those seem like like major issues that in order to get it from the point of where it requires full um deconstruction of the organism, you know, which is definitely and a useful and insightful approach back to something that's maybe hopefully happening in a dynamic setting, we can understand like how things are are altering with time. Um, you know, sort of like our keynote talk is. I think, you know, trying to bridge those two two spectrum I think is sort of as a as a challenge.",
            "speaking duration": 80,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "07:36-08:33",
            "transcript": "But if you're trying to detect things in multiple dimensions such as you say time, if things are going to take longer to get there, is it possible that you could just use multiple detectors? Because when we when we go and put two cameras on our system, we want them to be par focal basically. We want to them to be in the exact same focal plane. But can you alternate the detectors to detect um things that take longer to get there, just move the camera closer. I mean can I don't know if if with one detector we're going to be able to get to this thing and you and you guys are the physics people, but I can't imagine this is going to be solved with one camera. It's just going to take too darn long to image an organism.",
            "speaking duration": 57,
            "nods_others": 0,
            "smile_self": 10,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Sian You",
            "timestamp": "08:33-09:31",
            "transcript": "You're definitely not crazy. There is definitely a lot of multi view uh microscopes for all kinds of modalities over there. Uh actually people do it. I I think there are like uh several ways to come about it. One is kind of multi view, so you scan uh different regions at the same time and somehow they end up in different pathways and you can use different cameras to detect it. And for scattering problems or for aberration problems, uh you have this pupil plane where you can also simultaneously uh kind of correct uh different aberrations and scattering for different regions. So there are ways to deal with it, but uh then if you do that, then you are at the danger of even being more photon starved. Um, so it's always a tradeoff. The more multiplex, the the the less photons you have. So.",
            "speaking duration": 58,
            "nods_others": 0,
            "smile_self": 50,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "09:40-09:52",
            "transcript": "Uh Candace, would you like to introduce yourself? I know that we've been talking without you, but if you'd like to just mention um your area um that you work in and especially related to super resolution and maybe your interest in this area just briefly.",
            "speaking duration": 12,
            "nods_others": 0,
            "smile_self": 100,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Candace Fleischer",
            "timestamp": "09:52-10:15",
            "transcript": "Yeah, I I really apologize about that. Um, so I'm Candace Fleischer, I'm at Emory University in Atlanta. Um, my",
            "speaking duration": 23,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        }
    ]
}