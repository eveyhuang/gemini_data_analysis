{
    "meeting_annotations": [
        {
            "speaker": "Maryellen Giger",
            "timestamp": "00:00-00:20",
            "transcript": "have many you all are smart, you all have great AI technologies, but I think we're looking at the issues of how do we bring it all together? Um, um, to me, that's the challenge. Alex, I haven't put you on the spot. I'm sorry, you're right in the middle of my screen and I keep going.",
            "speaking duration": 20,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Beck Kamilov",
            "timestamp": "00:21-00:22",
            "transcript": "Zoom has a question, no, Jim?",
            "speaking duration": 1,
            "nods_others": 0,
            "smile_self": 100.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Jim Mitchell",
            "timestamp": "00:23-00:51",
            "transcript": "Yeah, I have I have a question quickly. I know in other domains, um, you know, I know a lot about the Tesla autopilot work. The crowd sourcing of images and and so on has helped them a lot to improve that. Is there crowd sourcing going on here and and and where do those images end up residing that everybody could have access to the same large set of images?",
            "speaking duration": 28,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Raising Hand",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger",
            "timestamp": "00:51-01:47",
            "transcript": "Uh, there is crowd sourcing in radiology. They do it at radiology meetings and also across the web where they they recruit some one project had over 180 radiologists who did multiple annotations on uh chest images. So they can do it. So and that's what I think I meant by technically all we can do a lot. It it it it we're in a very technical field and we're hindered by data and culture. Um, your other question even in um cellular images, many of you are in cellular. I don't know you talk about your experience there and I'm going to ask Alex about that, but you have to",
            "speaking duration": 56,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger",
            "timestamp": "01:47-02:02",
            "transcript": "so and that's what I think I meant by technically all we can do a lot. It it it it we're in a very technical field and we're hindered by data and culture. Um, your other question even in um cellular images, many of you are in cellular. I don't know you talk about your experience there and I'm going to ask Alex about that, but you have to people sit and circle cells. crazy just to create data and that gets back to bullet one. How many cells do you have to circle to have enough data to algorithm. Um, but yes, the crowd sourcing it does it does exist, but it's it's not on a massive um scale. Good point, Jim. And sorry I didn't see your hand ready. So I'm used to this, not the stationary one that they have on. Alex.",
            "speaking duration": 75,
            "nods_others": 0,
            "smile_self": 30.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Jim Mitchell",
            "timestamp": "02:03-02:04",
            "transcript": "Thank you back.",
            "speaking duration": 1,
            "nods_others": 0,
            "smile_self": 100.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Alex Walsh",
            "timestamp": "02:04-03:35",
            "transcript": "Yeah, we do a lot of circling cells. Um, yeah, we do auto fluorescence imaging, so it's low very low signal to noise and that creates a challenge for automated ways to process our data. And so we can see the patterns, but it's very hard to train traditional pipelines to do the segmentation. So that's why we've been working with AI. And actually, I really like Paris's point and you worded that much better than I was going to. Um, but my first interactions with AI and microscopy was stuff to for noise removal and um improving resolution and lowering laser power and stuff and I was just blown away by that work. And but I have right the same we know we know physical and biological boundaries on these conditions and I would like to see more integration of that into AI like if I'm segmenting a cell and it puts the nucleus, you know, right up against the edge of the cell, that's probably not where the nucleus is, right? It's more in the center. So how can we incorporate these things that we know um are are biological or physical boundaries and have the AI say like flag that oh that can't be right. Let's try again. Um, I think that gets into your errors, you know, what errors do we tolerate and how can we improve and minimize those.",
            "speaking duration": 91,
            "nods_others": 0,
            "smile_self": 50.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger",
            "timestamp": "03:36-04:00",
            "transcript": "I that's a a very good point. Um, when when I find when training AI, you should take advantage of a priori knowledge as well as what image are you inputting? Yes, one could say if I have infinite amount of data, infinite amount of truth, I can train it if I I just let it chug away, right?",
            "speaking duration": 24,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger",
            "timestamp": "04:00-04:16",
            "transcript": "But we don't have that especially in um the medical biomedical field, you know, you know, for things like image net, there's lots of pictures of cats and dogs and cars and light poles and all, but we don't have a lot of that annotated in the medical field even though we're generating so many medical images per day.",
            "speaking duration": 16,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger",
            "timestamp": "04:16-04:30",
            "transcript": "Um, and I I I I appreciate um these comments on the a priori because we we need to help the AI. I you know, sometimes I tell my students think like a human because if you know that um image presentation A is better for the human to look at than image presentation B.",
            "speaking duration": 14,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger",
            "timestamp": "04:30-05:13",
            "transcript": "Why give the AI raw data? In a way, you're you're training in files, the training of all these humans has they have been reading images for many, many years. So how many of you in a sense consider what image am I giving to the AI and do you sometimes do the preprocessing to help the AI learn because you have limited data?",
            "speaking duration": 43,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger",
            "timestamp": "05:13-05:13",
            "transcript": "Caroline, what do you think of that?",
            "speaking duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Carolyn Bayer",
            "timestamp": "05:13-06:05",
            "transcript": "Yeah, no, so I was nodding because um so I I'm also focused more on the imaging and instrumentation side. Um, but I work with collaborators that um do more of the sort of machine learning or, you know, neural network development. And um, you know, that's that definitely resonates that you have sometimes picked a data set to show a specific thing, right? Um, and that data set obviously then may not be representative of what that algorithm is actually going to encounter.",
            "speaking duration": 52,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger",
            "timestamp": "06:06-06:21",
            "transcript": "So if we had our database, our messy database, we could include both, we can include them of different spatial resolution, we can include AI to do the reconstruction. Um, so Shannon, how's this list coming? It looks good. Looks longer than three minutes. It looks really good.",
            "speaking duration": 15,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "06:31-06:34",
            "transcript": "I'm trying to get the main points.",
            "speaking duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger",
            "timestamp": "06:34-06:41",
            "transcript": "Okay. Um, do you want us to go through? Do you want to kind of summarize for us and then we can so so the topic here is",
            "speaking duration": 7,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger",
            "timestamp": "06:41-07:01",
            "transcript": "um what's the next big thing for this field? How can you what would be your dream of where it would go and um uh near the and I'll tell you how I've been trying to do my dream models.",
            "speaking duration": 20,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Beck Kamilov",
            "timestamp": "07:01-07:03",
            "transcript": "Is it the open question?",
            "speaking duration": 2,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Maryellen Giger",
            "timestamp": "07:03-07:07",
            "transcript": "That's is it a clinical question? What do you mean on the",
            "speaking duration": 4,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Beck Kamilov",
            "timestamp": "07:07-07:14",
            "transcript": "I'm sorry. like are we discussing big big should we kind of",
            "speaking duration": 7,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Beck Kamilov",
            "timestamp": "07:14-07:34",
            "transcript": "I think one thing that just was coming to my mind while we're discussing is like it would be amazing to have a recipe of how much data we need for a task. There is no recipe, you know, like you come to a problem, you say, I want to segment this thing.",
            "speaking duration": 20,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Beck Kamilov",
            "timestamp": "07:34-07:44",
            "transcript": "So, your collaborator asks, how much should I measure? Yeah. How much should I collect? Can anybody here answer?",
            "speaking duration": 10,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Beck Kamilov",
            "timestamp": "07:44-07:54",
            "transcript": "Okay, now, how about if you know some prior information about the problem, say nucleus is in that position or that. Now, can this help me cut the data and by how much?",
            "speaking duration": 10,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "07:54-07:54",
            "transcript": "Well",
            "speaking duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "Yes",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Beck Kamilov",
            "timestamp": "07:54-07:54",
            "transcript": "Right.",
            "speaking duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "Yes",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "07:55-08:01",
            "transcript": "I guess I guess for me I actually want to take it one step further and almost",
            "speaking duration": 6,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "08:01-08:04",
            "transcript": "almost take data out of the equation entirely and",
            "speaking duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "08:04-08:10",
            "transcript": "let me let me explain a bit where we've been talking a lot about",
            "speaking duration": 6,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "08:10-09:54",
            "transcript": "you know, we need a lot of data, we need a lot of ground truth. Given an infinite amount of data, um that was an interesting point to me because I still wonder like would the model still learn what we wanted to learn? And I'm not I hesitate before answering and I feel like that hesitation by itself says that even our models aren't quite there yet and that maybe more data isn't exactly the problem that we're looking at. Maybe I just I keep I keep thinking of like these new methods involving self-supervised learning and few shot learning and anomaly detection where it's less about how much data you can throw at the problem and more about how we can build this architecture that without it becoming kind of a handcrafted algorithm has very specific features that it looks for. And so as a result, you don't need this lengthy training process, you don't need terabytes and terabytes of data, but what you can have to again sort of inform the end user if they don't have enough data is some sort of uncertainty quantification at the end. So for instance, if you're trying to do some kind of segmentation and you have your data set, you don't necessarily need to do a kind of calculation of how much data do I need, you just kind of give it to the algorithm, the algorithm does its few shot semi or self-supervised training and then at the end of that spits out, okay, here's the segmentation that I did and here's my certainty that it's correct.",
            "speaking duration": 104,
            "nods_others": 0,
            "smile_self": 20.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        },
        {
            "speaker": "Beck Kamilov",
            "timestamp": "09:54-09:58",
            "transcript": "Do do we also need the certainty on the certainty so that we can trust the certainty?",
            "speaking duration": 4,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None"
        }
    ]
}