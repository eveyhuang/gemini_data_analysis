{
    "all_speakers": [
        "Arnold Hayer",
        "Lingyan Shi",
        "Richard Wiener",
        "Aseema Mohanty",
        "Melike Lakadamyali",
        "Kristen Maitland",
        "Vivian Qian Liu",
        "Mimi Sammarco",
        "Jin Zhang",
        "Joyoni Dey",
        "Douglas Shepherd",
        "Domenico (Nick) Galati",
        "Douglas Shepherd",
        "Matt Lovett-Barron"
    ],
    "total_speaking_length": 4067,
    "all_data": [
        {
            "speaker": "Kristen Maitland",
            "timestamp": "00:00-00:30",
            "transcript": "Um I I did appreciate the suggestion to use a um shared document and so Matt I actually set one up and I'll put it in the chat and that way everybody can um reach it. Hopefully let me know if you cannot access it and I will adjust the permissions. Um and so that way people can follow along and they can actually add as well, but um we can use that and then uh summarize at the end and copy it in.",
            "speaking_duration": 30,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "00:00",
            "end_time": "00:30",
            "annotations": {
                "acknowledge contribution": "Kristen acknowledges the suggestion to use a shared document, recognizing Matt's input.",
                "assign task": "Kristen assigns the task to the team members to inform her if they cannot access the document so she can adjust the permissions."
            }
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "00:30-01:01",
            "transcript": "Okay, so um similar to yesterday, we'll start off with taking a minute to think and write down um a question or a topic for discussion and then we will go around do some introductions. Hopefully you've interacted with some of you, but we'll we'll do kind of a brief intro and I would appreciate if your introduction um has some sort of direction towards this particular topic. So what you're interested in related to the imaging across temporal and spatial domains.",
            "speaking_duration": 31,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "00:30",
            "end_time": "01:01",
            "annotations": {
                "propose decision": "The speaker proposes a plan for the meeting, including taking a minute to think and write down a question or topic for discussion, and then doing introductions.",
                "explain or define term or concept": "The speaker explains that the introductions should be directed towards the topic of imaging across temporal and spatial domains to provide context for the discussion."
            }
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "01:01-01:02",
            "transcript": "Okay, so I'll set a timer for one minute and we'll have some silence.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "01:01",
            "end_time": "01:02",
            "annotations": {
                "assign task": "Kristen is assigning the task of setting a timer to herself."
            }
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "02:06-02:28",
            "transcript": "Okay, hopefully you all had a chance to gather your thoughts. Um so I'll just go down the list based on I think it's last name alphabet last name alphabetical and and just call on each of you to um introduce yourselves and um maybe just a thought on what would be um an area within this topic that might need research. So uh Joyoni.",
            "speaking_duration": 22,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "02:06",
            "end_time": "02:28",
            "annotations": {
                "assign task": "Kristen is assigning the task of introducing themselves and sharing a thought on a research area to each person on the list.",
                "encourage particpatioin": "Kristen is encouraging each person to participate by introducing themselves and sharing their thoughts on a research area.",
                "explain or define term or concept": "Kristen is explaining the process of how the introductions will be done, which is based on alphabetical order by last name."
            }
        },
        {
            "speaker": "Joyoni Dey",
            "timestamp": "02:28-04:06",
            "transcript": "Hi, um I am uh physics faculty at LSU. Uh I teach medical imaging medical imaging to medical physics um it's a medical physics program. And um my area of interest is uh right now X-ray inter X-ray interferometry for mammography and also I work on neutron interferometry uh in collaboration with uh NIST NCR group, the neutron uh center for research um for neutron research in NIST um. So um I also have worked on spect uh and uh quite extensively spect and one thing like sorry like gun to the head I the only thing I can remember that is uh you can do simultaneously um multiple modalities. So um so for example, interferometry is giving you attenuation and uh phase and uh scatter images completely different three physical uh properties simultaneously because you're able to capture the phase shift and scatter and you can do it with uh different radio tracers for um multi tracers for pet um pet imaging for example, you know, at different energies um for the same detector and um so that's that's what comes to my head quickly and uh if somebody thinks it's useful. I lack the biological part so if somebody gives me a problem I can think about it. Thank you.",
            "speaking_duration": 98,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "02:28",
            "end_time": "04:06",
            "annotations": [
                {
                    "explain or define term or concept": "Joyoni explains X-ray interferometry, neutron interferometry, and SPECT, providing background on her areas of expertise and research."
                },
                {
                    "present new idea": "Joyoni introduces the idea of simultaneously using multiple modalities, such as interferometry providing attenuation, phase, and scatter images, and using different radio tracers for PET imaging at different energies."
                },
                {
                    "offer constructive criticism": "Joyoni acknowledges her lack of biological expertise and offers to contribute her knowledge if someone provides a problem, indicating a willingness to improve the research by applying her skills to a specific challenge."
                }
            ]
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "04:06-04:07",
            "transcript": "Thank you.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "04:06",
            "end_time": "04:07",
            "annotations": {
                "acknowledge contribution": "Kristen Maitland acknowledges Joyoni Dey's contribution after they introduced themselves and their research interests."
            }
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "04:07-04:09",
            "transcript": "Okay, uh Nick.",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "04:07",
            "end_time": "04:09",
            "annotations": {
                "encourage particpatioin": "Kristen Maitland is inviting Nick to introduce himself and share his thoughts on the research topic, similar to how she invited Joyoni in the previous turn."
            }
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "04:11-05:30",
            "transcript": "Hi, my name's Domenico (Nick) Galati. I'm at Western Washington University. I'm in the biology department there. And uh the reason I think I was probably put in this group is because I I'm very interested in protein trafficking within small structures known as cilia and cilia are diffraction limited and they're really important for a lot of developmental clinical things so they they receive Sonic Hedgehog signaling and so that's really important for patterning. And what's interesting is that the cilia generally in terms of human mutations, they're not they're not abolished so they're still present but there's just minor defects in trafficking within the cilia and to the cilia. And so this has only been studied so far for the most part, the hardcore trafficking stuff has only been studied in static cells that aren't moving or in cilia that aren't moving but in a developing organism clearly they are moving. And so something that the field needs is to be able to study cilia trafficking in either a cilia that's beating because sometimes they beat back and forth to generate flow. Um that's really challenging because they can beat up to 15 to 20 hertz. So trying to image a diffraction limited spot and something that's beating at 20 hertz uh would be a challenge and so that's the kind of stuff that I'm interested in. I have the biological expertise and I can do, you know, I do fluorescence imaging and and live cell imaging and super resolution imaging and stuff like that. But um I think that that problem will need something beyond that. So thanks.",
            "speaking_duration": 79,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "04:11",
            "end_time": "05:30",
            "annotations": [
                {
                    "explain or define term or concept": "The speaker explains what cilia are and their importance in developmental clinical things, providing context for his research interest."
                },
                {
                    "present new idea": "The speaker introduces the idea of studying cilia trafficking in moving cilia, highlighting the challenges and the need for new approaches."
                }
            ]
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "05:30-05:32",
            "transcript": "Great, thank you.",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "05:30",
            "end_time": "05:32",
            "annotations": {
                "acknowledge contribution": "Kristen Maitland acknowledges the previous speaker's contribution."
            }
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "05:32-05:33",
            "transcript": "Okay, Arnold.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "05:32",
            "end_time": "05:33",
            "annotations": {
                "encourage particpatioin": "Kristen Maitland is inviting Arnold to introduce himself and share his thoughts on the topic, continuing the introductions from previous turns."
            }
        },
        {
            "speaker": "Arnold Hayer",
            "timestamp": "05:33-07:41",
            "transcript": "Hi everyone, my name is Arnold Hayer. I'm a faculty in the Department of Biology in McGill University. And uh my lab is primarily interested in signal transduction that regulates cell motility. So migration of million cells and uh not only of single cells but also of collectives of groups of cells. So the the main challenge related to the topic of this breakout session um in the context of my research is I think to link molecular events to subcellular events and to super cellular events. So so it's kind of the we need to understand what's happening at the molecular level um what is happening with for example polymerizing acting structures um but they happen at a very very fast time scale and uh cells move at a much much slower cells time scale. So uh the the big challenge then is to to link uh these molecular events to the larger um cellular outcomes. And um I don't quite know how to do that because you know if we if we monitor on single molecules we we can probably do that. Um but how do we know that which molecular signal event would then translate into a cell for example to polarize as a whole which happens on the scale of tens of minutes. So that would be something to to think about. And regarding this this um other question of uh how can we collect simultaneously at different times of resolution I I guess um one could think of of um imaging live cell imaging streams where you resolve where you try to monitor events at a relatively low resolution and then have online um automated image analysis that ident identifies relevant events zooms in goes at really high uh imaging frequency so um this is something that we don't do but but that we that I thought could be something of interest of course the technical challenges to overcome and and developing a robust workflow for for doing this. So.",
            "speaking_duration": 128,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "05:33",
            "end_time": "07:41",
            "annotations": [
                {
                    "explain or define term or concept": "Arnold explains his lab's focus on signal transduction regulating cell motility, defining it as the migration of single cells and collectives, providing context for his research area."
                },
                {
                    "present new idea": "Arnold presents the idea of linking molecular events to subcellular and supercellular events, highlighting the challenge of understanding how molecular signals translate into cellular outcomes, which is a novel concept in the context of the discussion."
                },
                {
                    "propose decision": "Arnold proposes the idea of imaging live cell imaging streams with online automated image analysis to identify relevant events and zoom in at high imaging frequency, suggesting a potential approach to address the challenge of collecting data at different time resolutions."
                }
            ]
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "07:41-07:42",
            "transcript": "Great, thank you.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "07:41",
            "end_time": "07:42",
            "annotations": {
                "acknowledge contribution": "Kristen Maitland acknowledges the previous speaker's contribution."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "07:47-09:57",
            "transcript": "Hi, um I'm Melike Lakadamla. I'm at U Penn. Um my lab um um uses and develops single molecule uh methods and uh in particular super resolution imaging to study spatial and temporal organization of cells. Um one question of interest in the lab is genome folding genome organization um and how that uh regulates gene activity. Um and when we when we study uh genome organization again this is as as Nick said it's a diffraction um sort of limited imaging cannot visualize how the DNA is folded in uh 3D space inside the nucleus. Um nucleus is very crowded um uh chromatin DNA uh folding happens at small length scales. Um so to uh visualize this we need very high resolution um methods like super resolution microscopy. Uh but these super resolution microscopy methods are often uh very slow. Um and so we also are interested in dynamics of the nucleus and to study that we have to do it separately. Um so um for example label subsets of um um uh chromatin interacting proteins or chromatin components and and track their dynamics at fast time scales. Um and somehow sort of relate those two things together in in in separate experiments. And so um you know there's always uh in in microscopy I think trade off between spatial and temporal resolution um and um I don't know if it's possible to you know either optimize that trade off or or get rid of it completely. That would be um quite uh uh an an important challenge I think um to tackle.",
            "speaking_duration": 130,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "07:47",
            "end_time": "09:57",
            "annotations": [
                {
                    "present new idea": "Melike introduces her lab's focus on single-molecule methods and super-resolution imaging for studying spatial and temporal organization of cells, which is a new topic in the discussion."
                },
                {
                    "explain or define term or concept": "Melike explains that diffraction-limited imaging cannot visualize how DNA is folded in 3D space inside the nucleus, providing context for the challenges in genome organization studies."
                },
                {
                    "present new idea": "Melike presents the idea of optimizing or eliminating the trade-off between spatial and temporal resolution in microscopy as an important challenge to tackle, which is a novel suggestion."
                }
            ]
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "00:00-00:18",
            "transcript": "interesting regions within your sample um that uh are are are actually interesting to further look at higher resolution sort of zoom up to those and do that in a fully automated way uh with a sort of intelligent type microscope. Um so those are my thoughts.",
            "speaking_duration": 18,
            "nods_others": 2,
            "smile_self": 11,
            "smile_other": 22,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "10:00",
            "end_time": "10:18",
            "annotations": {
                "expand on existing idea": "Melike is expanding on the previously discussed challenges of spatial and temporal resolution in microscopy by suggesting an automated, intelligent microscope that can zoom in on interesting regions for higher resolution imaging.",
                "present new idea": "Melike is presenting a new idea of using an intelligent microscope that can automatically zoom in on interesting regions for higher resolution imaging, which hasn't been explicitly mentioned before."
            }
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "00:18-00:23",
            "transcript": "Great, thank you. Vivian?",
            "speaking_duration": 5,
            "nods_others": 0,
            "smile_self": 20,
            "smile_other": 20,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "10:18",
            "end_time": "10:23",
            "annotations": {
                "acknowledge contribution": "Kristen Maitland acknowledges the previous speaker's contribution with 'Great, thank you.'",
                "encourage particpatioin": "Kristen Maitland invites Vivian to participate by calling her name."
            }
        },
        {
            "speaker": "Vivian Qian Liu",
            "timestamp": "00:24-03:11",
            "transcript": "Hello, uh I'm Vivian Liu. I'm from McGill University. Uh I'm at uh Institute of Cancer Technology and the McGill Center forology. I was trained as a molecularologist and uh worked in a biophysical lab on super resolution imaging on virus host interactions. So my lab uh I built a single molecule of polarization microscope to look at uh virus uh life cycle. Uh so far I have um so one of my question is uh when the virus replicates on the ER structure close to the nuclei uh nuclei and we uh by so our super resolution microscopy lose the ability to uh resolve those uh replication complex uh in 3D. So we can only do kind of turf. Uh so I'm hoping that uh if there's some methods that we can we can we can we can uh see those very small structures deep in the cell uh with high contrast. So I know that there are tradeoffs between um spatial resolution and the photo uh phototoxicity and as well as the floor for um you know, photon budget. So I think maybe a new floor for um uh that would be helpful and also uh adaptive imaging, for example, that you only uh activate the floor for that on your focal point will all the others uh you do not activate. So that's uh some uh some thoughts I have. Uh and also I am uh uh I really like to look at the dynamics of the virus moving um on the surface of the cells or moving from the inside to the out of the cells. So uh the challenge in here would be the temporal resolution. So um so far we use uh millisecond resolution to track the viruses, but I think there are some very tiny or detailed movements of these virus on the cell surface before they enter or before they enter the cells. So uh and also because virus are so small, they're like 100 to between 100 to 200 nanometers. So then that will require uh high spatial and temporal resolution to resolve those questions. So those are my thoughts on that.",
            "speaking_duration": 167,
            "nods_others": 0,
            "smile_self": 1,
            "smile_other": 1,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "10:24",
            "end_time": "13:11",
            "annotations": [
                {
                    "present new idea": "The speaker introduces the idea of using adaptive imaging, where fluorophores are only activated at the focal point to reduce phototoxicity and improve resolution, which is a novel approach not previously discussed in the conversation.",
                    "explain or define term or concept": "The speaker explains the concept of tradeoffs between spatial resolution, phototoxicity, and photon budget in microscopy, providing context for the challenges in imaging small structures deep within cells.",
                    "expand on existing idea": "The speaker expands on the challenges of imaging virus dynamics by mentioning the need for high spatial and temporal resolution to resolve the movements of viruses on the cell surface, building upon the earlier discussion of resolution limitations.",
                    "provide supporting evidence": "The speaker provides supporting evidence for the need for high resolution by stating that viruses are small (100-200 nanometers), which necessitates high spatial and temporal resolution to resolve their movements."
                }
            ]
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "03:11-03:12",
            "transcript": "Great, thank you very much.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "13:11",
            "end_time": "13:12",
            "annotations": {
                "acknowledge contribution": "Kristen Maitland acknowledges the previous speaker's contribution, Vivian, without necessarily agreeing or expanding on it."
            }
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "03:12-03:15",
            "transcript": "Okay, Matt.",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "13:12",
            "end_time": "13:15",
            "annotations": {
                "encourage particpatioin": "Kristen Maitland is inviting Matt to introduce himself and share his thoughts on the research topic, similar to how she invited others previously."
            }
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "03:15-05:12",
            "transcript": "Hi, Matt Love Baron. I'm in neurobiology at UC San Diego and in my lab uh we're really interested in how neural activity across the brain uh generates behaviors. And so I use microscopy because it's a great system to observe neural activity and I use small transgenic or small transparent fish species so that we can observe activity across the brain and uh without any kind of surgery using uh fluorescent sensors of neural activity like voltage or calcium. And one thing that I think is really important that neuroscience in general is trying to get at is how to link multiple levels of uh looking at the brain such as looking at the activity or the anatomy or what cells are connected to each other or what genes they express. And it's difficult to look at all these with the same method. So I've, you know, worked on some uh approaches to try and register different types of data together so we can look at the same cells under different conditions where we'll look at live uh neural activity imaging in an animal that's behaving and then we take the same animal, we fix it and we do, you know, multiple rounds of in situ hybridization to look at the genes expressed in those same cells using image registration to merge our live brain onto the fixed brain. And I'm really interested in kind of pushing that forward and seeing how many different types of data of imaging data we can merge together to try and link some of these different temporal and spatial domains. So using the tradeoffs of different types of microscopy in live tissue versus say fixed tissue where we can zoom in a lot more and look at, you know, single molecule gene expression and so forth. And so I'm I'm interested to see what what sorts of um imaging modalities we can apply to look at these other levels of organization in the brain and and what sorts of uh registration approaches would be best to merge very different types of data.",
            "speaking_duration": 117,
            "nods_others": 0,
            "smile_self": 10,
            "smile_other": 10,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "13:15",
            "end_time": "15:12",
            "annotations": [
                {
                    "explain or define term or concept": "The speaker explains their use of microscopy to observe neural activity in transparent fish species, clarifying their approach to studying brain function."
                },
                {
                    "present new idea": "The speaker introduces the idea of linking multiple levels of brain observation (activity, anatomy, gene expression) through image registration, which is a novel concept in the context of the discussion."
                },
                {
                    "expand on existing idea": "The speaker expands on the idea of merging different types of imaging data by describing their work on registering live neural activity imaging with fixed tissue analysis, adding details about their specific approach."
                }
            ]
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "05:12-05:14",
            "transcript": "Great, thank you.",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "15:12",
            "end_time": "15:14",
            "annotations": {
                "acknowledge contribution": "Kristen Maitland acknowledges the previous speaker's contribution with a simple 'Great, thank you.', but does not agree or expand on the ideas presented."
            }
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "05:14-05:16",
            "transcript": "Aseema?",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "15:14",
            "end_time": "15:16",
            "annotations": {
                "encourage particpatioin": "Kristen Maitland is inviting Aseema to introduce herself and contribute to the discussion, as the group is going around the list for introductions."
            }
        },
        {
            "speaker": "Aseema Mohanty",
            "timestamp": "05:16-06:30",
            "transcript": "Hi everyone. Um I'm Aseema Mohanty. I'm an assistant professor at Toughs. Um just started this year um in the electrical engineering department and um I work on nanonics, so chip scale optical devices. And um a lot of my work focuses on how do we manipulate light in 3D from a chip. Um so uh what I've kind of primarily been using this for is um in implantable neural probes for optogenetic neural stimulation. And a lot of our problem is the same as what uh Matt said um is reaching across, you know, multiple different regions of the brain but being able to do high resolution stimulation um for optogenetics.",
            "speaking_duration": 74,
            "nods_others": 0,
            "smile_self": 12,
            "smile_other": 12,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "15:16",
            "end_time": "16:30",
            "annotations": [
                {
                    "explain or define term or concept": "The speaker explains that they work on nanonics, defining it as chip scale optical devices, to provide context for their research."
                },
                {
                    "expand on existing idea": "The speaker expands on their work by explaining that they manipulate light in 3D from a chip, building on the initial introduction of their field of nanonics."
                },
                {
                    "provide supporting evidence": "The speaker provides supporting evidence for the relevance of their work to the group's discussion by stating that their problem is the same as what Matt said, which is reaching across multiple different regions of the brain with high resolution stimulation."
                }
            ]
        },
        {
            "speaker": "Aseema Mohanty",
            "timestamp": "06:30-06:31",
            "transcript": "Great.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "16:30",
            "end_time": "16:31",
            "annotations": {
                "express agreement": "The speaker is expressing agreement with the previous speaker's introduction and thoughts."
            }
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "06:31-06:32",
            "transcript": "Thank you.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "16:31",
            "end_time": "16:32",
            "annotations": {
                "acknowledge contribution": "Kristen Maitland acknowledges the previous speaker's contribution, signaling the end of their turn and moving the conversation forward."
            }
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "06:32-06:34",
            "transcript": "Mimi?",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "16:32",
            "end_time": "16:34",
            "annotations": {
                "encourage particpatioin": "Kristen Maitland is explicitly inviting Mimi to introduce herself and contribute to the discussion, continuing the round-robin introductions."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "06:35-08:07",
            "transcript": "I'm Mimi Smarco and I'm at T School of Medicine in New Orleans. I work um in the Department of Surgery and my field is um skeletal regeneration. Um we work in an adult model um and so a lot of what we do is sort of trying to overcome these phase specific um stages that you have where one is soft tissue and then that eventually develops into bone. Um so the imaging techniques um that we have to use are often really challenging. Um my lab specifically looks or has recently started to look at the effects of cell metabolism and how that actually drives um skeletal regeneration. Currently um it's fairly difficult to look at that in terms of we just started looking in terms of spatial transcriptomics and then um high plex proteomics, but really what you're looking at is um, you know, transcript before the enzyme or protein but not knowing if the enzyme is active. And so trying to look at different ways beyond Seahorse analytics, which is going to be an ex vivo analysis, um both spatially and over time in vivo would be incredibly useful to the field. Um and I think it would probably hold a lot of answers. So I'm really just here to see again what sort of imaging modalities which I think somebody else mentioned can be applied to the field. Um so looking at sort of what's here in these sorts of forums and then applying them back to my field.",
            "speaking_duration": 92,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "16:35",
            "end_time": "18:07",
            "annotations": [
                {
                    "explain or define term or concept": "The speaker explains their field of work as skeletal regeneration and the challenges of overcoming phase-specific stages in adult models, providing context for their research area."
                },
                {
                    "present new idea": "The speaker introduces the idea of looking at cell metabolism and how it drives skeletal regeneration, which is a relatively new area of focus for their lab."
                },
                {
                    "express frustration": "The speaker expresses frustration with the limitations of current techniques like spatial transcriptomics and high plex proteomics, as they don't provide information about enzyme activity."
                },
                {
                    "ask clarifying question": "The speaker is looking for different imaging modalities that can be applied to their field to overcome the limitations of current techniques, essentially asking what options are available."
                }
            ]
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "08:07-08:09",
            "transcript": "Great, thank you.",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "18:07",
            "end_time": "18:09",
            "annotations": {
                "acknowledge contribution": "Kristen Maitland acknowledges the previous speaker's contribution, signaling the end of their turn and preparing to move on to the next speaker."
            }
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "08:09-08:10",
            "transcript": "Doug?",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "18:09",
            "end_time": "18:10",
            "annotations": {
                "encourage particpatioin": "Kristen Maitland is explicitly inviting Doug to introduce himself and contribute to the discussion, as she has done with the previous participants."
            }
        },
        {
            "speaker": "Douglas Shepherd",
            "timestamp": "00:00-00:14",
            "transcript": "and it drives the technology sometimes because like there's a resolution race, right? But it it doesn't mean that we're answering questions better. And so I think some careful thought about what really needs to be quantified across space and time can make a big difference.",
            "speaking_duration": 14,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "20:00",
            "end_time": "20:14",
            "annotations": {
                "offer constructive criticism": "Doug is critiquing the field's focus on resolution, suggesting that it doesn't necessarily lead to better answers, which is a critique intended to improve the field's approach.",
                "present new idea": "Doug introduces the idea that careful consideration of what needs to be quantified across space and time can significantly improve research outcomes, which is a novel concept in the context of the discussion."
            }
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "00:15-00:20",
            "transcript": "Great, thank you. Um, and last but not least, uh Lingyan.",
            "speaking_duration": 5,
            "nods_others": 0,
            "smile_self": 60.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "20:15",
            "end_time": "20:20",
            "annotations": {
                "acknowledge contribution": "Kristen Maitland acknowledges the previous speaker's contribution with 'Great, thank you.'",
                "encourage particpatioin": "Kristen Maitland invites Lingyan to participate by calling her name."
            }
        },
        {
            "speaker": "Lingyan Shi",
            "timestamp": "00:21-02:33",
            "transcript": "Hello? Yes. Hi, everyone. Uh my name is Lingyan Shi. Uh I'm from bioengineering department at UCSD. Uh I just established my lab in 2019. Uh basically we are developing, we are developing the optical imaging platform and we integrate the Raman based microscopy such as stimulated Raman microscopy and with the multiphoton fluorescence microscopy together. So this is a multimodality imaging system not only allow us to visualize the metabolic activities such as those small metabolites like glucose, amino acids or fatty acids, we can directly visualize them because the isotope we add onto those small metabolites will form the new chemical bond, which is uh for example carbon deuterium bond, so we don't need to do click chemistry to add the bulky fluorescence probe anymore. So this layer of metabolic information uh can be visualized at the same time we want to see for example the calcium fluorescence signal in the same region of interest. So this uh combined imaging platform can be used to solve some biological questions such as neurovascular coupling system. Uh for example we we are so interested in how neuron talk to those other type of cells such as endothelial cells on the vasculature system like the the uh blood brain barrier for example. So uh another layer of information that we couldn't really uh image because of the technical limitation is how the endothelial cell from the vasculature system signaling back to the the neuron and uh how these like feedback uh feedbacks the circuits that work. So uh if we combine both the Raman based imaging with the multiphoton fluorescence imaging together then we can visualize both layers of information at the same time specially and temporarily for in vivo imaging.",
            "speaking_duration": 132,
            "nods_others": 0,
            "smile_self": 20.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "20:21",
            "end_time": "22:33",
            "annotations": [
                {
                    "explain or define term or concept": "The speaker explains their multimodal imaging system, integrating Raman-based microscopy and multiphoton fluorescence microscopy, to visualize metabolic activities by adding isotopes to small metabolites, forming new chemical bonds."
                },
                {
                    "present new idea": "The speaker introduces their combined imaging platform and its application to solve biological questions like neurovascular coupling, which allows for visualizing metabolic activities and calcium fluorescence signals simultaneously."
                },
                {
                    "expand on existing idea": "The speaker expands on the capabilities of their imaging platform, explaining how it can visualize the signaling between endothelial cells and neurons, addressing a previously unimageable layer of information."
                }
            ]
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "02:33-02:35",
            "transcript": "Great, thank you.",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "22:33",
            "end_time": "22:35",
            "annotations": {
                "acknowledge contribution": "Kristen Maitland acknowledges the previous speaker's contribution."
            }
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "02:57-03:28",
            "transcript": "I'm interested uh if you don't mind I'm interested to hear a little bit more about Nick's application looking at cilia because it seems like for a lot of us we have this issue of of looking at things with fluorescence microscopes and as a consequence it's a lot about the signal to noise of the sensor and so forth. But I mean it seems like Nick has this interesting um system where he's able to look at cilia uh that maybe doesn't require those sorts of labeling so I'm curious what maybe that would be an an easier one to scale up speed with some kind of camera based method.",
            "speaking_duration": 31,
            "nods_others": 0,
            "smile_self": 10.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "22:57",
            "end_time": "23:28",
            "annotations": [
                {
                    "ask clarifying question": "Matt is asking Nick to elaborate on his application of looking at cilia, specifically how it might not require fluorescence labeling, which is a common issue for others."
                }
            ]
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "03:28-04:43",
            "transcript": "Yeah, so so thanks for showing some interest in it. to traditionally if you need to image cilia that are beating, which is the most challenging situation, they're beating at anywhere from 20 hertz to some extreme protest that live in the sea can beat up to 150 hertz. So that's that's a different story. And typically you're imaging them if you if you need to do temporal imaging, you need to image them at around 100 hertz to get reliable, you know, wave forms. And so we do that with DIC. So DIC microscopy with just a camera is the best way to track the ciliary wave form. Um, but then if you want to track trafficking within this bending whip like wave form, that's where you need to do fluorescence. And so what the field has done is that they've either taken cilia that are beating and immobilized them pharmacologically so that they're not beating and then track protein trafficking within them, but that's such a major perturbation that, you know, we still don't know what protein trafficking within a beating cilium looks like because of that. So one thing, yeah, I don't know, you know, now you can do the combined DIC fluorescence, you know, that's not a challenging technique, but getting enough signal is then that becomes the challenge. So getting enough signal from what could be one to five proteins and part of like a particle train, um, to get enough signal from that to reliably track it within the wave form would seem to be what would be needed.",
            "speaking_duration": 75,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "23:28",
            "end_time": "24:43",
            "annotations": [
                {
                    "acknowledge contribution": "Nick acknowledges Matt's interest in his application looking at cilia.",
                    "explain or define term or concept": "Nick explains that cilia beat at anywhere from 20 hertz to 150 hertz and that temporal imaging requires imaging at around 100 hertz to get reliable wave forms.",
                    "provide supporting evidence": "Nick provides evidence that DIC microscopy with just a camera is the best way to track the ciliary wave form.",
                    "expand on existing idea": "Nick expands on the challenges of tracking trafficking within the bending whip like wave form, stating that the field has immobilized cilia pharmacologically, which is a major perturbation, and that getting enough signal from proteins to reliably track it within the wave form is needed."
                }
            ]
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "04:48-04:57",
            "transcript": "So I mean with today's cameras like you know scientific cameras those frame rates are not out of reach, right? 100 hertz um is not out of reach of an SCM.",
            "speaking_duration": 9,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "24:48",
            "end_time": "24:57",
            "annotations": {
                "expand on existing idea": "Melike is building on Nick's discussion of imaging cilia, suggesting that modern scientific cameras can achieve the necessary frame rates (100 Hz) for imaging, thus expanding on the technical feasibility of Nick's imaging challenge."
            }
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "05:57-06:07",
            "transcript": "And can I ask how you're uh how are you looking at the cilia such that like where's your optical plane? Is it that you're looking through a bunch of cilia or you're looking along the length of some of them?",
            "speaking_duration": 10,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "25:57",
            "end_time": "26:07",
            "annotations": {
                "ask clarifying question": "Matt asks Nick to clarify how he is looking at the cilia, specifically inquiring about the optical plane and whether he's looking through a bunch of cilia or along their length, following Nick's explanation of his work with cilia."
            }
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "06:28-07:53",
            "transcript": "So yeah, they're going to depending on the organism. So like one one rapid prototyping approach would be to use tetrahymena, which have a thousand cilia and they're easy to manipulate and they're genetic and you can you can either immobilize them so they're because the other side of this is that if it's happening in a moving cell, there's another set of translation. So it's a beating cilia on a moving cell, so that would be a different thing. So you can immobilize them, um, the the cells, not the cilia. And then if you do that, you can get a glancing blow of the side cilia where you can image just through an individual one. Um, I can show an example if if you'd like, uh, I can I can share screen.",
            "speaking_duration": 85,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "26:28",
            "end_time": "27:53",
            "annotations": [
                {
                    "expand on existing idea": "Nick is expanding on his previous explanation of imaging cilia, providing details about using tetrahymena as a rapid prototyping approach."
                },
                {
                    "offer constructive criticism": "Nick is suggesting a way to simplify the imaging process by immobilizing the cells, which could improve the ability to image individual cilia."
                },
                {
                    "propose decision": "Nick proposes to show an example by sharing his screen, suggesting a concrete action for the group."
                }
            ]
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "08:44-09:46",
            "transcript": "See here. So I guess this this is a a bunch of cilliates swimming around, but that's not what we're talking about. So here is an example of of the DIC type images that you can get and these this is acquired at 600 frames per second. And so we can slow down the wave form and we can track it, but now imagine trying to track a particle moving within that. Um, that's that seems to be the the the problem and I haven't seen anybody even come close to doing it. Again, one thing that people would do is they would maybe treat this with a drug that makes the cilia stop beating and then track them, but you know, that's that's the barrier. So some way to combine the DIC with fluorescence at the 100 to 200 hertz imaging frame would seem to be what would be needed.",
            "speaking_duration": 62,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "Domenico (Nick) Galati shared his screen to show his lab website. He navigated to the microscopy section and showed a video of cilliates swimming around. He explained that the video was acquired at 600 frames per second.",
            "start_time": "28:44",
            "end_time": "29:46",
            "annotations": [
                {
                    "provide supporting evidence": "Nick provides an example of DIC type images acquired at 600 frames per second to support his explanation of how cilia movement can be tracked."
                },
                {
                    "explain or define term or concept": "Nick explains that DIC microscopy with a camera is the best way to track the ciliary wave form, clarifying the method he uses."
                },
                {
                    "present new idea": "Nick presents the idea of combining DIC with fluorescence at 100 to 200 hertz imaging frame as a potential solution to the problem of tracking particles within a beating cilium."
                }
            ]
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "09:47-09:58",
            "transcript": "So I mean with today's cameras like you know scientific cameras those frame rates are not out of reach, right? 100 hertz um is not out of reach of an SCM.",
            "speaking_duration": 11,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "29:47",
            "end_time": "29:58",
            "annotations": {
                "expand on existing idea": "Melike is building on Nick's previous statement about the challenges of imaging cilia beating at high frequencies, suggesting that modern scientific cameras can achieve the necessary frame rates, thus expanding on the discussion of imaging techniques.",
                "provide supporting evidence": "Melike provides supporting evidence by stating that scientific cameras can achieve frame rates of 100 hertz, which is relevant to Nick's challenge of imaging cilia beating at high frequencies."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "00:00-00:31",
            "transcript": "cameras frame rate. And so then the question is like, can you correct for that motion, right? Um, because when you're tracking your protein, you're going to have to um, um, find a way to to subtract the motion of the cilium, uh, from the motion of the protein itself. Um, is that motion of the cilium very stereotypical, like can you sort of characterize and and and correct for it?",
            "speaking_duration": 31,
            "nods_others": 0,
            "smile_self": 10,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "30:00",
            "end_time": "30:31",
            "annotations": [
                {
                    "expand on existing idea": "Melike is building upon Nick's explanation of the challenges in imaging cilia, specifically the need to track protein movement within the moving cilium, by suggesting a potential solution involving motion correction."
                },
                {
                    "ask clarifying question": "Melike asks if the motion of the cilium is stereotypical and can be characterized and corrected for, seeking clarification on the nature of the cilium's movement to inform potential solutions for tracking protein motion within it."
                }
            ]
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "00:32-01:13",
            "transcript": "I believe I I can't personally. I think the computational folks can. Um, certainly, I think that they can do it. Um, and but but I don't I can't do it. So that would be that that's a barrier right there is that trying to, you know, I think correct and straighten would be one way to do it so that you could, you know, take that curve waveform, turn it into a linear rod and then correct for it. So that's a that's an interesting idea. Um, so that would definitely be a computational approach. And then with the C stuff, that yeah, I use a a prime 95B scientific C and the frame rates aren't the issue. It's it is definitely getting the the signal for the protein of interest.",
            "speaking_duration": 41,
            "nods_others": 0,
            "smile_self": 33,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "30:32",
            "end_time": "31:13",
            "annotations": {
                "expand on existing idea": "Nick expands on Melike's idea of correcting for the motion of the cilium, suggesting that computational folks could correct and straighten the curved waveform into a linear rod.",
                "acknowledge contribution": "Nick acknowledges Melike's idea of correcting for the motion of the cilium as an interesting idea.",
                "explain or define term or concept": "Nick explains that he uses a prime 95B scientific C camera and that the frame rates are not the issue, but rather getting the signal for the protein of interest is the challenge."
            }
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "01:14-01:19",
            "transcript": "Is there any way to have a non fluorescence contrast agent against some of these proteins of interest?",
            "speaking_duration": 5,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "31:14",
            "end_time": "31:19",
            "annotations": {
                "ask clarifying question": "Matt asks if there is a way to use a non-fluorescent contrast agent against the proteins of interest, seeking alternative methods to address the signal-to-noise issue in fluorescence microscopy that Nick mentioned."
            }
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "01:20-01:44",
            "transcript": "Good thought. I don't know. That's a good thought. Um, you know, one thing that potentially maybe, I don't know. I don't know much about this quantitative phase imaging. But QPI might be one way to to kill two birds with one stone and that just avoid we wouldn't have molecular specificity, but even tracking one of the granules moving within that structure, maybe that would be a good QPI type approach.",
            "speaking_duration": 24,
            "nods_others": 0,
            "smile_self": 25,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "31:20",
            "end_time": "31:44",
            "annotations": {
                "acknowledge contribution": "The phrase \"Good thought\" acknowledges Matt's suggestion of using a non-fluorescence contrast agent.",
                "present new idea": "Nick introduces the idea of using Quantitative Phase Imaging (QPI) as a potential method to track granules within the cilia, which hasn't been discussed before.",
                "expand on existing idea": "Nick expands on the discussion of imaging techniques by suggesting QPI as a way to track granules, building on the previous discussion of DIC and fluorescence microscopy."
            }
        },
        {
            "speaker": "Douglas Shepherd",
            "timestamp": "01:45-02:58",
            "transcript": "So one issue with a lot of QPIs is it's multiple images. I mean there are ones that aren't, but um, typically you need to introduce some sort of diversity in the phase so then you can extract, you know, what the refracted index was. So you need to look at the image somehow in with multiple views. So this can be pretty low though for certain techniques and so the frame rate can still get pretty high. Um, the you know, one the issue is how much is it moving in 3D in like one sort of time step, right? So that that also so let's say you needed minimum three views, I'm just guessing, you know, you could make some technique, you know, how far is it going to displace between each of those three shots or do you need to come up with some sort of simultaneous multifocal technique, which exists. But you keep every time you do that you split the light, so you're really going to need transmitted light measurements, right where your photon, your excitation photons are doing the work, not your emission photons. Um, so I do think the QPA stuff is is definitely a really promising way to go there, but it does typically require some sort of re computational recombination of multiple views.",
            "speaking_duration": 73,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "31:45",
            "end_time": "32:58",
            "annotations": {
                "explain or define term or concept": "The speaker explains that QPI typically requires multiple images to introduce diversity in phase and extract the refractive index, clarifying the technique for others in the conversation.",
                "expand on existing idea": "The speaker builds on the idea of using QPI by discussing the issue of movement in 3D during the time step and suggesting potential solutions like simultaneous multifocal techniques, adding details and considerations to the initial suggestion.",
                "provide supporting evidence": "The speaker provides supporting evidence for the challenges of QPI by mentioning the need for multiple views and the potential for light splitting, which can impact the required transmitted light measurements."
            }
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "02:59-03:11",
            "transcript": "No, that makes sense. And so yeah, there there is also it's not a totally planar waveform. There is a three-dimensional rotary aspect to it as well. Um, the the most substantial translation is is planar, but then there is this little twist along with it.",
            "speaking_duration": 12,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "32:59",
            "end_time": "33:11",
            "annotations": {
                "acknowledge contribution": {
                    "Explanation": "The phrase \"No, that makes sense\" acknowledges Doug's prior explanation of QPI limitations."
                },
                "expand on existing idea": {
                    "Explanation": "Nick expands on the discussion of cilia movement by adding that the waveform is not totally planar and has a three-dimensional rotary aspect, building on Doug's comment about 3D movement."
                }
            }
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "03:11-03:22",
            "transcript": "And so the maybe the multifocal approach where I assume then you can use optics to get multiple focal planes simultaneously on the same camera or do you need different cameras?",
            "speaking_duration": 11,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "33:11",
            "end_time": "33:22",
            "annotations": {
                "ask clarifying question": "Nick is asking a clarifying question about the multifocal approach Doug mentioned, specifically whether multiple focal planes can be achieved simultaneously on the same camera or if different cameras are needed, building on the discussion of QPI and imaging challenges."
            }
        },
        {
            "speaker": "Douglas Shepherd",
            "timestamp": "03:23-03:57",
            "transcript": "There's there's multiple approaches. Um, the most rigorously optically corrected way would would sort of do it with some tricks in 4A space and then you would get different views on the same camera. You can also do a port a less sophisticated version where you literally just use a couple cameras and displace where the image is being formed out of each of them, but this has some aberration cost. And so there's a couple ways to do this. Um, and so, you know, the simplest one to build is this one where you just displace a couple cheap cameras, you know, so that they're looking at different positions. So,",
            "speaking_duration": 34,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "33:23",
            "end_time": "33:57",
            "annotations": {
                "expand on existing idea": "Doug is expanding on Nick's question about multifocal approaches by describing multiple ways to implement them, building on the previous discussion about imaging cilia.",
                "explain or define term or concept": "Doug explains the concept of using 'tricks in 4A space' for optical correction, and describes the less sophisticated version of displacing cameras, clarifying different approaches to multifocal imaging.",
                "provide supporting evidence": "Doug provides details about the aberration cost associated with the less sophisticated approach, supporting his explanation of the different methods."
            }
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "03:58-04:17",
            "transcript": "What about um, uh, I used to do a little bit of light field microscopy where you put a lenslet array, uh, so that you can get kind of multiple views in the same camera frame. But the issue is then it's really computationally expensive to deconvolve into an image and resolution is only so so.",
            "speaking_duration": 19,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "33:58",
            "end_time": "34:17",
            "annotations": {
                "present new idea": {
                    "Explanation": "Matt introduces the idea of using light field microscopy with a lenslet array to capture multiple views in the same camera frame, which hasn't been discussed before in this specific context."
                },
                "expand on existing idea": {
                    "Explanation": "Matt is expanding on the discussion of multifocal imaging techniques by introducing light field microscopy as another option."
                },
                "offer constructive criticism": {
                    "Explanation": "Matt points out the limitations of light field microscopy, specifically the computational expense of deconvolution and the limited resolution, which is a critique of the method's practicality."
                }
            }
        },
        {
            "speaker": "Douglas Shepherd",
            "timestamp": "04:19-04:44",
            "transcript": "I mean, I think those methods are getting a lot better. Um, they're very similar in spirit to the also different views for QPI. So, so either way you're talking about somehow combining something that has a different view of the image to then try and reconstruct it in 3D, right? The nice part about the QPI is it gets the refractive index and that's still a bit tricky to get with a light field setup. So, um, yeah.",
            "speaking_duration": 25,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "34:19",
            "end_time": "34:44",
            "annotations": {
                "expand on existing idea": "Doug is expanding on Matt's suggestion of using light field microscopy by comparing it to QPI and discussing the similarities in combining different views for 3D reconstruction, building upon the discussion of imaging techniques for Nick's cilia application.",
                "provide supporting evidence": "Doug supports his point by stating that QPI provides the refractive index, which is difficult to obtain with a light field setup, providing a reason why QPI might be a better approach.",
                "explain or define term or concept": "Doug explains that both light field microscopy and QPI involve combining different views of an image to reconstruct it in 3D, clarifying the underlying principle of these techniques in the context of the discussion."
            }
        },
        {
            "speaker": "Arnold Hayer",
            "timestamp": "04:49-05:00",
            "transcript": "Can I ask a question about the the transport phenomenon that you're interested in looking at inside the cilia? Is that motor based transport vesicular or free diffusion? That might also be important for for the speed that you need to have in order to pick up the movement.",
            "speaking_duration": 11,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "34:49",
            "end_time": "35:00",
            "annotations": {
                "ask clarifying question": "Arnold asks Nick a question to clarify the type of transport within the cilia that Nick is interested in, specifically whether it's motor-based, vesicular, or free diffusion, to better understand the speed requirements for imaging."
            }
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "05:05-05:11",
            "transcript": "Yeah, it's microtubial motor based and so it's thought to occur around the micron per second type range.",
            "speaking_duration": 6,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "35:05",
            "end_time": "35:11",
            "annotations": {
                "expand on existing idea": "This utterance expands on Arnold's question about the transport phenomenon within cilia, providing details about the mechanism (microtubial motor based) and speed (micron per second range) of the transport."
            }
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "05:11-06:17",
            "transcript": "And it's it's it's kind of fascinating. So cryo EM has actually resolved. So there's two microtubules. There's there's 18 microtubules within the cilium and they're arranged in this really beautiful structure called an axoneme. And what people have figured out with cryo EM with just static snapshots of frozen cilia is that one set of motors walk up the A microtubule and another set of motors walk down the B microtubule. Um, and so that's what we have at the electron microscopy level is that there's actually a highway and you know, the plus end directed ones are going on the A tubule and the minus end directed ones are going down the B tubule.",
            "speaking_duration": 66,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "35:11",
            "end_time": "36:17",
            "annotations": [
                {
                    "provide supporting evidence": "The speaker provides evidence from cryo EM studies to support their description of microtubule motor-based transport within cilia, building on the previous discussion about imaging challenges and potential solutions."
                },
                {
                    "explain or define term or concept": "The speaker explains the structure of microtubules within the cilium, including the arrangement of the axoneme and the direction of motor movement on A and B tubules, to provide context for the discussion on cilia imaging."
                },
                {
                    "express enthusiasm": "The speaker expresses enthusiasm about the topic of microtubule motor-based transport within cilia, indicating their interest in the subject."
                }
            ]
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "06:17-06:43",
            "transcript": "Can I ask a quick question? Um, so you had mentioned previously that a lot of your issue is um, kind of being photon starved and not being able to get enough light in the end of the day. Is that because of the labeling or is it because of the optical system and somewhere you're throwing out a lot of the light? What would what is the kind of cause?",
            "speaking_duration": 26,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "36:17",
            "end_time": "36:43",
            "annotations": {
                "ask clarifying question": "Nick is asking a question to clarify the cause of the photon starvation issue that was previously mentioned, inquiring whether it's due to labeling or the optical system."
            }
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "06:44-07:27",
            "transcript": "Yeah, so I mean it's a good I I I don't quite know the answer. My assumption is is that it's a little bit of both. And so like people have only really tried to do this because with with wide field, right? I think it's it's generally like a wide field approach because we do want the most number of photons and we want speed. So camera based wide field analysis is kind of the standard approach. And so, you know, we're we're we could label brighter, we could try to, you know, you know, I guess wide field with deconvolution would probably be the next step. So just to do simple deconvolution would probably be the next step, but beyond that, I I don't know where the photons. I don't know, you know, we can try just going brighter. We're using typical FPs like GFP and and neon, which is pretty bright, but so could be a combination of both.",
            "speaking_duration": 43,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "36:44",
            "end_time": "37:27",
            "annotations": [
                {
                    "explain or define term or concept": "The speaker explains that camera-based widefield analysis is the standard approach for imaging cilia because it allows for the most photons and speed, clarifying the current methodology in the field."
                },
                {
                    "expand on existing idea": "The speaker expands on the problem of photon starvation by suggesting potential solutions like brighter labeling and widefield deconvolution, building upon the previous discussion about the challenges of imaging cilia."
                }
            ]
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "07:27-07:44",
            "transcript": "Maybe I'll add the other factor is you can't just blast it with more light because it's not good for the cell or the, you know, so you have the issues there. It's the tradeoff of illumination without damage, collection of signal very fast, so you're limiting the time that you can capture those photons coming out.",
            "speaking_duration": 17,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "37:27",
            "end_time": "37:44",
            "annotations": {
                "provide supporting evidence": "Kristen Maitland provides supporting evidence by highlighting the limitation of increasing light intensity due to potential cell damage, which supports the discussion about challenges in fluorescence microscopy and signal acquisition.",
                "explain or define term or concept": "Kristen Maitland explains the concept of the trade-off between illumination intensity and cellular damage, which is relevant to the ongoing discussion about imaging techniques.",
                "expand on existing idea": "Kristen Maitland expands on the existing idea of photon starvation by adding the constraint of phototoxicity, which is a critical factor in live-cell imaging."
            }
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "07:45-09:52",
            "transcript": "So it's like we almost had a case study, right? Nick's uh um Celia tracking and uh molecular molecule or or granular tracking within the Celia. Uh I wanted to um go back to there's a little bit of a shared theme between uh Arnold uh and Melika's um introduction, right? In both cases, I think um they talked about for example, Arnold talk about um uh temporarily, you know, you do slow imaging and then uh zoom in. You you you know, get something you're interested and you do fast imaging focusing on uh the processes that you really want to study in detail. And Melika talked about spatially, uh maybe lower resolution imaging and then you find something interesting and zoom in do uh super resolution that really high resolution imaging. I found that that shared theme very interesting. Um, do we want to um as a group talk a little bit about that what are the challenges um and and um there's a little bit of a right? uh AI guided uh automatic zooming in uh that's already been done what what's people uh experience?",
            "speaking_duration": 127,
            "nods_others": 0,
            "smile_self": 75,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "37:45",
            "end_time": "39:52",
            "annotations": [
                {
                    "acknowledge contribution": "Jin Zhang acknowledges Nick's contribution by referring to his cilia tracking as a 'case study', recognizing the value of his input to the discussion."
                },
                {
                    "present new idea": "Jin Zhang introduces the idea of discussing the shared theme between Arnold and Melika's introductions, which involves adapting imaging resolution based on the area of interest."
                },
                {
                    "encourage particpatioin": "Jin Zhang encourages participation by asking the group to talk about the challenges and experiences related to AI-guided automatic zooming in during imaging."
                }
            ]
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "00:00-00:03",
            "transcript": "and um we want to discuss a little bit more about that.",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 50.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "40:00",
            "end_time": "40:03",
            "annotations": {
                "encourage particpatioin": "This utterance encourages further discussion on the topic of AI-guided zooming in for imaging, building on the previous suggestion to discuss the challenges and experiences related to this theme."
            }
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "00:03-00:34",
            "transcript": "Yeah, I put that in. I I agree. I put that into the shared Google Doc because I thought that did seem like a common uh approach. Yeah, so is that does that end up being more of a discussion of how to design um a microscope that integrates its hardware with with uh like a machine learning online approach or or just a user guided approach. I mean, I I I don't know if uh the others have ideas about that.",
            "speaking_duration": 31,
            "nods_others": 1,
            "smile_self": 32.25,
            "smile_other": 3.22,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "40:03",
            "end_time": "40:34",
            "annotations": [
                {
                    "express agreement": "Matt agrees with Jin Zhang's observation about a shared theme in Arnold and Melike's introductions, indicating agreement with a proposed idea.",
                    "ask clarifying question": "Matt asks if the discussion should focus on designing a microscope that integrates hardware with machine learning or a user-guided approach, seeking clarification on the direction of the discussion."
                }
            ]
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "00:34-01:34",
            "transcript": "I mean, for what I mentioned, I think you do need an automated sort of machine learning guided approach. A user guided approach is usually too time consuming and you know, um low throughput again. Um and so you need something that um, you know, can integrate hardware with like recognition of what the image is telling you to guide the hardware um to the right field of view to the right focal plane. Um and change between objectives, right? Going from low magnification, low NA to high mag, high NA. Um and and then, you know, focusing and zooming into the right spot and changing modalities of imaging.",
            "speaking_duration": 60,
            "nods_others": 1,
            "smile_self": 15.0,
            "smile_other": 1.67,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "40:34",
            "end_time": "41:34",
            "annotations": {
                "expand on existing idea": "Melike expands on the idea of zooming in on interesting regions of a sample for higher resolution imaging, which was previously discussed by Arnold and herself, by suggesting an automated, machine learning-guided approach.",
                "propose decision": "Melike proposes the need for an automated, machine learning-guided approach for microscopy.",
                "provide supporting evidence": "Melike supports the need for an automated approach by stating that a user-guided approach is too time-consuming and has low throughput."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "01:34-01:34",
            "transcript": "I'm back.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "41:34",
            "end_time": "41:34",
            "annotations": [
                {
                    "None": "This utterance does not fit any of the codes in the codebook."
                }
            ]
        },
        {
            "speaker": "Arnold Hayer",
            "timestamp": "01:37-02:02",
            "transcript": "I guess the feature detection is something that one could not develop as a universal tool because the features might very context and question be question dependent. So one would have to develop algorithms of being able to detect those rare events or you know, structural arrangements of signals and uh feed that back into into a response of the microscope.",
            "speaking_duration": 25,
            "nods_others": 1,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "41:37",
            "end_time": "42:02",
            "annotations": {
                "expand on existing idea": "Arnold is expanding on the idea of AI-guided automated zooming, adding that feature detection algorithms would need to be context-specific.",
                "offer constructive criticism": "Arnold is offering constructive criticism by pointing out the limitations of a universal feature detection tool and suggesting the need for context-specific algorithms."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "02:03-02:17",
            "transcript": "Yeah, may be very application dependent, right? But maybe could be something you could train, right? For your specific application.",
            "speaking_duration": 14,
            "nods_others": 0,
            "smile_self": 50.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "42:03",
            "end_time": "42:17",
            "annotations": {
                "expand on existing idea": "Melike is expanding on Arnold's idea that feature detection is context-dependent, suggesting that while it may be application-specific, it could be trained for a particular application, building upon the previous discussion about AI-guided zooming.",
                "express agreement": "Melike agrees with Arnold's point that feature detection is context-dependent."
            }
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "02:19-02:21",
            "transcript": "Is this is Oh sorry, go ahead please.",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "Yes",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "42:19",
            "end_time": "42:21",
            "annotations": [
                {
                    "encourage particpatioin": "Matt encourages someone else to speak, as he says 'go ahead please' after starting to speak himself."
                }
            ]
        },
        {
            "speaker": "Aseema Mohanty",
            "timestamp": "02:22-02:41",
            "transcript": "Is there any value in um kind of like sparsely randomly checking super high resolution and then, you know, you get a feel for what's going on on the large scale. I mean, I'm not sure for your applications, but that could be an approach as well.",
            "speaking_duration": 19,
            "nods_others": 0,
            "smile_self": 78.95,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "42:22",
            "end_time": "42:41",
            "annotations": {
                "present new idea": "Aseema suggests a novel approach of sparsely and randomly checking super high resolution to understand the large-scale context, which hasn't been explicitly discussed before.",
                "encourage particpatioin": "Aseema ends her suggestion by saying \"I mean, I'm not sure for your applications, but that could be an approach as well\", inviting others to consider its applicability to their specific problems."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "02:42-04:14",
            "transcript": "So, I mean, I guess just to give you an idea, right? When we do high resolution imaging because we're using 100x objective, um our field of view is very limited, right? Um, and so the throughput is very low. We image one cell at a time. Um, now with a CMOS camera is again very increasing a bit the field of view and the throughput, um but it's a couple of cells at a time. And that um, you know, limits you in terms of um, you know, if you have rare populations in your sample, will you ever sample them um in your imaging if you're only imaging 10 cells in and again it's a slow imaging modality, you image one cell in every uh, you know, 20 minutes, 15 minutes, um something like that. And so, um, you know, one approach is again like make it automated so that you can image um thousands of cells um automatically. Um, and again randomly sample or maybe a more sort of intelligent approach would be uh maybe there are low resolution features that, you know, um mark those rare populations that you can um find um with a higher throughput approach then you can assume it to those with high resolution uh right rather than just randomly sampling and hoping you will find one of those cells in your in your image. I don't know if that uh answers your question.",
            "speaking_duration": 92,
            "nods_others": 0,
            "smile_self": 10.87,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "42:42",
            "end_time": "44:14",
            "annotations": {
                "explain or define term or concept": "The speaker explains the limitations of high-resolution imaging, such as the limited field of view and low throughput, because they are using a 100x objective.",
                "expand on existing idea": "The speaker expands on the idea of AI-guided zooming by discussing the limitations of current high-resolution imaging techniques and suggesting that automation could help image thousands of cells automatically, either randomly or through a more intelligent approach using low-resolution features to identify rare populations.",
                "provide supporting evidence": "The speaker provides supporting evidence for the need for automated imaging by explaining that with current methods, they can only image a couple of cells at a time, which limits the ability to find rare populations, and it takes 15-20 minutes to image one cell."
            }
        },
        {
            "speaker": "Aseema Mohanty",
            "timestamp": "04:14-04:14",
            "transcript": "Yeah, absolutely.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "44:14",
            "end_time": "44:14",
            "annotations": {
                "express agreement": "The speaker explicitly agrees with the previous statement about the need for intelligent approaches to find rare populations in high-resolution imaging."
            }
        },
        {
            "speaker": "Douglas Shepherd",
            "timestamp": "04:18-05:07",
            "transcript": "So, one thing I'm curious about for these applications is we just had the a bit of the label free discussion. And so these tend to be much less phototoxic, but you lack specificity. You don't have molecular labeling. So, how hard is it to know if the event in your case, I guess specifically, I know generalizing is difficult, um that the event would have started occurring if you had a label free measurement that was lower resolution and then you could switch over to doing some sort of super res. I mean that requires integrating across data modalities, but it's it's pretty easy to integrate some of these label free methods in an opposing arm on the microscope. And so then you could try and image that way and then switch over. So, but the question is, will you actually know it's happening with a label free method if you don't have the molecular readout and I don't know if that's the case or not.",
            "speaking_duration": 49,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "44:18",
            "end_time": "45:07",
            "annotations": {
                "ask clarifying question": {
                    "Explanation": "Doug asks how hard it is to know if an event would have started occurring with a label-free measurement before switching to super-resolution, seeking clarification on the feasibility of this approach given the lack of molecular labeling, building on the previous discussion about label-free methods."
                },
                "expand on existing idea": {
                    "Explanation": "Doug expands on the idea of integrating label-free methods with super-resolution imaging, suggesting it's easy to integrate label-free methods and proposing to image that way and then switch over, building on the previous discussion about label-free methods."
                }
            }
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "05:11-05:30",
            "transcript": "Couldn't you in principle just um, I mean if you're training a model to be able to identify these things, you could just collect enough data where you could predict at least with some reasonable degree of accuracy, you could predict something from a label free method based on fluorescence detection as well and then use that to guide further experiments.",
            "speaking_duration": 19,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "45:11",
            "end_time": "45:30",
            "annotations": {
                "present new idea": "Matt introduces the idea of training a model to predict events from label-free methods based on fluorescence detection, which could then guide further experiments, a novel approach not previously discussed.",
                "expand on existing idea": "This utterance expands on the discussion of integrating label-free methods with super-resolution imaging by suggesting a way to use label-free data to predict events detectable by fluorescence, building upon the previous discussion about label-free methods and their limitations.",
                "propose decision": "Matt proposes the decision to collect enough data to train a model that can predict events from label-free methods based on fluorescence detection, suggesting a concrete action for the group to consider."
            }
        },
        {
            "speaker": "Douglas Shepherd",
            "timestamp": "05:30-06:07",
            "transcript": "Yeah, I think I in principle I agree with that, but let's say that you're interested in, you know, transcription factor searching, right? You're never going to know if like a certain transcription factor has been shuttled to the nucleus most of the time from a label free measurement from a stimuli. So I I think there's cases it'll work, there's cases it won't work and I guess this is where I'm trying to figure out kind of, you know, where the value in it is.",
            "speaking_duration": 37,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "45:30",
            "end_time": "46:07",
            "annotations": {
                "expand on existing idea": "Doug is building on Matt's idea of using machine learning to predict from label-free methods, but he is adding a caveat that it might not work in all cases, specifically mentioning transcription factor searching as an example where label-free measurements might not be sufficient.",
                "offer constructive criticism": "Doug is offering constructive criticism by pointing out the limitations of using label-free methods for certain applications, suggesting that it might not always be possible to accurately predict events without molecular labeling, which is intended to improve the discussion and consider the limitations of the proposed approach."
            }
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "06:08-07:02",
            "transcript": "I think that may actually um, you know, perhaps this is where we could go into the multi, you know, modality um discussion, right? Uh for a label free based modality uh and um optical and some of the other modalities uh can they be um integrated uh and if so um computationally or experimentally uh do we, you know, have um, you know, sort of a registration or internal reference uh or do we need to have that uh to connect and integrate imaging data from different modalities. And from live cells versus fixed cells as well. Um this is a question.",
            "speaking_duration": 54,
            "nods_others": 0,
            "smile_self": 33.33,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "46:08",
            "end_time": "47:02",
            "annotations": {
                "propose decision": "The speaker proposes to discuss the integration of multi-modalities, including label-free and optical, computationally or experimentally, which suggests a concrete choice for the group to focus on.",
                "ask clarifying question": "The speaker asks whether registration or internal reference is needed to connect and integrate imaging data from different modalities and from live cells versus fixed cells, requesting further discussion and clarification on the topic."
            }
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "07:05-07:50",
            "transcript": "this is yeah, as I mentioned earlier, this has worked really well for me to move from live tissue where we look at neural activity during behavior to fixed samples where we can do single molecule fish even in different microscopes. It's as long as we can find the same cells, we can register back and forth between the two data sets. And some of that is by virtue of the fact that the larval zebrafish is small and it's easy to get gross level registration and then um the fine cellular level registration isn't such a problem when things are broadly overlapped.",
            "speaking_duration": 45,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "47:05",
            "end_time": "47:50",
            "annotations": {
                "provide supporting evidence": "Matt provides evidence from his own work to support the idea of integrating data from different modalities, mentioning his success in registering data from live and fixed samples by finding the same cells."
            }
        },
        {
            "speaker": "Arnold Hayer",
            "timestamp": "07:50-08:57",
            "transcript": "So one challenge that we face and uh that is related to using biosensors reporters for subcellular activities in migrating cells is that usually we can't do more than one or two reporters um at the same time. But perhaps you would like to know what five or 10 different things are doing in a specific event. So what we've come up with is is try to make cells behave in a very stereotypic way. For example, move along a a track of extracellular matrix which then has a turn. So you can have a cell that migrates along that track and then it has to turn and then you can especially temporarily analyze what's happening during this specific turning process. And then you have different cells, you you don't use the same cell but you use different cells in a stereotypical behavior um to register the different molecular events uh over time. So that's that's something that, you know, we're we're trying to um establish more and there there certainly with with microfabrication there's there ways of of uh forcing cells into specific behaviors and and helping with that issue. goes back to this multiplexing uh problem of spatial temporal analysis.",
            "speaking_duration": 67,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "47:50",
            "end_time": "48:57",
            "annotations": [
                {
                    "present new idea": "Arnold introduces a new approach to address the challenge of limited multiplexing in biosensor experiments by making cells behave in a stereotypic way, allowing for spatial and temporal analysis of specific events across different cells."
                },
                {
                    "expand on existing idea": "Arnold expands on the idea of using microfabrication to force cells into specific behaviors, which helps address the multiplexing problem in spatial temporal analysis, building on the previous discussion of challenges in imaging and analysis."
                }
            ]
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "09:08-09:39",
            "transcript": "Could you uh could you also have something where, you know, you have a bunch of different sensors at once in the same cell, but they may be somewhat broadly distributed and then they are each tagged with a barcode and even if they're all in the same color, then afterwards you could fix the sample and do some kind of uh fixed tissue labeling or multi round fixed tissue labeling to identify based on the barcode what what what sensor it was even though they were all, you know, green at the time or something like that.",
            "speaking_duration": 31,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "49:08",
            "end_time": "49:39",
            "annotations": {
                "present new idea": "Matt introduces a new idea of using multiple sensors tagged with barcodes in the same cell, which can be identified later through fixed tissue labeling, to address the multiplexing problem discussed by Arnold."
            }
        },
        {
            "speaker": "Arnold Hayer",
            "timestamp": "09:39-09:56",
            "transcript": "Okay, okay. Yeah, yeah, I see I see what you mean. So you would have you would multiplex the uh the acquisition and then later basically deconvolve and decide who was who was who in the end. Yeah, that's an interesting idea. We haven't we haven't thought about that yet, but that could could definitely work.",
            "speaking_duration": 17,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "49:39",
            "end_time": "49:56",
            "annotations": {
                "acknowledge contribution": {
                    "Explanation": "Arnold acknowledges Matt's suggestion of using barcodes to multiplex sensors and deconvolve them later, showing he understands the idea."
                },
                "expand on existing idea": {
                    "Explanation": "Arnold builds on Matt's idea by summarizing the approach of multiplexing acquisition and deconvolving later to identify the sensors."
                },
                "express agreement": {
                    "Explanation": "Arnold explicitly agrees that Matt's idea could potentially work for multiplexing and deconvolving sensors."
                }
            }
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "09:56-10:01",
            "transcript": "I guess you need to sort of link that that read out to that barcode.",
            "speaking_duration": 5,
            "nods_others": 0,
            "smile_self": 40.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "49:56",
            "end_time": "50:01",
            "annotations": {
                "expand on existing idea": "This utterance builds upon Matt's idea of using barcodes to identify different sensors in cells, adding the detail that the readout needs to be linked to the barcode."
            }
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "00:00-00:01",
            "transcript": "code somehow.",
            "speaking_duration": 1.0,
            "nods_others": 0,
            "smile_self": 100.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "50:00",
            "end_time": "50:01",
            "annotations": {
                "expand on existing idea": "This utterance builds upon the previously mentioned idea of using barcodes to identify different sensors within cells, adding a detail about linking the readout to the barcode."
            }
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "00:05-00:27",
            "transcript": "Yeah, I was thinking again, I mean based just on my own experience of doing it with an in situ hybridization approach afterwards once the cells are fixed, but it would it assumes that you can register between the live data where everything is green and the fixed data where you can disaggregate who's who. Um and I I mean yeah, I don't know how how feasible that would be within that type of cell.",
            "speaking_duration": 22.0,
            "nods_others": 1,
            "smile_self": 0.0,
            "smile_other": 13.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "50:05",
            "end_time": "50:27",
            "annotations": {
                "expand on existing idea": "Matt is expanding on Arnold's idea of using different cells in a stereotypical behavior to register different molecular events over time, by suggesting a method to multiplex the acquisition and then later deconvolve and decide who was who in the end.",
                "provide supporting evidence": "Matt provides supporting evidence based on his own experience of doing it with an in situ hybridization approach afterwards once the cells are fixed, to support the feasibility of his idea.",
                "ask clarifying question": "Matt ends by questioning the feasibility of his idea within that type of cell, seeking further input or validation."
            }
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "00:28-01:22",
            "transcript": "So uh Andrew actually put uh something in the chat. Uh so there are perhaps uh optogenetic tools that could uh tattoo a cell. That's uh that's an idea as well. Uh related to Arnold um comment the the computational multiplexing or kind of an internal reference. I think that has been used also in a lot of other settings, for example, in cell migration, right? Like the the rather than turn like the the the edge of the cell could serve as a internal reference in some context as well. So I guess related to uh either the optogenetic tattooing or some cell features that serve as internal reference. Um can we think about linking different modalities? Uh I think Lingyan has something to say.",
            "speaking_duration": 54.0,
            "nods_others": 2,
            "smile_self": 70.0,
            "smile_other": 10.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "50:28",
            "end_time": "51:22",
            "annotations": [
                {
                    "acknowledge contribution": {
                        "Explanation": "The speaker acknowledges Andrew's contribution by mentioning that he put something in the chat."
                    }
                },
                {
                    "present new idea": {
                        "Explanation": "The speaker introduces the idea of using optogenetic tools to tattoo a cell, which is a novel concept in the context of the discussion."
                    }
                },
                {
                    "expand on existing idea": {
                        "Explanation": "The speaker expands on Arnold's comment about computational multiplexing by suggesting that cell features can serve as an internal reference, building upon the previous discussion."
                    }
                },
                {
                    "encourage particpatioin": {
                        "Explanation": "The speaker encourages Lingyan to participate by stating that she has something to say, inviting her to contribute to the discussion."
                    }
                }
            ]
        },
        {
            "speaker": "Lingyan Shi",
            "timestamp": "01:23-01:53",
            "transcript": "Yes, yes. Uh I think another modality, actually I like Matt idea about the barcoding. But the barcoding you mentioned maybe it's fluorescence fluorescence related. And I think there is a possibility that we do barcoding with Raman Raman based technique.",
            "speaking_duration": 30.0,
            "nods_others": 1,
            "smile_self": 90.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Raising Hand",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "51:23",
            "end_time": "51:53",
            "annotations": [
                {
                    "express agreement": "Lingyan expresses agreement with Matt's idea about barcoding, acknowledging his contribution to the discussion.",
                    "present new idea": "Lingyan presents a new idea to use Raman-based techniques for barcoding, which is a different modality than the fluorescence-based barcoding previously mentioned."
                }
            ]
        },
        {
            "speaker": "Lingyan Shi",
            "timestamp": "01:53-02:42",
            "transcript": "Oh, even without the barcoding technique, we can do hyperspectral hyper hyperspectral imaging with the Raman based technology. So that means if we can speed up the imaging collection, then we have a stack of image that covers a certain spectrum. So each molecule, each molecule that we want to look at have its own spectrum profile.",
            "speaking_duration": 49.0,
            "nods_others": 1,
            "smile_self": 90.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Raising Hand",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "51:53",
            "end_time": "52:42",
            "annotations": [
                {
                    "expand on existing idea": "Lingyan is expanding on Matt's idea of barcoding by suggesting that hyperspectral imaging with Raman-based technology can be used even without barcoding, building upon the discussion of multiplexing and linking different modalities.",
                    "explain or define term or concept": "Lingyan explains that hyperspectral imaging with Raman-based technology involves collecting a stack of images covering a certain spectrum, where each molecule has its own unique spectral profile, providing clarity on the technique's capabilities.",
                    "provide supporting evidence": "Lingyan provides supporting evidence for using hyperspectral imaging by stating that each molecule has its own spectrum profile, strengthening the idea that this technique can be used to identify different molecules."
                }
            ]
        },
        {
            "speaker": "Lingyan Shi",
            "timestamp": "02:42-03:54",
            "transcript": "So if the the image stack, the hyperspectral image stack have for example uh 512 multiplied by 512 pixels and each pixel have its own spectrum information covered. And that allow us to do computational algorithm to do clustering, clustering out the same similar spectrum groups of the pixels. So if group one have this same Raman spectrum, then these pixels will be assigned to uh for example red color, one color. And group two, we we cluster out again and we assign different second color. Uh then there's no limitation as long as we can group out uh a group of of spectrum profile, we can assign a color for that specific group of molecule. So the the in the end, each pixel, there is a dominating molecule signal. Dominating molecule signal is um the the the final assigned color.",
            "speaking_duration": 72.0,
            "nods_others": 0,
            "smile_self": 90.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Raising Hand",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "52:42",
            "end_time": "53:54",
            "annotations": [
                {
                    "explain or define term or concept": "Lingyan is explaining how hyperspectral imaging with Raman-based technology works, specifically how each pixel in the image stack contains spectrum information that can be clustered and assigned a color based on the dominating molecule signal."
                }
            ]
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "03:55-03:56",
            "transcript": "Can I ask a question about that?",
            "speaking_duration": 1.0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "53:55",
            "end_time": "53:56",
            "annotations": {
                "ask clarifying question": "Nick asks if he can ask a question, likely to get clarification on Lingyan's previous statement about Raman-based barcoding and hyperspectral imaging."
            }
        },
        {
            "speaker": "Domenico (Nick) Galati",
            "timestamp": "03:56-04:42",
            "transcript": "I I was wondering because because we I've started looking into doing a little bit of Raman for another project because we have a spectroscopist and we just got a new Renishaw imaging Raman. And I'm curious, is there a way to if you wanted to label so that you could mark a subcellular compartment with fluorescence, right? Because maybe, you know, you're you're you know, I know you can you can you can separate mitochondria and nucleus, you know, things big structures really well. But if you wanted to label something like say a cilium so that you could identify it with fluorescence and then image the cilium with Raman, how can you separate out the fluorescence from the Raman signal? And if so, like that that seems like a really cool multimodal approach to be able to say here's a cellular compartment. Now what is the biochemical makeup of this compartment would be something that could be really interesting.",
            "speaking_duration": 46.0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "53:56",
            "end_time": "54:42",
            "annotations": [
                {
                    "ask clarifying question": "Nick is asking if there is a way to separate fluorescence from Raman signal when labeling a subcellular compartment, seeking clarification on how to combine these imaging modalities."
                },
                {
                    "present new idea": "Nick is presenting a new idea of combining fluorescence labeling with Raman imaging to identify a cellular compartment and analyze its biochemical makeup."
                }
            ]
        },
        {
            "speaker": "Lingyan Shi",
            "timestamp": "04:42-06:14",
            "transcript": "Yeah, so that is exactly what what I just talked about in the beginning. Uh for example the the neuro neurovascular coupling system that I was talking about, you have the calcium signal which is fluorescence based. So it's a photon that emitted from the molecule. But at the same time, we look at the same region of interest with Raman signal. So that is chemical bound vibrational modes. So we can collect different modalities in the same you know even same region of interest and it doesn't influence each other because it's it's different imaging modality. So then for for that do you do you use like the regular 488 laser for Gcamp excitation and then you use like an infrared laser for the Raman signal? Is that what you do?",
            "speaking_duration": 92.0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Raising Hand",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "54:42",
            "end_time": "56:14",
            "annotations": [
                {
                    "explain or define term or concept": "The speaker explains that Raman signal is chemical bound vibrational modes, clarifying the concept for the listener."
                },
                {
                    "ask clarifying question": "The speaker asks a question about the specific lasers used for Gcamp excitation and Raman signal, seeking clarification on the experimental setup."
                }
            ]
        },
        {
            "speaker": "Lingyan Shi",
            "timestamp": "06:14-06:15",
            "transcript": "Uh I do two photon fluorescence.",
            "speaking_duration": 1.0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:14",
            "end_time": "56:15",
            "annotations": {
                "explain or define term or concept": "Lingyan is responding to Nick's question about how to separate fluorescence from Raman signals, and clarifies that she uses two-photon fluorescence, which is a specific type of fluorescence microscopy."
            }
        },
        {
            "speaker": "Lingyan Shi",
            "timestamp": "06:15-06:15",
            "transcript": "Lingyan Shi_UCSD has started screen sharing",
            "speaking_duration": 0.0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "None",
            "start_time": "56:15",
            "end_time": "56:15",
            "annotations": {
                "None": "This utterance is a statement of action and does not fit any of the codes."
            }
        },
        {
            "speaker": "Lingyan Shi",
            "timestamp": "06:16-08:24",
            "transcript": "So which means that because the focus plan will be a little bit different if you use visible laser instead of near infrared laser. So I use the same laser to do the the pump for the size imaging, but use the same wavelength to do the two photon fluorescence for for Gcamp or for other fluorescence signal. So other fluorescence signal can be uh can be imaged by two photon fluorescence very well. For example the the the one that we usually talk about like label free NADH or flavor molecules and we can quickly just image with two photon fluorescence in the same you know even same region of interest. And it doesn't influence each other because it's it's different imaging modality.",
            "speaking_duration": 128.0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows a PowerPoint presentation titled \"Aging La Jolla (Autosaved)\". The current slide is titled \"Label Free SRS hyperspectral image\" and contains a diagram illustrating the hyperspectral imaging process, an intensity plot of a highlighted pixel, and several images showing different Raman shifts.",
            "start_time": "56:16",
            "end_time": "58:24",
            "annotations": [
                {
                    "explain or define term or concept": "The speaker explains that using different lasers (visible vs. near-infrared) will result in slightly different focal planes, clarifying a technical aspect of their imaging setup in response to a question about separating fluorescence and Raman signals."
                },
                {
                    "expand on existing idea": "The speaker expands on their previous explanation of their imaging setup by detailing how they use the same laser wavelength for both SRS imaging and two-photon fluorescence, building on the discussion of combining Raman and fluorescence imaging."
                },
                {
                    "provide supporting evidence": "The speaker provides supporting evidence for the feasibility of their approach by mentioning that label-free NADH or flavin molecules can be quickly imaged with two-photon fluorescence in the same region of interest, supporting the idea of combining different imaging modalities."
                }
            ]
        },
        {
            "speaker": "Joyoni Dey",
            "timestamp": "09:24-09:59",
            "transcript": "Do we have a two minutes to discuss another quick idea? I want to ask Dr. uh Somarco, Dr. Mimi. Yes, so uh I have a little bit of experience with tumor modeling with ODE, you know, advective reactive equations and I have incorporated with that, you know, necrosis um and also uh partial oxygen oxygen partial pressure.",
            "speaking_duration": 35.0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "59:24",
            "end_time": "59:59",
            "annotations": [
                {
                    "present new idea": "Joyoni introduces a new idea to discuss tumor modeling with ODE, which hasn't been mentioned before in the conversation.",
                    "encourage particpatioin": "Joyoni encourages participation by asking Dr. Somarco to discuss the idea."
                }
            ]
        },
        {
            "speaker": "Joyoni Dey",
            "timestamp": "00:00-00:01",
            "transcript": "model will help.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "60:00",
            "end_time": "60:01",
            "annotations": [
                {
                    "provide supporting evidence": "Joyoni Dey is offering their experience with tumor modeling as a potential aid to Dr. Mimi's work on skeletal regeneration, suggesting that their model could be helpful."
                }
            ]
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "00:01-00:07",
            "transcript": "Yeah, that would be super helpful. Um, I I'm do you want to",
            "speaking_duration": 6,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "60:01",
            "end_time": "60:07",
            "annotations": [
                {
                    "express agreement": "Mimi expresses agreement that Joyoni's tumor modeling experience would be helpful for her work on skeletal regeneration."
                },
                {
                    "encourage particpatioin": "Mimi encourages Joyoni to continue speaking by asking if she wants to do something."
                }
            ]
        },
        {
            "speaker": "Joyoni Dey",
            "timestamp": "00:07-00:11",
            "transcript": "Yeah, we can talk offline. We can talk offline and not derail it. Yeah.",
            "speaking_duration": 4,
            "nods_others": 0,
            "smile_self": 50.0,
            "smile_other": 25.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "Yes",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "60:07",
            "end_time": "60:11",
            "annotations": [
                {
                    "confirm decision": "Joyoni confirms the decision to discuss the tumor modeling offline, indicating agreement with a previously suggested course of action to avoid disrupting the current discussion."
                }
            ]
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "00:11-00:14",
            "transcript": "I I don't want to interrupt uh",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 25.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "60:11",
            "end_time": "60:14",
            "annotations": [
                {
                    "None": "No code applies to this utterance."
                }
            ]
        },
        {
            "speaker": "Joyoni Dey",
            "timestamp": "00:14-00:23",
            "transcript": "you know, the other discussions was very exciting and interesting. So I didn't want to interrupt. But we can write a line in that and we can talk offline.",
            "speaking_duration": 9,
            "nods_others": 0,
            "smile_self": 55.0,
            "smile_other": 22.0,
            "distracted_others": 0,
            "hand_gesture": "Raising Hand",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "60:14",
            "end_time": "60:23",
            "annotations": [
                {
                    "express enthusiasm": "The speaker expresses enthusiasm by saying the other discussions were very exciting and interesting."
                },
                {
                    "acknowledge contribution": "The speaker acknowledges the other discussions were exciting and interesting."
                }
            ]
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "00:23-00:25",
            "transcript": "Okay. I'll message you offline then. Okay, thank you.",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "60:23",
            "end_time": "60:25",
            "annotations": [
                {
                    "confirm decision": "Mimi confirms the decision to discuss the tumor modeling offline, which was proposed by Joyoni in the previous utterance."
                },
                {
                    "acknowledge contribution": "Mimi acknowledges Joyoni's offer to discuss tumor modeling offline."
                }
            ]
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "00:26-01:10",
            "transcript": "Actually, I do think that that, you know, the previous discussion was really great and already, you know, indicates a great idea for a, you know, a project proposal. Um, I do think that both Joyoni and Mimi might have some um input on this kind of multimodal approach because you're working generally at larger scales and especially Mimi with um doing kind of like the I think you do microCT with then the spatial transmic and I think that um, you know, just using using structural information at larger scales and then kind of how you would um either multimodal to get different information and or then use that spatial information to decide where to sample with high resolution. I think either of you might be able to contribute um some ideas here.",
            "speaking_duration": 44,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "60:26",
            "end_time": "61:10",
            "annotations": [
                {
                    "express enthusiasm": "Kristen expresses enthusiasm about the previous discussion, stating it indicates a great idea for a project proposal, showing her excitement about the potential outcome of the discussion."
                },
                {
                    "encourage particpatioin": "Kristen encourages Joyoni and Mimi to participate in the multimodal approach discussion, suggesting they might have valuable input based on their work at larger scales and with different imaging modalities."
                }
            ]
        },
        {
            "speaker": "Joyoni Dey",
            "timestamp": "01:10-01:44",
            "transcript": "Yeah, I have worked on region based reconstruction as well, MLM regional reconstruction, you know, like the challenge is that you won't have artifacts from taking only a region, you know, so um, so there are some iterative reconstructions that can help there.",
            "speaking_duration": 34,
            "nods_others": 0,
            "smile_self": 35.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Raising Hand",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "61:10",
            "end_time": "61:44",
            "annotations": [
                {
                    "expand on existing idea": "Joyoni is building on Kristen's suggestion of using spatial information to decide where to sample with high resolution by mentioning her experience with region-based reconstruction and how to avoid artifacts."
                },
                {
                    "provide supporting evidence": "Joyoni supports her experience in region-based reconstruction by mentioning MLM regional reconstruction and iterative reconstructions that can help avoid artifacts."
                }
            ]
        },
        {
            "speaker": "Joyoni Dey",
            "timestamp": "01:44-02:01",
            "transcript": "So do one would only one idea go from each of these breakout session or we can discuss some other ones too for later, you know, it doesn't need to be for this cycle or we can I can still discuss with Dr. um Sammarco.",
            "speaking_duration": 17,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "61:44",
            "end_time": "62:01",
            "annotations": [
                {
                    "propose decision": "Joyoni is proposing a decision about whether they should limit themselves to one idea per breakout session or if they can discuss other ideas for later, showing a suggestion for the group's process."
                }
            ]
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "02:01-02:27",
            "transcript": "Yes, I think that these breakout rooms are really meant to just seed ideas and learn about each other. And then, you know, really find the time outside of these structured times to be able to talk about um potential collaborations for the project proposals or even beyond that. And you will have, you know, future years to discuss as well, but I think if you have a seed that you're willing to both work on for for a project proposal that this is, you know, where that starts.",
            "speaking_duration": 26,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "62:01",
            "end_time": "62:27",
            "annotations": {
                "encourage particpatioin": "Kristen Maitland is encouraging participants to use the breakout rooms to generate ideas and find potential collaborations, building on the previous discussions about imaging modalities and challenges.",
                "express enthusiasm": "Kristen Maitland expresses enthusiasm for the breakout rooms as a way to seed ideas and learn about each other, encouraging potential collaborations for project proposals and beyond."
            }
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "02:30-03:13",
            "transcript": "Uh, can I ask so for to summarize what we'll uh talk about in the report out, we'll kind of try and focus on those two, these two ideas of that we discussed this concept of uh, you know, a smart microscope kind of thing that would be able to screen with low res data and then um ideally without user guided um input to then uh find things for higher res field of views. And then the second point about, you know, various ideas for combining things across modalities to um to to take advantage and and limit uh the downsides of each of these different modalities and stuff like that. That's essentially how we should summarize, I think.",
            "speaking_duration": 43,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "62:30",
            "end_time": "63:13",
            "annotations": {
                "propose decision": {
                    "Explanation": "Matt proposes a concrete choice for the group, suggesting how they should summarize their discussion for the report out, focusing on the smart microscope concept and combining modalities, building on the discussion about AI guided microscopes and multimodal imaging."
                }
            }
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "03:13-03:21",
            "transcript": "Okay. Is any if anyone wants to add to the doc, I've just been taking notes on these things, but um please go ahead.",
            "speaking_duration": 8,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "63:13",
            "end_time": "63:21",
            "annotations": [
                {
                    "encourage particpatioin": "Matt encourages others to contribute to the shared document, building on the previous discussion about summarizing ideas and adding to the document."
                }
            ]
        },
        {
            "speaker": "Joyoni Dey",
            "timestamp": "03:35-03:41",
            "transcript": "So um if it's okay, I'm adding so if it's okay, I'm adding the third idea of a",
            "speaking_duration": 6,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "63:35",
            "end_time": "63:41",
            "annotations": [
                {
                    "present new idea": "Joyoni is about to introduce a new idea, as indicated by the phrase \"adding the third idea\", which suggests a novel concept not previously discussed."
                }
            ]
        },
        {
            "speaker": "Joyoni Dey",
            "timestamp": "03:41-03:56",
            "transcript": "model based approach with Dr. Sammarco, you know, like modeling like using a OD equation.",
            "speaking_duration": 15,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "63:41",
            "end_time": "63:56",
            "annotations": [
                {
                    "present new idea": "Joyoni is presenting a new idea of a model-based approach with Dr. Sammarco using an ODE equation, which has not been previously discussed in the conversation."
                }
            ]
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "03:56-04:00",
            "transcript": "Yeah, I think that's a that's a um important point.",
            "speaking_duration": 4,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "63:56",
            "end_time": "64:00",
            "annotations": {
                "express agreement": "The speaker explicitly agrees with a prior statement, acknowledging its importance, which is in reference to the idea of a model-based approach with Dr. Sammarco."
            }
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "04:00-04:32",
            "transcript": "Um, perhaps uh, you know, in terms of in addition to experimentally uh trying to integrate different uh information across different scales using modeling approach um to connect data uh from different experiments and then uh make them coherent and integrate that information is is also uh an another approach and um",
            "speaking_duration": 32,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "64:00",
            "end_time": "64:32",
            "annotations": {
                "expand on existing idea": "Jin Zhang is building on the previous discussion about integrating different information across scales by suggesting that a modeling approach can be used to connect data from different experiments, making them coherent and integrating the information, which expands on the idea of combining information across modalities.",
                "express agreement": "Jin Zhang agrees with the previous point about the importance of modeling, indicating agreement with the idea of using a model-based approach."
            }
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "04:32-04:32",
            "transcript": "So maybe we can uh you know, you want to elaborate a little bit on that?",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "64:32",
            "end_time": "64:32",
            "annotations": {
                "encourage particpatioin": "Jin Zhang is encouraging someone to elaborate on the model-based approach idea that Joyoni Dey mentioned, inviting them to contribute to the discussion."
            }
        },
        {
            "speaker": "Joyoni Dey",
            "timestamp": "04:32-05:54",
            "transcript": "Uh what I have worked on is um for tumor models uh is uh you know, there are the um uh one set of equations where you describe the tumor population growth and the death like say natural apoptosis and there are these advective reactive equations OD equations describing that and then um what I have done is that didn't describe how the necrosis starts, okay? So what I had contributed was that I also have have another simultaneous equation where um I describe the oxygen partial pressure. I would get that from the spec imaging, okay? And then uh where the axial partial pressure pressure goes down to zero is where the necrosis will start in the tumor. So uh so that I can now start evolving this tumor equation with time and now the necrosis will start. So there is like I thought that that might easily translate to something that Dr.",
            "speaking_duration": 82,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Raising Hand",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "64:32",
            "end_time": "65:54",
            "annotations": [
                {
                    "explain or define term or concept": "Joyoni explains the tumor model she has worked on, including equations describing tumor population growth, death (apoptosis), advective-reactive equations, and oxygen partial pressure, to provide context for her contribution."
                },
                {
                    "expand on existing idea": "Joyoni expands on the idea of a model-based approach by describing how she incorporated oxygen partial pressure into tumor models to describe necrosis, building on the previous discussion of modeling approaches."
                }
            ]
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "05:54-05:55",
            "transcript": "Yeah.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "65:54",
            "end_time": "65:55",
            "annotations": {
                "express agreement": "The speaker explicitly agrees with the previous statement by Joyoni Dey about the potential of a model-based approach, indicating agreement with the idea."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "05:55-07:31",
            "transcript": "Sammarco was saying because I can describe the cellular metabolism and the physical structures of the uh osteoblast and osteocytes, you know, population and I could apply it to the anybody else's cellular um like, you know, all the things other things that we discussed. Yeah, we that would actually be perfect um for some of the stuff we work on. We there's we have this inexplicable we're working on an aged mouse model and so I think the the pictures up on the on the picture board or whatever, but you know, I get this higher bone mineral density and the regenerated bone. It's direct ossification. There's no cartilage intermediate. I get this um really high bone mineral density and it's in very specific places. And so the man handle or the woman handling of the microCT stacks and trying to actually align um where that is in relation to the histology, I can sort of reverse the spaces and then do CD31 and and say okay, that's an endothelial space but being able to correlate some of this high bone mineral density to what's around it, which presumably is either nerve or vascular space, um would actually lead to what the mechanism is that's underlying that, particularly if I can go in and sort of um look at that in terms of spatial transcriptomics and be able to say like, yeah, there's my you know, there's my vasculature and there's my osteoblast and you can see that it correlates like when we look at this in 2D. But um, we haven't been able to successfully take these CT stacks and be able to mathematically show it. I mean, I think the field hasn't really either because you end up with all these figures are just like, hey, it kind of looks like this. And then just hope somebody doesn't ask for some sort of quantification for it. You just sort of go, oh, we can't do it. Um, so but but I I think that's really needed um to be able to, you know, find out better ways to if you could do that, I mean, you could basically target where you where you want to interosseous growth for like prosthetic integration and stuff, which would be really nice.",
            "speaking_duration": 96,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "65:55",
            "end_time": "67:31",
            "annotations": [
                {
                    "expand on existing idea": "This sentence builds on Joyoni's previous suggestion of using a model-based approach, adding details about how it could be applied to describe cellular metabolism and physical structures."
                },
                {
                    "provide supporting evidence": "Mimi provides supporting evidence for the need of a model-based approach by describing the challenges in correlating bone mineral density with surrounding structures in her aged mouse model, highlighting the limitations of current methods and the potential benefits of a more quantitative approach."
                },
                {
                    "present new idea": "Mimi presents a new idea of targeting interosseous growth for prosthetic integration based on the model-based approach."
                }
            ]
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "07:46-07:55",
            "transcript": "Uh, could I ask Kristen, as we're getting uh close to finishing up, how do I put this into that the slide deck?",
            "speaking_duration": 9,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "67:46",
            "end_time": "67:55",
            "annotations": {
                "ask clarifying question": "Matt is asking Kristen for clarification on how to add information to the slide deck, as the group is nearing the end of their discussion."
            }
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "07:55-08:05",
            "transcript": "Um, so I would say that you should be able to copy and paste into the power the the the PowerPoint, whatever Google slides version.",
            "speaking_duration": 10,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "67:55",
            "end_time": "68:05",
            "annotations": {
                "assign task": "Kristen is assigning the task of copying and pasting information into the Google Slides presentation."
            }
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "08:05-08:05",
            "transcript": "Are you unable to do that?",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "68:05",
            "end_time": "68:05",
            "annotations": {
                "ask clarifying question": "Kristen is asking Matt if he is unable to copy and paste into the Google Slides, seeking clarification on a technical issue he raised."
            }
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "08:06-08:17",
            "transcript": "I can do that and then do I just put it back in the drive? Like I copy it, save it and then drag it into the drive because I don't seem to have online access to edit uh what's on the drive.",
            "speaking_duration": 11,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "68:06",
            "end_time": "68:17",
            "annotations": [
                {
                    "ask clarifying question": "Matt is asking Kristen for clarification on how to upload the edited slide deck back into the shared drive, as he doesn't have direct online editing access."
                }
            ]
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "08:17-08:20",
            "transcript": "Maybe Richard can help us with that. I um",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "68:17",
            "end_time": "68:20",
            "annotations": {
                "assign task": {
                    "Explanation": "Kristen assigns the task of helping Matt with the slide deck to Richard."
                }
            }
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "08:20-08:26",
            "transcript": "Or I mean we could also paste uh just what we've written here in there as well. I mean that's also uh an approach.",
            "speaking_duration": 6,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "68:20",
            "end_time": "68:26",
            "annotations": {
                "propose decision": "Matt is suggesting a concrete action for the group to take, which is to paste what they've written into the shared document as an alternative to directly editing the document online, following a discussion about summarizing the breakout session's ideas."
            }
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "08:27-08:31",
            "transcript": "Yes, I I think just paste into the um the field.",
            "speaking_duration": 4,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "68:27",
            "end_time": "68:31",
            "annotations": {
                "confirm decision": "Kristen Maitland confirms the decision to paste the information into the field, finalizing the approach for sharing the information."
            }
        },
        {
            "speaker": "Richard Wiener",
            "timestamp": "08:31-09:25",
            "transcript": "Yeah, you can paste it into the PowerPoint. When you get when you have it at the point that you want to have it in in uh what you want to present, when it's edited down, the easiest thing is just paste it in. That should work. Uh yeah, I I don't seem to have access to it. It says that it it won't preview the file and I can't load it. the same thing happened yesterday. so Oh, maybe we're having maybe it's there's so many people trying to put stuff in. Uh wait for a couple minutes or uh share it with someone else and another person can try. I I I haven't seen this problem in the previously, but it looks like we're getting enough people working in the document that it's um it's not making it as easy as we as it usually is or what we hoped for. So uh bear with us for a little bit. Don't if you if you're ready to paste something in but you guys want to talk some more, don't waste the time, please please do that.",
            "speaking_duration": 54,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Raising Hand",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows the logo of the Research Corporation for Science Advancement.",
            "start_time": "68:31",
            "end_time": "69:25",
            "annotations": [
                {
                    "assign task": "Richard suggests that Matt can paste the information into the PowerPoint, assigning Matt the task of transferring the notes.",
                    "express frustration": "Richard expresses frustration that he doesn't have access to the file, indicating a problem with the document sharing."
                }
            ]
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "09:26-09:31",
            "transcript": "And I'm I'm going to bug out but uh what parts of the discussion I've heard have been really interesting.",
            "speaking_duration": 5,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows the logo of the Research Corporation for Science Advancement.",
            "start_time": "69:26",
            "end_time": "69:31",
            "annotations": {
                "express enthusiasm": "Matt expresses that the discussion has been interesting."
            }
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "09:31-09:32",
            "transcript": "Thank you, Richard.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows the logo of the Research Corporation for Science Advancement.",
            "start_time": "69:31",
            "end_time": "69:32",
            "annotations": [
                {
                    "acknowledge contribution": "Jin Zhang acknowledges Richard's help with the technical difficulties of the shared document."
                }
            ]
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "09:32-09:59",
            "transcript": "Can I ask uh the the modeling based integration, can that be extended to uh, you know, other cross scales, single molecule to subcellular uh subcellular to um, you know, multicellular collective uh migration uh, you know, and then multi the multi tissue level, anything um along",
            "speaking_duration": 27,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows the logo of the Research Corporation for Science Advancement.",
            "start_time": "69:32",
            "end_time": "69:59",
            "annotations": {
                "ask clarifying question": "Jin Zhang is asking if the modeling-based integration approach discussed earlier can be applied to other scales, such as from single molecules to subcellular levels, indicating a need for clarification on the scope of the modeling approach."
            }
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "00:00-00:03",
            "transcript": "Those line has have people thought about that aspect?",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 33.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "70:00",
            "end_time": "70:03",
            "annotations": {
                "encourage particpatioin": "Jin Zhang is asking if anyone has thought about extending the modeling-based integration to other cross scales, encouraging others to share their thoughts on the topic."
            }
        },
        {
            "speaker": "Arnold Hayer",
            "timestamp": "00:07-00:50",
            "transcript": "I think I think yes. I think I think it could be so like the the example that I I said about cells, you know, turning forcing cells into specific turning thing. One could also say, okay, let's just look at what cells naturally do and then identify particular um for example, movement patterns and during specific movement patterns make that as a detection um point of detection where you say like, okay, this is the this is the event. So that's more more like using using a a set of features that have that have to happen in order to to detect the event and then and then focus focus in um",
            "speaking_duration": 43,
            "nods_others": 0,
            "smile_self": 10.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "70:07",
            "end_time": "70:50",
            "annotations": {
                "expand on existing idea": "Arnold is expanding on Jin's question about extending modeling-based integration across different scales, suggesting that one could identify particular movement patterns in cells and use those as detection points to focus in on specific events.",
                "present new idea": "Arnold presents a new idea of using cell movement patterns as a detection point for specific events, which hasn't been explicitly discussed before."
            }
        },
        {
            "speaker": "Joyoni Dey",
            "timestamp": "00:50-01:13",
            "transcript": "The modeling work that I have worked on is mostly on the tissue level. But I guess you can do transportation models and you know, it's a matter of learning the math and I don't think I can do it by next week, but but you know, uh yeah, in the future definitely uh I'll be at least very interested in doing something like that.",
            "speaking_duration": 23,
            "nods_others": 0,
            "smile_self": 65.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "70:50",
            "end_time": "71:13",
            "annotations": [
                {
                    "explain or define term or concept": "The phrase \"modeling work that I have worked on is mostly on the tissue level\" explains the scale at which Joyoni's modeling expertise lies, providing context for her potential contributions.",
                    "express enthusiasm": "The phrase \"in the future definitely uh I'll be at least very interested in doing something like that\" expresses Joyoni's enthusiasm for future collaboration and exploration of transportation models."
                }
            ]
        },
        {
            "speaker": "Douglas Shepherd",
            "timestamp": "01:13-02:57",
            "transcript": "So we've done this with snapshot RNA fish data. So the problem with RNA fish strategy is you have to kill the sample. But often the if you measure say 100 cells and you perturb them and then you measure them at similar time points, you actually get very repeatable behavior often for certain gene networks. So it turns out you can actually link the snapshot data using some modeling ideas. So these come more from control theory, so they're things like chemical master equation and these other things. What's really interesting about them is you can actually predict if you take enough data what the cells might do under a new stimuli or you can predict which time points you should then measure to reduce your uncertainty. So you can do sort of a coarser set of experiments and then predict like where do I need to fill in to understand the dynamics in my system better. And we actually showed that you can do this well enough where you can extract say um elongation rates of RNA from these snapshot data. So then we went back into the line system and actually measured the elongation rate and actually showed we got it right from the computational inference. The the problem we run into there again is we just started with an overwhelming amount of data. So we're stuck to this idea of like, can we it's really easy to start from this overwhelming data, specify a model and say I should have measured here. I think it gets much harder to start from a sort of first principles modeling and say this is where I need to be doing my measurements to learn the most over space and time and that problem I think is is still really challenging and I don't have a good handle on the best way to approach it. So.",
            "speaking_duration": 104,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "71:13",
            "end_time": "72:57",
            "annotations": [
                {
                    "provide supporting evidence": "Doug provides evidence of using snapshot RNA fish data and modeling ideas to link data and predict cell behavior under new stimuli, supporting the idea of using modeling to integrate data across scales, which was brought up by Jin in the previous turn."
                },
                {
                    "expand on existing idea": "Doug expands on the idea of modeling-based integration by describing how they used control theory and chemical master equations to predict cell behavior and reduce uncertainty, building on Jin's suggestion to use modeling to connect data from different experiments."
                }
            ]
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "02:57-03:11",
            "transcript": "Related to an earlier comment America made, this also almost you know, we can go back link back to our case study at the the beginning. If you can model uh the behaviors of the the the cilia.",
            "speaking_duration": 14,
            "nods_others": 0,
            "smile_self": 50.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "72:57",
            "end_time": "73:11",
            "annotations": [
                {
                    "expand on existing idea": "This utterance builds upon the earlier discussion about integrating different information across different scales, specifically linking it back to the case study of cilia behavior modeling."
                }
            ]
        },
        {
            "speaker": "Joyoni Dey",
            "timestamp": "03:11-03:12",
            "transcript": "Yeah.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "73:11",
            "end_time": "73:12",
            "annotations": [
                {
                    "express agreement": "Joyoni Dey expresses agreement with a prior statement, indicated by the use of \"Yeah.\""
                }
            ]
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "03:12-03:21",
            "transcript": "Um that could also provide uh some you know, some possibilities of linking those different scales.",
            "speaking_duration": 9,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "73:12",
            "end_time": "73:21",
            "annotations": {
                "expand on existing idea": "This utterance builds upon the previous discussion about modeling the behaviors of cilia, suggesting that it could provide possibilities for linking different scales, expanding on the idea of using modeling to integrate information across scales."
            }
        },
        {
            "speaker": "Joyoni Dey",
            "timestamp": "03:21-03:22",
            "transcript": "USD.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "73:21",
            "end_time": "73:22",
            "annotations": [
                {
                    "None": "This utterance does not contain any content that can be coded using the provided codebook."
                }
            ]
        },
        {
            "speaker": "Vivian Qian Liu",
            "timestamp": "03:22-04:56",
            "transcript": "Yes, uh I think a lot of the discussion we made so far are applied to my research. So the the case study for the cilia, having a lot of approaches would apply to viruses as well because they are the cilia is like uh uh the little filaments on the cell. So the virus would attach on those. So the movement of the cilia kind of reflects uh sort of the movement of the virus particle on the cell before they enter. Uh and also the uh I think the uh user guided imaging where where you uh you focus where with a with a bigger field and then you zoom in on a smaller field. That's exactly what I would looking for. And I think if we can I think that it it it definitely would help with the uh photo bleaching or photo toxicity uh problem so that with a uh bigger field we don't need we uh we kind of spread out the intensity of the of the lasers. But uh once we find an interesting field, we can zoom it in then we only look at that small part and look at it at super resolution and that way we can keep the cell alive while we're imaging. So yes, that was the thing I found most interesting from that from this discussion.",
            "speaking_duration": 94,
            "nods_others": 0,
            "smile_self": 10.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows a background image of the Aurora Borealis, also known as the Northern Lights. The image remains static throughout the segment.",
            "start_time": "73:22",
            "end_time": "74:56",
            "annotations": [
                {
                    "express agreement": "The speaker agrees that the discussion applies to their research, specifically the cilia case study, and that approaches used for cilia could apply to viruses as well, since viruses attach to these filaments."
                },
                {
                    "expand on existing idea": "The speaker expands on the idea of user-guided imaging, where one focuses on a larger field and then zooms in on a smaller field, stating that this is exactly what they are looking for and that it would help with photo bleaching and photo toxicity problems."
                }
            ]
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "05:19-05:25",
            "transcript": "Great. Uh I think uh any any other comments um thoughts?",
            "speaking_duration": 6,
            "nods_others": 0,
            "smile_self": 66.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows a background image of the Aurora Borealis, also known as the Northern Lights. The image remains static throughout the segment.",
            "start_time": "75:19",
            "end_time": "75:25",
            "annotations": {
                "encourage particpatioin": "Jin Zhang is explicitly encouraging other members of the group to contribute their thoughts and comments to the discussion."
            }
        },
        {
            "speaker": "Lingyan Shi",
            "timestamp": "05:26-06:11",
            "transcript": "I just have a quick uh last comment on the on the um RNA in situ fish for spatial information. Just just idea that how we can combine the hyper spectrum Raman imaging uh since we just discussed that each pixel will be belong to a certain molecule uh that is the metabolic actually the mapping the metabolic activity. So we can also uh validate by the fish in situ fish multiplex imaging to do the label free fish in the in the future. That's just my my idea on on that direction. A quick comments on that. Yeah.",
            "speaking_duration": 45,
            "nods_others": 0,
            "smile_self": 75.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows a background image of the Geisel Library at UCSD. The image remains static throughout the segment.",
            "start_time": "75:26",
            "end_time": "76:11",
            "annotations": [
                {
                    "present new idea": "Lingyan proposes a new idea to combine hyperspectral Raman imaging with FISH in situ hybridization for spatial information, building on the previous discussion about Raman imaging and its ability to map metabolic activity at the molecular level."
                },
                {
                    "expand on existing idea": "Lingyan expands on the idea of combining hyperspectral Raman imaging with FISH by suggesting that it could be used to validate label-free FISH in the future, building on the previous discussion about Raman imaging and its ability to map metabolic activity at the molecular level."
                }
            ]
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "06:12-06:21",
            "transcript": "Sounds like that's an idea that you guys can explore it a little bit Matt and Lingyan. that's also close in terms of collaboration for you guys.",
            "speaking_duration": 9,
            "nods_others": 0,
            "smile_self": 77.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows a background image of the Geisel Library at UCSD. The image remains static throughout the segment.",
            "start_time": "76:12",
            "end_time": "76:21",
            "annotations": {
                "encourage particpatioin": "Jin Zhang encourages Matt and Lingyan to explore the idea further, suggesting a potential collaboration between them based on their discussion about combining hyperspectral Raman imaging with RNA in situ fish."
            }
        },
        {
            "speaker": "Joyoni Dey",
            "timestamp": "06:21-06:23",
            "transcript": "Yeah, USD.",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows a background image of the Geisel Library at UCSD. The image remains static throughout the segment.",
            "start_time": "76:21",
            "end_time": "76:23",
            "annotations": [
                {
                    "express agreement": "The speaker explicitly agrees with a previous statement, indicated by the use of \"Yeah\"."
                }
            ]
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "06:23-06:27",
            "transcript": "We have 30 seconds 30 seconds left.",
            "speaking_duration": 4,
            "nods_others": 0,
            "smile_self": 75.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows a background image of the Geisel Library at UCSD. The image remains static throughout the segment.",
            "start_time": "76:23",
            "end_time": "76:27",
            "annotations": [
                {
                    "None": "No code applies to this utterance."
                }
            ]
        },
        {
            "speaker": "Lingyan Shi",
            "timestamp": "06:27-06:30",
            "transcript": "Great discussions uh everyone.",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows a background image of the Geisel Library at UCSD. The image remains static throughout the segment.",
            "start_time": "76:27",
            "end_time": "76:30",
            "annotations": [
                {
                    "express enthusiasm": "Lingyan expresses enthusiasm for the discussions that took place in the breakout session."
                }
            ]
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "06:30-06:34",
            "transcript": "We're counting to end it.",
            "speaking_duration": 4,
            "nods_others": 0,
            "smile_self": 50.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows a background image of the Geisel Library at UCSD. The image remains static throughout the segment.",
            "start_time": "76:30",
            "end_time": "76:34",
            "annotations": [
                {
                    "express humor": "The speaker is making a lighthearted comment about the meeting ending soon, which can be interpreted as a form of humor."
                }
            ]
        },
        {
            "speaker": "Jin Zhang",
            "timestamp": "06:37-06:40",
            "transcript": "Kristen, you have any last comment?",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows a background image of the Geisel Library at UCSD. The image remains static throughout the segment.",
            "start_time": "76:37",
            "end_time": "76:40",
            "annotations": {
                "encourage particpatioin": "Jin Zhang is explicitly asking Kristen for her final thoughts, encouraging her to contribute to the discussion."
            }
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "06:40-06:47",
            "transcript": "No, I'm trying to squeeze all of our discussion into our box on the on the presentation.",
            "speaking_duration": 7,
            "nods_others": 0,
            "smile_self": 57.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows a background image of the Geisel Library at UCSD. The image remains static throughout the segment.",
            "start_time": "76:40",
            "end_time": "76:47",
            "annotations": [
                {
                    "express frustration": "Kristen expresses frustration about fitting the discussion summary into the presentation box, indicating a challenge or difficulty she's facing."
                }
            ]
        },
        {
            "speaker": "Matt Lovett-Barron",
            "timestamp": "06:47-06:52",
            "transcript": "Thank you. Please feel free to cut some things out and I'll just I'll I'll read from the breakout room doc. So.",
            "speaking_duration": 5,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows a background image of the Geisel Library at UCSD. The image remains static throughout the segment.",
            "start_time": "76:47",
            "end_time": "76:52",
            "annotations": [
                {
                    "acknowledge contribution": "Matt acknowledges Kristen's effort in summarizing the discussion, but does not agree or expand on it."
                }
            ]
        },
        {
            "speaker": "Kristen Maitland",
            "timestamp": "06:52-06:54",
            "transcript": "Okay, sounds great.",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 100.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "The screen shows a background image of the Geisel Library at UCSD. The image remains static throughout the segment.",
            "start_time": "76:52",
            "end_time": "76:54",
            "annotations": {
                "express agreement": "Kristen Maitland expresses agreement with the previous discussion, indicating a positive assessment of the ideas shared."
            }
        }
    ]
}