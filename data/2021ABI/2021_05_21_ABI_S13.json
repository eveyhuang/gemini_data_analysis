{
    "all_speakers": [
        "Melike Lakadamyali",
        "Dylan Burnette",
        "Katy Keenan",
        "Shannon Quinn",
        "Dylan McCreedy",
        "Unknown speaker",
        "Mimi Sammarco",
        "Samuel Achilefu",
        "Ferdinand Schweser",
        "Dylan McCreedy | Texas A&M",
        "Domenico",
        "Crystal Rogers"
    ],
    "total_speaking_length": 3579,
    "all_data": [
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "00:00-00:20",
            "transcript": "to uh uh invert the signal that we that we get with the MRI. Usually uh the raw signal, not the images that get out of the scanner um directly. And uh what we've recently started uh doing more is uh the combination, multi-parametric combination of quantitative MRI.",
            "speaking_duration": 20,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "00:00",
            "end_time": "00:20",
            "annotations": {
                "develop idea": "The speaker is expanding on their approach to inverting MRI signals and their recent use of multi-parametric combination of quantitative MRI, providing more details and examples.",
                "signal expertise": "The speaker implies their expertise in MRI signal processing by explaining their methods and recent approaches."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "00:00-00:14",
            "transcript": "and that is complementary in the different imaging contrasts. Um also in part based on physical models. Yeah, so uh who's next? Katie.",
            "speaking_duration": 13,
            "nods_others": 0,
            "smile_self": 15,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "00:20",
            "end_time": "00:34",
            "annotations": {
                "develop idea": "The speaker expands on an existing idea by mentioning its complementarity in different imaging contrasts and its basis on physical models.",
                "process management": "The speaker manages the meeting flow by asking who's next and addressing Katie."
            }
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "00:15-00:45",
            "transcript": "I'm Katy Keenan. I'm at the National Institute of Standards and Technology and we work to do validation of quantitative MRI techniques. We've developed some standards and methods to do that and we're expanding into uh low field imaging. Um so point of care MRI. Uh next uh Dylan.",
            "speaking_duration": 29,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 1,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "00:35",
            "end_time": "01:05",
            "annotations": {
                "signal expertise": "The speaker explicitly states her own expertise and qualifications related to the task, mentioning her work at the National Institute of Standards and Technology and her involvement in validating quantitative MRI techniques.",
                "develop idea": "The speaker expands on her area of expertise by mentioning the development of standards and methods for validation and their expansion into low field imaging."
            }
        },
        {
            "speaker": "Dylan McCreedy",
            "timestamp": "00:46-01:23",
            "transcript": "Hi, I'm Dylan McCreedy. I'm an assistant professor in biology at Texas A&M. So my lab focuses mostly on tissue clearing and light sheet imaging and then looking at three-dimensional analysis of neural circuits before and and after injury and we focus mostly on the sample and imaging side and really trying to learn more about um or or establish collaborations for potential 3D analysis of complex um dense neural circuits.",
            "speaking_duration": 37,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 1,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "01:06",
            "end_time": "01:43",
            "annotations": {
                "Signal expertise": "The speaker explicitly states their position, institution, and their lab's focus areas.",
                "Identify gap": "The speaker mentions what they are trying to learn more about or establish collaborations for.",
                "Clarify goal": "The speaker is defining their objectives or areas of focus."
            }
        },
        {
            "speaker": "Dylan McCreedy",
            "timestamp": "01:23-01:26",
            "transcript": "Crystal, you're next uh on my side.",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 50,
            "smile_other": 0,
            "distracted_others": 1,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "01:43",
            "end_time": "01:46",
            "annotations": {
                "encourage participation": "The speaker is inviting someone else to contribute by saying 'Crystal, you're next uh on my side.'",
                "process management": "The utterance manages the meeting flow by indicating whose turn it is to speak."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "01:26-02:30",
            "transcript": "Sorry. I don't know if it's my internet but the sound keeps cutting out on me. Um I'm Crystal Rogers at UC Davis. I am a developmental cell molecular biologist. Um and I really focus on um understanding how proteins work together um and in networks to control the formation of uh neural crest cells and cells that um essentially make the craniofacial region of the animal. Um I am really interested I chose this session because in development we have a lot of images. So we take high resolution um across time and space these beautiful images, but I think in my field at least we don't do enough to quantify the and actually get all of the data we can out of these images and I thought that um it would be really great to figure out how to take these, you know, Z stacks, these confocal whatever you're you're using and figure out better ways to actually define, you know, cell tension, protein interaction, um and and and pull as much out of that as possible.",
            "speaking_duration": 64,
            "nods_others": 0,
            "smile_self": 20,
            "smile_other": 0,
            "distracted_others": 1,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "01:46",
            "end_time": "02:50",
            "annotations": {
                "signal expertise": "Crystal explicitly states her expertise in developmental cell molecular biology.",
                "identify gap": "Crystal points out a gap in her field regarding the quantification of data from images.",
                "clarify goal": "Crystal implies a goal of improving the quantification of data from images."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "02:30-02:31",
            "transcript": "Oh, Dylan.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 1,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "02:50",
            "end_time": "02:51",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "02:34-03:32",
            "transcript": "I am uh Dylan Burnette. Um I am a cell biologist uh and I'm currently at Vanderbilt University. I am interested in how the heart grows and since I'm a cell biologist, I focus on cells because that's kind of what we do. And so I'm interested in how cells both proliferate in the heart and also enlarge. Uh and to do that we use a variety of optical uh so light based uh microscopes all the way from very low mag imaging for screening purposes uh like 10 or 20x objectives low mag uh up to super resolution and now for better or for worse we're doing expansion microscopy plus super resolution. Um and that's pretty much what we're doing. Uh and the whole goal is to see basically how a heart muscle cell grows actually enlarges and then survives different conditions that uh would necessarily kill a human if it would happen in vivo.",
            "speaking_duration": 58,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "02:54",
            "end_time": "03:52",
            "annotations": {
                "signal expertise": "The speaker explicitly states their background and expertise as a cell biologist and their current position at Vanderbilt University.",
                "clarify goal": "The speaker clarifies their research goal, which is to understand how heart muscle cells grow, enlarge, and survive under different conditions."
            }
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "03:32-03:33",
            "transcript": "And I think uh on this thing is Nick is the next.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "03:52",
            "end_time": "03:53",
            "annotations": {
                "process management": "The speaker is managing the meeting flow by indicating who is next to speak."
            }
        },
        {
            "speaker": "Domenico",
            "timestamp": "03:33-04:34",
            "transcript": "Hi, my name's Nick. I'm assistant professor of biology at Western Washington University. Um I'm also a cell biologist and my my primary research focus is trying to understand how uh organelle called the cilium is assembled and the cilium is a is a important signaling structure. It also generates hydrodynamic force when it beats back and forth. So it can do both of those things and it's a it's a cytoskeletal structure that is organized by the centrosome. So the centrosome is what organizes the mitotic spindle so it's really important for separating two cells during division.",
            "speaking_duration": 61,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 1,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "03:53",
            "end_time": "04:54",
            "annotations": {
                "signal expertise": "The speaker explicitly states his position as an assistant professor of biology and identifies himself as a cell biologist.",
                "develop idea": "The speaker elaborates on his research focus, explaining what the cilium is and its functions."
            }
        },
        {
            "speaker": "Domenico",
            "timestamp": "04:34-04:34",
            "transcript": "And last is Mimi, I believe.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "04:54",
            "end_time": "04:54",
            "annotations": {
                "process management": "The speaker is attempting to manage the meeting flow by moving to the next person for introduction."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "04:34-05:14",
            "transcript": "Um, I'm Mimi Smarco. I'm an assistant professor in the Department of Surgery at Tulane School of Medicine in New Orleans. Um, to follow on a tale of Crystal who works in developmental biology. Um, I work in skeletal regeneration, um, in an adult model of the mouse and the objective is basically to look at, um, pro bone formation pathways, um, to be able to apply this back.",
            "speaking_duration": 40,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "04:54",
            "end_time": "05:34",
            "annotations": {
                "None": "No relevant code directly applies to this utterance as it is primarily an introductory statement."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "05:14-05:32",
            "transcript": "So we predominantly use microCT to look at bone growth and then of course the way Crystal does, you know, we do IHC and um looking at antigens and protein expression completely separately in two dimensions. You know, we don't get very good quantification. Um, and so we're interested in kind of like melding those things together.",
            "speaking_duration": 18,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "05:34",
            "end_time": "05:52",
            "annotations": {
                "develop idea": "The speaker is elaborating on their current methods and another group's method.",
                "identify gap": "The speaker mentions the limitation in quantification they are facing."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "05:32-05:33",
            "transcript": "Did I miss someone? Wait.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "05:52",
            "end_time": "05:53",
            "annotations": {
                "ask question": "The speaker is requesting information about whether she missed someone's introduction."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "05:33-05:34",
            "transcript": "Just introduce the next person.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "05:53",
            "end_time": "05:54",
            "annotations": {
                "encourage participation": "The speaker is inviting the group to introduce the next person, encouraging participation."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "05:34-05:35",
            "transcript": "Anyway, it ends with me.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "05:54",
            "end_time": "05:55",
            "annotations": {
                "process management": "Mimi Sammarco is managing the flow of introductions by indicating she is the last person to introduce herself."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "05:35-05:36",
            "transcript": "Um, I I don't know what list we're going from.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "05:55",
            "end_time": "05:56",
            "annotations": {
                "identify gap": "Mimi explicitly recognizes her lack of knowledge about what list is being referred to."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "05:36-05:37",
            "transcript": "I'm just going to be honest.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "05:56",
            "end_time": "05:57",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "05:37-05:39",
            "transcript": "I'm guessing the Shannon go?",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "05:57",
            "end_time": "05:59",
            "annotations": {
                "ask question": "Mimi Sammarco is seeking information about whose turn it is next in the introductions, indicated by her inquiry about Shannon."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "05:39-05:39",
            "transcript": "Yeah.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "05:59",
            "end_time": "05:59",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "05:39-05:40",
            "transcript": "Uh I I I'm I was just going based on the person I saw next in the tile, that's all.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "05:59",
            "end_time": "06:00",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "05:40-05:41",
            "transcript": "Oh. That's Chris. I'm trying to see.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "06:00",
            "end_time": "06:01",
            "annotations": {
                "process management": "Mimi Sammarco is managing the meeting flow by trying to identify the next person to introduce."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "05:41-05:42",
            "transcript": "Anybody left?",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "06:01",
            "end_time": "06:02",
            "annotations": {
                "process management": "The speaker is inquiring about the presence of any additional participants, indicating a management of the meeting flow or introduction process."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "05:42-05:42",
            "transcript": "The bot maybe?",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "06:02",
            "end_time": "06:02",
            "annotations": {
                "express humor": "The utterance 'The bot maybe?' can be interpreted as an attempt at humor, making a lighthearted comment about the possibility of a bot being the only other participant left to introduce themselves."
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "05:42-05:42",
            "transcript": "Yeah.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "06:02",
            "end_time": "06:02",
            "annotations": {
                "Supportive response": "It serves as a minimal agreement or acknowledgment to the previous statement."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "05:42-05:43",
            "transcript": "Okay.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "06:02",
            "end_time": "06:03",
            "annotations": {
                "None": "The utterance is a minimal response that does not clearly fit into any other category."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "05:43-05:43",
            "transcript": "We're good then.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "06:03",
            "end_time": "06:03",
            "annotations": {
                "None": "No relevant code explicitly applies to this utterance"
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "05:43-05:44",
            "transcript": "Wonderful.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "06:03",
            "end_time": "06:04",
            "annotations": {
                "Supportive response": "The speaker expresses a positive sentiment towards the introductions or the state of the meeting."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "05:44-06:02",
            "transcript": "I think we've gone round. Katie, do you have anything to say as we continue?",
            "speaking_duration": 18,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "06:04",
            "end_time": "06:22",
            "annotations": {
                "process management": "The speaker is managing the meeting flow by suggesting they've completed the introductions and is about to proceed.",
                "encourage participation": "The speaker is inviting Katie to contribute by asking if she has anything to say as they continue."
            }
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "06:40-06:40",
            "transcript": "No.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "07:00",
            "end_time": "07:00",
            "annotations": {
                "None": "No relevant code directly applies to this utterance as it is a simple negation without adding substantial content to the conversation."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "06:41-06:41",
            "transcript": "Okay, good.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "07:01",
            "end_time": "07:01",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "06:44-07:14",
            "transcript": "Um typically we ask um all the members to write down at least something they think about this topic, what's the burning issues you would like us to discuss and um and then we select someone that will be the reporter for this group. Um I hope somebody will volunteer to be that person.",
            "speaking_duration": 30,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "07:04",
            "end_time": "07:34",
            "annotations": {
                "process management": "The speaker is explaining the process of how the group will discuss the topic and select a reporter.",
                "assign task": "The speaker is assigning a task (or seeking a volunteer for the task) of being the reporter for the group.",
                "encourage participation": "The speaker is encouraging participation by seeking a volunteer for the role of the reporter."
            }
        },
        {
            "speaker": "Dylan McCreedy",
            "timestamp": "07:18-07:18",
            "transcript": "I can do the report.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "07:38",
            "end_time": "07:38",
            "annotations": {
                "assign task": "The speaker is taking on a responsibility, which can be seen as a form of assigning a task to oneself, even though the typical definition involves assigning tasks to others."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "07:18-07:19",
            "transcript": "I just sorry, I just have a question.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "07:38",
            "end_time": "07:39",
            "annotations": {
                "ask question": "The speaker is requesting information or clarification on a topic, indicating a need for further discussion or explanation."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "07:19-07:30",
            "transcript": "Are we talking about deep tissue imaging in this session or quantitative imaging? I'm I'm I'm a bit confused.",
            "speaking_duration": 11,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "07:39",
            "end_time": "07:50",
            "annotations": {
                "ask question": "The speaker is requesting information or clarification on the topics to be discussed.",
                "clarify goal": "The speaker is seeking clarity on the objectives or topics of discussion for the session."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "07:31-07:31",
            "transcript": "It says meeting room six.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "07:51",
            "end_time": "07:51",
            "annotations": {
                "process management": "This code applies because Ferdinand Schweser is providing information about the meeting location, which relates to managing meeting arrangements."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "07:31-07:32",
            "transcript": "Maybe it's another session.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "07:51",
            "end_time": "07:52",
            "annotations": {
                "Ask Question": "Melike Lakadamyali is seeking clarification about whether they are in the correct session."
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "07:32-07:33",
            "transcript": "Uh quantitative imaging.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "07:52",
            "end_time": "07:53",
            "annotations": {
                "Supportive response": "Shannon Quinn is validating or agreeing with the direction of the discussion by confirming the topic as 'quantitative imaging'."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "07:33-07:39",
            "transcript": "I thought I was in the quantitative imaging session. I don't do any deep tissue imaging.",
            "speaking_duration": 6,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "07:53",
            "end_time": "07:59",
            "annotations": {
                "ask question": "The speaker is seeking clarification on the session topic, implicitly asking for confirmation or information.",
                "clarify goal": "The speaker is also trying to understand or clarify the goal or topic of the session they are in."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "07:39-07:42",
            "transcript": "I do both, but it's I thought it said quantitative imaging.",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "07:59",
            "end_time": "08:02",
            "annotations": {
                "ask question": "Mimi Sammarco is seeking clarification on the session topic, indicating uncertainty.",
                "clarify goal": "Her question aims at understanding the objective or focus of the session."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "07:42-07:43",
            "transcript": "It's quantitative imaging.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "08:02",
            "end_time": "08:03",
            "annotations": {
                "supportive response": "The utterance confirms the session topic as quantitative imaging, supporting Shannon's previous statement."
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "07:43-07:45",
            "transcript": "Yeah. Yeah, quantitative imaging.",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "08:03",
            "end_time": "08:05",
            "annotations": {
                "supportive response": "Shannon Quinn is expressing agreement and confirmation of the topic as quantitative imaging."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "07:45-07:45",
            "transcript": "Wonderful.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "08:05",
            "end_time": "08:05",
            "annotations": {
                "supportive response": "The speaker is expressing agreement or validation for the previous discussion or agreement."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "07:45-08:11",
            "transcript": "I I was thrown to a different group then that's fine. I can do quantitative imaging. This is interesting. That's fine. Okay, then let's move into quantitative imaging. Um initially they threw me into deep tissue imaging and maybe I ended up with you guys which is fun.",
            "speaking_duration": 26,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "08:05",
            "end_time": "08:31",
            "annotations": {
                "process management": "He manages the meeting flow by suggesting they move into quantitative imaging.",
                "clarify goal": "He defines what they will focus on, which is quantitative imaging.",
                "supportive response": "He shows a positive attitude towards the current situation and agrees to proceed with quantitative imaging."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "08:11-08:12",
            "transcript": "So, um will somebody volunteer to uh be our reporter for this?",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "08:31",
            "end_time": "08:32",
            "annotations": {
                "process management": "The speaker is organizing the group by asking for a volunteer to be the reporter, which involves managing group activities.",
                "assign task": "The speaker is designating a role (reporter) within the group, which involves assigning responsibility."
            }
        },
        {
            "speaker": "Dylan McCreedy",
            "timestamp": "08:18-08:18",
            "transcript": "I can do the report.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "08:38",
            "end_time": "08:38",
            "annotations": {
                "process management": "This code applies because Dylan McCreedy is facilitating the meeting process by volunteering for the role of reporter."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "08:18-08:19",
            "transcript": "Dylan volunteered. Thanks Dylan.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "08:38",
            "end_time": "08:39",
            "annotations": {
                "acknowledge contribution": "The speaker verbally recognizes another group member's input by stating Dylan volunteered.",
                "supportive response": "The speaker expresses a positive evaluation of Dylan's contribution by thanking him."
            }
        },
        {
            "speaker": "Dylan McCreedy",
            "timestamp": "08:20-08:20",
            "transcript": "Thanks Crystal.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "08:40",
            "end_time": "08:40",
            "annotations": {
                "acknowledge contribution": "Dylan McCreedy is verbally recognizing Crystal Rogers' input by thanking her."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "08:21-08:21",
            "transcript": "Okay.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "08:41",
            "end_time": "08:41",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "08:21-08:21",
            "transcript": "That is great.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "08:41",
            "end_time": "08:41",
            "annotations": {
                "supportive response": "The speaker expresses a positive sentiment towards the previous contribution, indicating agreement or approval."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "00:00-00:25",
            "transcript": "brain last week in our group and the data generated just enormous and how do you quantify that? How do you process it? Um so it's a challenge that everybody faces every day. Um and can you just give us an idea of your thoughts in this area and what you think.",
            "speaking_duration": 25,
            "nods_others": 3,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "10:20",
            "end_time": "10:45",
            "annotations": {
                "ask question": "The speaker requests information on how to quantify and process the data generated.",
                "identify gap": "The speaker acknowledges that dealing with enormous data is a challenge that everybody faces.",
                "encourage participation": "The speaker invites the group to share their thoughts and ideas on the matter."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "00:25-00:59",
            "transcript": "I have a lot of really basic thoughts, but I'll start with basically, um there are so many different pipelines for image analysis and currently, we don't have normalized or consistent processes so that image analysis actually ends up so that the data you get is the same across even users in the same lab, much less across labs and I think that that makes it really difficult to compare apples to apples uh when you're doing research to try to quantify things with images.",
            "speaking_duration": 34,
            "nods_others": 2,
            "smile_self": 0.09,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "10:45",
            "end_time": "11:19",
            "annotations": {
                "identify gap": "Crystal Rogers explicitly recognizes a gap in the current processes of image analysis, which is the lack of normalized or consistent processes.",
                "critical response": "She is providing a critical response to the current state of image analysis by highlighting the difficulties it poses for research."
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "01:00-01:08",
            "transcript": "When you say normalization, do you mean in terms of the experimental protocols or the computational pre-processing or everything?",
            "speaking_duration": 8,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "11:20",
            "end_time": "11:28",
            "annotations": {
                "ask question": "The speaker is requesting clarification on what Crystal Rogers meant by 'normalization', seeking more information on whether it pertains to experimental protocols, computational pre-processing, or everything."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "01:08-01:37",
            "transcript": "Well, okay, so that's that gets into it. Yes, obviously there are also experimental protocols that differ, but even if you hand people the same image, how they process or pre-process that before. So for example, say you're using ImageJ and you're just going to threshold or you're going to filter or you know, something like that could be different um across users and so then you end up getting different data and so it's really hard to then what's real? Like define what's real.",
            "speaking_duration": 29,
            "nods_others": 0,
            "smile_self": 0.1,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "11:28",
            "end_time": "11:57",
            "annotations": {
                "develop idea": "Crystal is expanding on the idea of challenges in image analysis, specifically about processing steps and their impact on data consistency.",
                "ask question": "Crystal asks a question seeking clarification on what is real or how to define it in the context of image analysis outcomes.",
                "identify gap": "Crystal points out the difficulty in comparing data across different analyses due to inconsistent processing methods, highlighting a gap in current practices."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "01:38-02:10",
            "transcript": "I just like to comment that in the MRI world, we distinguish between quantitative imaging and quantitative analysis. So you can do a quantitative analysis on a qualitative image. Right so you just get an anatomical brain scan and you measure the volume of a certain region, you get a quantitative metric but the imaging technique is not quantitative. But you can you can have a thermometry technique that gives you a number for the temperature that would be a quantitative technique.",
            "speaking_duration": 32,
            "nods_others": 1,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "11:58",
            "end_time": "12:30",
            "annotations": {
                "signal expertise": "The speaker is explicitly stating his understanding and experience with MRI terminology, which implies a level of expertise in the field."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "02:11-02:26",
            "transcript": "That's a good point. So separating the actual quantitative imaging from quantitative analysis of the imaging, whatever type of imaging you're looking at. That's I think that's something we need to clarify.",
            "speaking_duration": 15,
            "nods_others": 0,
            "smile_self": 0.2,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "12:31",
            "end_time": "12:46",
            "annotations": {
                "develop idea": "The speaker is expanding on an existing idea by emphasizing the need to clarify the distinction between quantitative imaging and quantitative analysis.",
                "ask question": "The speaker implicitly suggests a need for clarification or discussion, which can be seen as questioning the group's understanding or agreement."
            }
        },
        {
            "speaker": "Domenico",
            "timestamp": "02:27-03:25",
            "transcript": "Yeah, I think Crystal, I I agree with you. I mean as somebody that was really into ImageJ because I'm not sophisticated enough to use deep learning and that kind of stuff. You know, just having the human be the one that sets the threshold is to me this this this problem that I think is pervasive and like you said, you can give somebody the same image. So I I I I don't know the answer to it. I think it probably involves deep learning or something. But I think that, you know, and I had a discussion about this yesterday with somebody that's in the deep learning field and and their take is that ImageJ is now already obsolete and that all of this is going into the deep learning realm and people that like me that use ImageJ are going to get left behind. And I was sad to hear that, but I think it just seems like it's true. So I I'm very curious then from the deep learning folks, you know, what can be done to lower the activation energy to getting into that so that I don't have to write Python code, which I don't know how to do.",
            "speaking_duration": 58,
            "nods_others": 0,
            "smile_self": 0.2,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "12:47",
            "end_time": "13:45",
            "annotations": {
                "supportive response": "The speaker expresses agreement with Crystal Rogers.",
                "develop idea": "The speaker expands on his experience with ImageJ and thoughts on deep learning.",
                "ask question": "The speaker requests information on how to get into deep learning."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "03:25-03:25",
            "transcript": "Take.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 1.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "13:45",
            "end_time": "13:45",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "03:25-03:45",
            "transcript": "Well, I I would push back on that for exactly the reason that you said. The activation energy is way too high to get into deep learning right now unless you're a deep learning practitioner. And as I have never heard that ImageJ is on the way out. That still seems like kind of the de facto standard for non computer science people.",
            "speaking_duration": 20,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "13:45",
            "end_time": "14:05",
            "annotations": {
                "critical response": "The speaker is questioning the idea that ImageJ is obsolete and deep learning is the immediate solution.",
                "supportive response": "The speaker agrees with the underlying concern about the challenges of adopting deep learning.",
                "develop idea": "The speaker is expanding the discussion on the feasibility and challenges of transitioning to deep learning from ImageJ."
            }
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "03:46-03:49",
            "transcript": "Someone mentioned that yesterday too and I just let it slide.",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "14:06",
            "end_time": "14:09",
            "annotations": {
                "acknowledge contribution": "The speaker is verbally recognizing another group member's input, but not agreeing or expanding."
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "03:50-03:51",
            "transcript": "Yeah, I don't.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "14:10",
            "end_time": "14:11",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "03:51-03:56",
            "transcript": "I mean we use all kind of tools in my lab and ImageJ is a perfectly fine tool to use for.",
            "speaking_duration": 5,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "14:11",
            "end_time": "14:16",
            "annotations": {
                "Supportive Response": "Dylan Burnette is positively evaluating ImageJ, a tool mentioned in the context of image analysis, showing agreement with its utility.",
                "Develop Idea": "Dylan Burnette is contributing to the discussion on image analysis tools by sharing his experience with ImageJ, thereby expanding on the existing conversation about tools used in image analysis."
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "03:56-03:56",
            "transcript": "Yeah.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "14:16",
            "end_time": "14:16",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "03:58-04:03",
            "transcript": "And you can have custom models in ImageJ, so they'll probably even machine learning AI models now that you can just load.",
            "speaking_duration": 5,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "14:18",
            "end_time": "14:23",
            "annotations": {
                "develop idea": "The speaker is expanding on the idea of using ImageJ for image analysis by mentioning the possibility of using custom models, including machine learning AI models.",
                "supportive response": "The speaker is providing a supportive comment about ImageJ, indicating a positive aspect of using it.",
                "offer feedback": "The speaker is providing feedback on how ImageJ can be used, specifically mentioning custom models."
            }
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "04:03-04:47",
            "transcript": "And and if and if you're writing software like we do to that we want other people to use, if you make it an ImageJ plugin, it's more likely to be used. And so a lot of this like like uh I don't like this um uh imaging snobbery that can pop up very quickly. Uh and it's not just imagers, you know other scientists, you know, biochemists can be snobs too with their techniques. But when it when it comes down to it, you know, a lot of great research is being done with a a 10x objective in the field.",
            "speaking_duration": 44,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "14:23",
            "end_time": "15:07",
            "annotations": {
                "develop idea": "The speaker is expanding on the idea of using ImageJ and its implications for research usability.",
                "critical response": "The speaker is criticizing the attitude of 'imaging snobbery' that can arise among researchers."
            }
        },
        {
            "speaker": "Domenico",
            "timestamp": "04:47-05:07",
            "transcript": "I I I'm I'm glad to hear that because I've got probably tens of thousands of lines of ImageJ macro code that that like my life depends on. And it's not elegant, but I mean it does what it needs to do. But again, like Crystal was saying, somebody else might have a different set of 10,000 lines and that leads into this different answer or subtly different answer and so I I I'd love to hear people chime in.",
            "speaking_duration": 20,
            "nods_others": 0,
            "smile_self": 0.2,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "15:07",
            "end_time": "15:27",
            "annotations": {
                "acknowledge contribution": "The speaker acknowledges Crystal's previous point and builds upon it.",
                "develop idea": "The speaker expands on the idea that variability in image analysis methods can lead to different results.",
                "encourage participation": "The speaker invites others to share their thoughts, encouraging participation in the discussion."
            }
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "05:07-05:10",
            "transcript": "So the the first thing is how do you segment out the data?",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "15:27",
            "end_time": "15:30",
            "annotations": {
                "ask question": "The speaker is requesting information on how to segment out the data, which is a clear question seeking clarification or expertise from other team members."
            }
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "05:10-06:10",
            "transcript": "Because that's really the first step of this is is how do you segment out and you don't have to go full AI and be all mathematical about it. Uh there are some very uh easy to use tools like elastic for example, where you can use machine learning so you're guiding the process. And the end result of that is you're guiding the the the computer to make make decisions, but the end result is a standard that and and and we typically use this. We in our lab is like someone threshold holds her images and they give the other person the actual elastic parameters and they can threshold their images exactly the same way. And it's easy because yeah, as I said already pointed out the the threshold has to be low enough so that people can actually access the technology.",
            "speaking_duration": 60,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "15:30",
            "end_time": "16:30",
            "annotations": {
                "propose new idea": "The speaker introduces a specific approach (using elastic for machine learning) to solve the problem of standardizing image analysis.",
                "develop idea": "The speaker elaborates on their approach, providing examples of how they use elastic parameters to standardize image analysis in their lab.",
                "offer feedback": "The speaker provides a suggestion for making the technology more accessible by lowering the threshold for usage."
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "06:10-06:10",
            "transcript": "Or if you really want to go crazy, put it into a docker image.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "16:30",
            "end_time": "16:30",
            "annotations": {
                "offer feedback": "Provide specific suggestions for improvement, modification of existing ideas or approaches proposed by other group members."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "06:11-06:27",
            "transcript": "Right, it seems to be more a reporting issue because I mean you you could upload your ImageJ scripts to GitHub and then reference them in your manuscripts or uh like create a video while you while you do it and just publish it somewhere so people could reproduce it exactly the same way.",
            "speaking_duration": 16,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "16:31",
            "end_time": "16:47",
            "annotations": {
                "propose new idea": "The speaker suggests uploading ImageJ scripts to GitHub and creating videos of the analysis process to improve reproducibility.",
                "develop idea": "The speaker elaborates on the idea of improving reproducibility through better sharing and reporting of image analysis methods.",
                "offer feedback": "The speaker provides specific suggestions for improving the reproducibility of image analysis."
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "06:29-07:14",
            "transcript": "Sorry, reproducible open science is a big um interest of mine. So yeah, I could go off on tangents on this, but there are and again, this is kind of where I come from, but there are technological answers to the issue of reproducibility, especially if it comes from a place of kind of tweaking knobs on ImageJ. Um there are a lot of scripting answers to that in particular, yeah, let's build out this docker image where I can just push a button and get the exact same results that you did. Um but that is obviously not an answer for everybody citing the aforementioned activation energy.",
            "speaking_duration": 45,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "16:49",
            "end_time": "17:34",
            "annotations": {
                "develop idea": "Shannon is expanding on the idea of reproducibility and technological solutions.",
                "supportive response": "Shannon is expressing a positive view towards using technological solutions for reproducibility.",
                "offer feedback": "Shannon is providing specific suggestions (e.g., using scripting, docker images) for improving reproducibility."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "07:14-07:41",
            "transcript": "So is is there really a challenging problem then because imaging is data as you know. Um and we see a lot of beautiful pictures every day. But the question is what do they really mean? And have you faced similar challenges and how do you deal with it and are there opportunities for us to solve big problems in that area.",
            "speaking_duration": 27,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "17:34",
            "end_time": "18:01",
            "annotations": {
                "ask question": "The speaker is requesting information or experiences from the group members.",
                "encourage participation": "The speaker is inviting group members to share their thoughts or experiences.",
                "clarify goal": "The speaker aims to understand the challenges and potential opportunities in the area of imaging data analysis."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "07:46-08:37",
            "transcript": "Feeling very stressed about that question because it goes back to the original question which is how did you do the experiment? What are you looking at and what are you using to capture these data because we are all doing different things and I'm realizing that across cell types, across systems, across, you know, mechanisms, like if you're using immunohistochemistry versus um you know, immunofluorescence versus a reporter, you're going to get a different answer even using the exact same system and processing the images the same way even if you're looking at the same protein and that's very stressful because then you get back to this issue of transparency and if I repeat your experiment with a slightly different method and then I get a different answer and the the numbers I get out of it are different then like what are we even doing here?",
            "speaking_duration": 51,
            "nods_others": 0,
            "smile_self": 0.1,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "18:06",
            "end_time": "18:57",
            "annotations": {
                "identify gap": "The speaker highlights the gap in standardization and transparency in image data analysis across different experiments and methods.",
                "clarify goal": "The speaker discusses the challenges in achieving comparable and meaningful data across different experiments, implicitly clarifying the goal of image data analysis."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "08:38-10:00",
            "transcript": "One thing I thought like you and I probably do the same thing when you're looking at IHC or you know, histochemistry right in a two-dimensional space and so you're going for depth through say like in my case it's going to be a limb, right? And if we took two limbs off the same animal and I hit it more lateral and you more medial, right? That signal can be very different. Independent of the fact that I might normalize against Daby or like cell count and you might normalize against area. But it seems like in that sort of something that would be predictive because we've tried you don't want to sacrifice the entire sample but to get a sampling across it because I'm sitting in these groups and trying to discuss like three-dimensional anything deeper than like, you know, 20 micrometers basically which is essentially a section becomes very difficult.",
            "speaking_duration": 82,
            "nods_others": 0,
            "smile_self": 0.1,
            "smile_other": 0.0,
            "distracted_others": 1,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "18:58",
            "end_time": "20:20",
            "annotations": {
                "identify gap": "Explicitly recognizing the lack of effective methods for 3D analysis beyond superficial layers.",
                "develop idea": "Expanding on existing ideas by providing specific examples from her work in histochemistry and IHC.",
                "supportive response": "Sharing her experiences and challenges which might be relatable and supportive to others in the discussion."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "00:00-00:33",
            "transcript": "signaling gradient is going to go away in one direction and get stronger in another. I um, of course I'm like, oh this seems so easy, but anybody who's in computer programming is like, look, that's going to take me half of my career, it's not interesting. But um, but you know, um, I think that would probably um level the playing field a little bit if you could see even just sacrificing a little bit of the sample to get on either side and doing it multiple times in one hit. Does that make sense? No. Everyone's blank stare says no, but um,",
            "speaking_duration": 33,
            "nods_others": 0,
            "smile_self": 25,
            "smile_other": 0,
            "distracted_others": 1,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "20:20",
            "end_time": "20:53",
            "annotations": {
                "identify gap": "The speaker explicitly discusses the challenges and limitations in analyzing signaling gradients across different directions, indicating a gap in current techniques.",
                "ask question": "The speaker asks for feedback or clarification on her idea, 'Does that make sense? No. Everyone's blank stare says no, but um,'"
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "00:33-00:37",
            "transcript": "Are you talking about individual sections or like Z stack sections or",
            "speaking_duration": 4,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "20:53",
            "end_time": "20:57",
            "annotations": {
                "ask question": "The speaker is seeking clarification on what type of sections are being referred to, indicating a request for information."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "00:38-00:50",
            "transcript": "Oh, individual sections. Like even if you're a parent, you're not even Z stacking it, but just being able to get a better feel quantitatively in a predictive way through space that way, right, HC.",
            "speaking_duration": 12,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 1,
            "hand_gesture": "None",
            "interuption": "Yes",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "20:58",
            "end_time": "21:10",
            "annotations": {
                "ask question": "The utterance seeks confirmation or agreement from others, indicated by 'right, HC.'",
                "develop idea": "Mimi Sammarco is expanding on a previous idea or discussion about imaging and analysis techniques."
            }
        },
        {
            "speaker": "Mimi Sammarco",
            "timestamp": "00:50-00:57",
            "transcript": "No. It could be in the phrasing of what I said.",
            "speaking_duration": 7,
            "nods_others": 0,
            "smile_self": 57,
            "smile_other": 0,
            "distracted_others": 1,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "21:10",
            "end_time": "21:17",
            "annotations": {
                "None": "The utterance does not explicitly fit into any of the provided codes as it is a minimal response with a meta-comment on communication."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "00:58-01:04",
            "transcript": "Well, so I think Dylan is like a 3D imaging expert essentially of animals. So he could probably speak to it.",
            "speaking_duration": 6,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "21:18",
            "end_time": "21:24",
            "annotations": {
                "signal expertise": "Crystal Rogers explicitly states Dylan's expertise in 3D imaging of animals.",
                "encourage participation": "By mentioning Dylan's expertise, Crystal encourages him to contribute to the discussion."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "01:04-01:07",
            "transcript": "You must be talking about Dylan McCreedy because I",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 33,
            "smile_other": 33,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "21:24",
            "end_time": "21:27",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "01:07-01:08",
            "transcript": "I am. I have no",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 100,
            "smile_other": 100,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "21:27",
            "end_time": "21:28",
            "annotations": {
                "None": "No relevant code applies to this utterance as it is too incomplete to determine a specific code."
            }
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "01:08-01:12",
            "transcript": "I don't know Dylan McCreedy.",
            "speaking_duration": 4,
            "nods_others": 0,
            "smile_self": 50,
            "smile_other": 50,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "21:28",
            "end_time": "21:32",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "01:08-01:10",
            "transcript": "I am. I have no and Dylan Burnette is",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 100,
            "smile_other": 100,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "Yes",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "21:28",
            "end_time": "21:30",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "01:12-01:30",
            "transcript": "a serious single cell expert. So you know, they both have these beautiful images at different resolutions. One is very 2D, but you can get in depth resolution within the cell, which is Dylan Burnette and then Dylan McCreedy is doing the system. So I'm obviously a fan of both of their work, so that you guys should talk about that.",
            "speaking_duration": 18,
            "nods_others": 0,
            "smile_self": 33,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "21:32",
            "end_time": "21:50",
            "annotations": {
                "Acknowledge contribution": "The speaker recognizes the contributions of Dylan Burnette and Dylan McCreedy by mentioning their work and expressing admiration for it.",
                "Supportive response": "The speaker is expressing a positive view of the work by suggesting that the researchers should talk to each other, implying value in their interaction.",
                "Encourage participation": "The speaker encourages interaction between Dylan Burnette and Dylan McCreedy by suggesting they should talk about their work."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "01:30-04:09",
            "transcript": "I I have to admit I don't completely understand the discussion right now, but I see uh there was a point made that there's a limit of reproducibility in the literature. Personally, I don't care much about that because I think if if you publish something and you study something and you find something uh and someone cannot reproduce it, you still found something. I mean it's a problem certainly that another lab cannot reproduce it, but I mean truth is not generated by one publication anyways. I mean someone will shed different light from a different angle on the same problem and then uh knowledge is generated over time. For example, in my field uh with the imaging technique that we use, uh the literature is completely heterogeneous. Some people find something, some people don't find it, they find the opposite thing. Uh and that's also because of course the patient cohorts are different. Um but I think if if we uh do something with our technique and we reproducibly find the same thing in different groups, uh then it is some sort of evidence. If someone else finds something different in a different cohort with a different group, then we maybe have to think about um what does that tell us? What does it really mean? Is it due to the different technique or is something else different? In MRI field what's really a problem and just to shed a different angle here, um is the specificity of the quantitative imaging. So what are we even measuring there? Because if if someone talks about quantitative MRI, one thing I've learned in the past 15 years, it's not specific. If someone tells you you can do myelin imaging, it's not only myelin. Uh if someone tells you they're measuring iron, it's not only iron. It's always affected by other things. And uh a problem and even if you if you read the read the literature naively, of times it's very hard to understand because I think many people don't understand that their technique is limited and uh if they do, they don't want to put the finger on it because the reviewers might not like it. So for me it's more the people need to be educated about what the limits are of their own techniques and the question for me would be how do we solve the specificity issue? So how do we really get techniques that are that are measuring what we want to measure and are not affected by other things. For example, we try to measure iron, but all the techniques many techniques that measure iron are also affected by myelin. So if you have a demyelinating disease and myelin changes, uh what are you even measuring there? I mean what are we even doing as Crystal said, I mean does it even make sense?",
            "speaking_duration": 279,
            "nods_others": 0,
            "smile_self": 11,
            "smile_other": 0,
            "distracted_others": 1,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "21:50",
            "end_time": "24:29",
            "annotations": {
                "develop idea": "The speaker elaborates on the challenges of reproducibility and specificity in quantitative imaging, developing the discussion on these topics.",
                "ask question": "The speaker asks for solutions to the specificity issue in quantitative imaging, inquiring about how to improve techniques.",
                "supportive response": "The speaker provides a thoughtful and encouraging contribution to the discussion, focusing on understanding and addressing challenges."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "04:09-04:12",
            "transcript": "I think that totally translates also to the down to the cell level.",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "24:29",
            "end_time": "24:32",
            "annotations": {
                "develop idea": "The speaker is expanding on an idea previously discussed, relating it to their own area of interest or perspective at the cellular level.",
                "supportive response": "The utterance expresses agreement or validation with a previous point."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "04:12-04:22",
            "transcript": "That's a really good point. What are you measuring and is it the same thing from sample to sample or in your case person to person.",
            "speaking_duration": 10,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "24:32",
            "end_time": "24:42",
            "annotations": {
                "ask question": "The speaker is explicitly asking for information or clarification on what is being measured and its consistency across samples or persons.",
                "identify gap": "The speaker's question implies a recognition of potential gaps or inconsistencies in measurement techniques or their interpretation across different samples or persons."
            }
        },
        {
            "speaker": "Domenico",
            "timestamp": "04:22-05:02",
            "transcript": "Yeah, I think that that's I agree that like the what you said Ferdinand at the beginning about is it are you doing qualitative are you doing quantitative analysis with a qualitative image. And I think in in the fluorescence field very it's it's very rare that somebody's going to actually optically calibrate their scope so that that pixel intensity maps to a photon count.",
            "speaking_duration": 40,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "24:42",
            "end_time": "25:22",
            "annotations": {
                "develop idea": "The speaker is expanding on Ferdinand's idea about the importance of distinguishing between qualitative and quantitative analysis, especially in the context of imaging.",
                "critical response": "The speaker is providing a negative evaluation of current practices in the fluorescence field, specifically highlighting the rarity of optical calibration of scopes."
            }
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "05:02-05:17",
            "transcript": "And there's so many caveats when people start talking about I'm like I'm counting molecules. Really? Are you really uh dealing with all the blinking that happens with every single floor for known to humankind? Like these sort of things become very, very contentious very quickly when people start saying I'm actually being quantitative with fluorescence.",
            "speaking_duration": 15,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "25:22",
            "end_time": "25:37",
            "annotations": {
                "critical response": "The speaker is questioning and challenging the validity of people's claims when they say they are quantitatively counting molecules with fluorescence, highlighting potential issues like blinking of fluorophores.",
                "ask question": "The speaker directly asks a question about dealing with blinking in fluorescence microscopy, seeking clarification or understanding of the challenges involved."
            }
        },
        {
            "speaker": "Domenico",
            "timestamp": "05:17-05:57",
            "transcript": "Yeah. What one one thing that we've been we've gotten into is is fluorescence lifetime and fluorescence lifetime is actually very stable and and and reproducible across different solvents. And so and that's a that's a single it's a photon counting technique. So it's photon counting but then the lifetime decay seems to be very, very reproducible across different areas. But the problem with it is then kind of like what Ferdinand's saying with the MRI,",
            "speaking_duration": 40,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "25:37",
            "end_time": "26:17",
            "annotations": {
                "develop idea": "Domenico (Nick) Galati expands on the idea of using fluorescence lifetime as a method, providing details about its stability and reproducibility.",
                "supportive response": "Domenico (Nick) Galati provides a positive evaluation of fluorescence lifetime as a technique."
            }
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "06:01-06:13",
            "transcript": "I totally I totally come down on the side of qualitative imaging quantitative measurements. That's that's that's that's been that's been our bread and butter for 20 years.",
            "speaking_duration": 12,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "26:21",
            "end_time": "26:33",
            "annotations": {
                "signal expertise": "The speaker is explicitly stating their experience and familiarity with qualitative imaging and quantitative measurements.",
                "supportive response": "The speaker is expressing agreement or validation for qualitative imaging and quantitative measurements."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "06:13-06:17",
            "transcript": "I think that's just that's just pragmatic.",
            "speaking_duration": 4,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "26:33",
            "end_time": "26:37",
            "annotations": {
                "supportive response": "The speaker is expressing agreement or validation with a previous statement, indicating a supportive response to the conversation."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "06:18-07:07",
            "transcript": "Okay. All right. Uh Ferdinand raised an issue about uh reproducibility, which is really critical in the literature today. Um how do you we address that? It's not critical for him, but for the journals, that's a critical part of publishing your data. Uh reproducibility is part of your grant proposals.",
            "speaking_duration": 49,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "26:38",
            "end_time": "27:27",
            "annotations": {
                "ask question": "The speaker is asking for opinions or solutions on addressing reproducibility.",
                "clarify goal": "The speaker clarifies the importance of reproducibility for journals and grant proposals, relating to defining objectives or expectations."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "07:07-07:17",
            "transcript": "I think I think there are multiple levels of reproducibility. I mean if you do an experiment, it should be reproducible.",
            "speaking_duration": 10,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "27:27",
            "end_time": "27:37",
            "annotations": {
                "develop idea": "The speaker is expanding on the concept of reproducibility, elaborating on its importance and implications in scientific experiments."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "07:17-07:18",
            "transcript": "That's actually a really good point.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "27:37",
            "end_time": "27:38",
            "annotations": {
                "supportive response": "The speaker is expressing agreement and validation with the previous speaker's contribution without adding new content."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "07:18-08:02",
            "transcript": "The open access to the way that things were. So I was in the bioimaging North America session and we you can talk, you can give everybody every piece of information about your analysis from the first step to the last, but if you don't tell them what objective you used, how much exposure time it had or this is in the case of light microscopy, but you know, if I'm sure there's very similar settings like you're talking about with the code for the MRI, if you don't give them the acquisition information, there's no way that can ever be repeated exactly.",
            "speaking_duration": 44,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "27:38",
            "end_time": "28:22",
            "annotations": {
                "clarify goal": "The speaker clarifies the goal of ensuring reproducibility in research by detailing the necessity of sharing comprehensive methodological information, such as objective used, exposure time, and acquisition settings."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "08:02-09:59",
            "transcript": "So I don't know about the equipment and you're producing, you know, exactly the experiment, but from an image analysis point of view, if your results are so dependent on the exact parameter that you chose to do your analysis, how robust is that result anyway, right? And so if you you're you're your result only works for one threshold value and one threshold only and you change the threshold slightly and you get a completely different result, should you really be publishing about that result? Um, um, I mean often, you know, you do a parameter sweep, you say, okay, here's a range of values and thresholds where this result is robust and there's a reason why it doesn't work at higher thresholds because I'm throwing away my signal or or whatever, right? Um, so it makes sense that it will not be, you know, uh reproducible.",
            "speaking_duration": 117,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 1,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "28:22",
            "end_time": "30:19",
            "annotations": {
                "ask question": "The utterance asks questions about the robustness of results and whether they should be published if highly dependent on specific parameters, seeking clarification and discussion on the matter."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "00:00-00:22",
            "transcript": "at this higher threshold value. So I mean I I I I I I think you know these these are valid concerns but at the same time I think they're concerns that can be to some extent um sort of um um softened by, you know, seeing how how robust is your result to the exact analysis or parameters that you're picking in your in your analysis.",
            "speaking_duration": 22,
            "nods_others": 2,
            "smile_self": 20,
            "smile_other": 30,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "30:20",
            "end_time": "30:42",
            "annotations": {
                "offer feedback": "The speaker is providing a suggestion on how to approach the issue of parameter variability, by considering the robustness of the results."
            }
        },
        {
            "speaker": "Dylan McCreedy",
            "timestamp": "00:22-01:45",
            "transcript": "So is one thing that I wanted to discuss is is how do we bridge kind of the gap between people that have the qualitative images and people that do that quantitative analysis. You know, is there a solution where you send your images in and you say this is the feature I'm interested in, you know, what data can be derived from those features. Um and then that way it sort of takes out sort of the the subjective sort of input into things. I'm just trying to think because there's been a lot of talk about activation energy and I I'd share that same issue where I think about deep learning and I have no idea where to start. And and so we can do some image analysis. We've repurposed uh tractography in a DTI based method for fluorescent intensity and so we can do tractography on fluorescent based images. So that's kind of one approach we've been able to take multimodality analysis and use it in different ways. But if you were to ask me to do deep learning, I would immediately go to somebody else and say, here are my images, what can you do with them? So how do we how do we sort of bridge that gap, reduce that activation energy while still trying to increase reproducibility.",
            "speaking_duration": 83,
            "nods_others": 3,
            "smile_self": 10,
            "smile_other": 10,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "30:42",
            "end_time": "32:05",
            "annotations": {
                "propose new idea": "The speaker proposes discussing how to bridge the gap between qualitative and quantitative image analysis.",
                "ask question": "The speaker asks for solutions or ideas on bridging the gap and reducing activation energy for deep learning.",
                "offer feedback": "The speaker suggests a potential approach by mentioning the repurposing of tractography in a DTI-based method for fluorescent intensity."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "01:45-01:57",
            "transcript": "Are there people who so you know how there are people now who no longer have wet labs and they literally just take data sets from single cell seek and they they do reanalysis and they sort of so there are people who do this for imaging, right?",
            "speaking_duration": 12,
            "nods_others": 1,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "32:05",
            "end_time": "32:17",
            "annotations": {
                "ask question": "The speaker is requesting information about the existence of people who reanalyze imaging datasets, similar to those in single-cell research."
            }
        },
        {
            "speaker": "Dylan McCreedy",
            "timestamp": "01:57-02:01",
            "transcript": "I think Shannon's raising his hand is one of those people.",
            "speaking_duration": 4,
            "nods_others": 0,
            "smile_self": 50,
            "smile_other": 50,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "32:17",
            "end_time": "32:21",
            "annotations": {
                "acknowledge contribution": "The utterance verbally recognizes another group member's input (Shannon Quinn's potential role in reanalyzing datasets).",
                "supportive response": "The utterance positively acknowledges Shannon Quinn's potential role without adding new content or disagreeing."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "02:01-02:03",
            "transcript": "Okay.",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 100,
            "smile_other": 100,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "32:21",
            "end_time": "32:23",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "02:04-05:16",
            "transcript": "Um so there is in open science a concept of um study pre-registration uh which is mostly known within the context of wet lab bench work where you sort of lay out all the steps of your protocol that and that's what's peer reviewed before you've ever done a single experiment. And this idea is starting to get adopted by the more computational community as well where it's like here are all of the steps that we're going to do before we've ever even done any of it and so not only does it take out researcher degrees of freedom that idea of, you know, parameter scanning um to figure out what works, um but it also in theory improves reproducibility to the point where like literally anybody could else could do this and tell you your results before you've even done it yourself. Um of course the problem with that is incentive uh is is one of incentive where you you as an academic gets get no reward for doing this other than the thrill of having your work be reproducible. Um and so there has been an effort to try to there are some entities that will publish the results of your pre-registration regardless of the outcome assuming that the pre-registration passes peer review. Um and that seems like that might be a pretty effective way at least on the computational end of helping out with reproducibility. Now once you get into things like deep learning models and you know, this kind of probabilistic training procedure, we are still kind of grasping for ideas because now you're talking about a model whose behavior is not entirely deterministic. Um and so simply pressing play um given, you know, a guy a series of steps is not by definition always going to give you the same result. Um and so that's kind of still an outstanding issue.",
            "speaking_duration": 192,
            "nods_others": 2,
            "smile_self": 10,
            "smile_other": 10,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "32:24",
            "end_time": "35:36",
            "annotations": {
                "develop idea": "The speaker elaborates on the concept of study pre-registration and its implications for reproducibility.",
                "identify gap": "The speaker highlights the challenge of achieving reproducibility with deep learning models.",
                "offer feedback": "The speaker suggests that pre-registration could be a solution to improve reproducibility in research."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "05:17-05:21",
            "transcript": "Yeah, we go back to image J.",
            "speaking_duration": 4,
            "nods_others": 0,
            "smile_self": 100,
            "smile_other": 100,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "35:37",
            "end_time": "35:41",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "05:22-05:26",
            "transcript": "Um but I think that's the way you get over the activation energy, right? As you look at the ecosystem that's currently in use and you ask how can I bring quantitative imaging or whatever platform it is that is needed to this workflow.",
            "speaking_duration": 4,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "35:42",
            "end_time": "35:46",
            "annotations": {
                "offer feedback": "The speaker provides a suggestion for how to overcome the challenge of activation energy.",
                "develop idea": "The speaker builds on previous discussion about challenges in quantitative imaging.",
                "supportive response": "The utterance is supportive in nature, suggesting a way forward."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "05:26-05:53",
            "transcript": "Absolutely. I think the super resolution field is a good example of this um you know because our data actually not even intensity based. Um it's points in space and um you know we've gone through this like um uh phase where we were um um trying to to convert our images into intensity based images um but there a lot of problems with that and you know like the quantification again was very dependent on how you converted it into an intensity based image etc.",
            "speaking_duration": 27,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "35:46",
            "end_time": "36:13",
            "annotations": {
                "identify gap": "The speaker explicitly recognizes a limitation in current super-resolution microscopy data analysis methods, specifically in handling data that is not intensity-based.",
                "develop idea": "The speaker elaborates on their experience with converting non-intensity-based data into intensity-based images for analysis, highlighting challenges in quantification."
            }
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "05:53-05:55",
            "transcript": "we can go ahead from you. Perfect time to jump in.",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "36:13",
            "end_time": "36:15",
            "annotations": {
                "encourage participation": "The speaker invites someone to contribute to the discussion.",
                "process management": "The speaker manages the discussion flow by inviting someone to speak."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "05:55-05:56",
            "transcript": "Yes, go ahead, Katie.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "36:15",
            "end_time": "36:16",
            "annotations": {
                "encourage participation": "The speaker is inviting Katie to contribute her thoughts or ideas, encouraging her participation in the discussion."
            }
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "05:56-06:00",
            "transcript": "Okay, uh so I was going to say um once you have these points.",
            "speaking_duration": 4,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "36:16",
            "end_time": "36:20",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Domenico",
            "timestamp": "06:00-06:28",
            "transcript": "I I I I think that's a really good idea and and I think with the image J stuff, I think one thing that ends up happening is that there's a few plugins that everybody uses. Like I think the colocalization plugin and image J is very well done. I think a lot of people tend like so that's just an example of one thing that I think some people use. But then there's the proliferation of like tweak plugins that take something that instead of funneling everybody through the same thing, then it starts proliferating laterally and that where that's where like I think a lot of the variability pops up. So I think it would be great if we could kind of funnel people through the most useful plugins and and not necessarily leave them static, still improve them, but then not branch off and go laterally. It seems like that's something that happens a lot.",
            "speaking_duration": 28,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "36:20",
            "end_time": "36:48",
            "annotations": {
                "propose new idea": "Domenico suggests a strategy for using ImageJ plugins more efficiently.",
                "develop idea": "He expands on the benefits and issues with the current plugin management approach.",
                "offer feedback": "Domenico provides feedback on how to improve the use of ImageJ plugins."
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "06:28-07:01",
            "transcript": "I I I feel like that touches on this first point of how do we get the most information out of an image or set of images is because it's going to be very application specific, right? And so you're describing this plugin and and at least this is my speculation but the reason for that lateral expansion is because everybody has a slightly different application which requires a slightly a slightly tweaked version of that plugin in order to get the most information they can out of those images. Um and yeah, I I don't have a good answer for that.",
            "speaking_duration": 33,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "36:48",
            "end_time": "37:21",
            "annotations": {
                "develop idea": "The speaker is elaborating on the challenges of image analysis and the specificity of applications.",
                "identify gap": "The speaker highlights the gap in current methods for image analysis, specifically the issue with plugin specificity and application."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "07:02-07:32",
            "transcript": "Absolutely. I think the super resolution field is a good example of this um you know because our data actually not even intensity based. Um it's points in space and um you know we've gone through this like um uh phase where we were um um trying to to convert our images into intensity based images um but there a lot of problems with that and you know like the quantification again was very dependent on how you converted it into an intensity based image etc.",
            "speaking_duration": 30,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "37:22",
            "end_time": "37:52",
            "annotations": {
                "develop idea": "The speaker is expanding on an existing idea by providing a specific example from the super-resolution field.",
                "identify gap": "The speaker is highlighting a challenge in image analysis regarding data conversion and quantification."
            }
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "07:32-07:35",
            "transcript": "Katie, we want to hear from you. Perfect time to jump in.",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "37:52",
            "end_time": "37:55",
            "annotations": {
                "encourage participation": "The speaker is inviting Katie to contribute to the conversation, encouraging her participation."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "07:37-07:37",
            "transcript": "Yes, go ahead, Katie.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "37:57",
            "end_time": "37:57",
            "annotations": {
                "encourage participation": "The speaker invites Katie to contribute her thoughts or ideas, encouraging participation."
            }
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "07:39-07:54",
            "transcript": "Okay, uh so I was going to say um once you have these points, can you tell us a little bit more about like what the quantitative aspect that follows is or for somebody who's like thresholding, what comes next in the process.",
            "speaking_duration": 15,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "37:59",
            "end_time": "38:14",
            "annotations": {
                "ask question": "Katy Keenan is requesting information about the quantitative aspect that follows having data points and what comes next in the process of thresholding, indicating she is seeking clarification or more information."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "07:55-08:53",
            "transcript": "Yeah, all of these are mainly for segmenting things things that are interesting. And so we have been trying as a field to sort of converge and you know we have now good ways like software and some of it is imageJ plugins where you go from the raw data which is your, you know, single molecule data for example to point data, right? So those single molecules are localized and you get your point um patterns. Uh but then going from those point patterns to something biologically meaningful and quantitate is is you know, so like it it's it's a wide open field and we don't have any kind of um, you know, um workflow or combined like algorithm that does everybody still has to write their own Python or whatever MATLAB code for it. Um and so yeah, I feel like there are some fields that are mature that maybe like there's a workflow, you apply it, everybody uses it and it works and then there are other fields where you have to um like tweak it and and and and and make it work for your own application.",
            "speaking_duration": 58,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "38:15",
            "end_time": "39:13",
            "annotations": {
                "develop idea": "The speaker is expanding on the current state of image analysis, discussing the process from raw data to biologically meaningful quantification.",
                "signal expertise": "The speaker demonstrates knowledge of image analysis techniques and challenges, specifically mentioning ImageJ plugins and the need for coding.",
                "identify gap": "The speaker highlights the lack of a standardized workflow or algorithm for quantifying biological images, noting that each field has to develop its own solutions."
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "08:53-09:04",
            "transcript": "Do you think that come just comes down to like standardizing protocols and normalizing data and now all of a sudden the same workflow will work for everybody? Or do you think it's more complicated than that?",
            "speaking_duration": 11,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "39:13",
            "end_time": "39:24",
            "annotations": {
                "ask question": "The utterance is a question seeking opinions on whether standardizing protocols and normalizing data would be sufficient to create a universally applicable workflow."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "09:05-09:32",
            "transcript": "I think it's more complicated than that because we're looking things that are very different. Some people are looking at tiny sort of clusters of receptors on the membrane. Other people are looking at complex like uh microtubular arrays and you know that that have very different um, you know, structure. And so how you analyze all that data is dependent on what your data actually looks like.",
            "speaking_duration": 27,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "39:25",
            "end_time": "39:52",
            "annotations": {
                "develop idea": "The speaker is expanding on the idea that analyzing imaging data is complicated due to the diversity of the data types.",
                "identify gap": "The speaker is highlighting a gap in the field, which is the lack of a universal approach to analyzing different types of imaging data."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "00:00-00:42",
            "transcript": "your image, right? So, for example, in the super resolution field, one thing we do with the point pattern analysis is use density based clustering or, you know, things that we borrowed from other fields. Um, Voronoi, um, tessellation, uh, to make Voronoi polygons around the points and say, okay, small polygons correspond to dense regions that actually have signal. And so I'm going to zoom into those regions, um, and and segment them out of background, um, information and then maybe interrogate their size, um, you know, um, do they form domains? How big are those domains, etc.",
            "speaking_duration": 42,
            "nods_others": 2,
            "smile_self": 10,
            "smile_other": 30,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "40:20",
            "end_time": "41:02",
            "annotations": {
                "develop idea": "The speaker is elaborating on existing ideas or methods in the field of super resolution, specifically how they use certain techniques for image analysis.",
                "signal expertise": "The speaker is sharing specific technical expertise in image analysis, detailing methods such as point pattern analysis and Voronoi tessellation."
            }
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "00:43-01:07",
            "transcript": "And would you say at this point that um like it's a physics informed or biology informed or uh like is there a model that's been proposed that you're applying and testing or um are these sort of um I would say like more data analysis, like looking for the the patterns. Um.",
            "speaking_duration": 24,
            "nods_others": 1,
            "smile_self": 12,
            "smile_other": 25,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "41:03",
            "end_time": "41:27",
            "annotations": {
                "ask question": "The speaker is requesting information or clarification on the methodology or approach used in the analysis."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "01:07-01:19",
            "transcript": "It's more the latter looking for the patterns in the data, right? And um trying to sort of make a biological sort of um interpretation of those patterns.",
            "speaking_duration": 12,
            "nods_others": 1,
            "smile_self": 33,
            "smile_other": 33,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "41:27",
            "end_time": "41:39",
            "annotations": {
                "develop idea": "The speaker is expanding on the idea of analyzing data by looking for patterns and interpreting them biologically."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "01:19-01:46",
            "transcript": "that is is the problem there in terms of reproducibility really the analysis of the final image or the acquisition of the image? Because yeah, you can set a slightly different threshold but you would probably still detect if something goes up or goes down, which is usually enough for the hypothesis.",
            "speaking_duration": 27,
            "nods_others": 1,
            "smile_self": 15,
            "smile_other": 30,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "41:39",
            "end_time": "42:06",
            "annotations": {
                "ask question": "The speaker asks a question about where the problem of reproducibility lies, in the analysis of the final image or the acquisition of the image.",
                "offer feedback": "The speaker provides a perspective on the reproducibility issue, discussing the impact of threshold settings on the detection of changes."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "01:46-01:55",
            "transcript": "I mean for confirming your hypothesis that whatever adding some chemical reduces the number of cells or what I mean you don't need to be quantitative there so much. a little bit but if you're 50% off compared to another lab it probably doesn't matter much in most applications.",
            "speaking_duration": 9,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "42:06",
            "end_time": "42:15",
            "annotations": {
                "develop idea": "The speaker is expanding on the idea of quantitative imaging by discussing its application in confirming hypotheses and the practical considerations of measurement variability across labs."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "01:56-02:34",
            "transcript": "Yeah, I mean, reproducibility is more, I mean, I, you know, I don't mean to sort of go back to the topic of reproducibility, but it was more like, you know, these are not, like, I can't just apply the image processing tools that exist for, you know, confocal images to my data. It just doesn't work because the data is not the same. So, um, the field has to sort of come up with new ideas about how do you, how do you extract information from this point data, um, that is not like pixel and intensity based.",
            "speaking_duration": 38,
            "nods_others": 0,
            "smile_self": 13,
            "smile_other": 13,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "42:16",
            "end_time": "42:54",
            "annotations": {
                "propose new idea": "The speaker suggests the need for new ideas to extract information from point data that is not pixel and intensity-based.",
                "identify gap": "The speaker identifies a gap in current methodologies, noting that existing image processing tools for confocal images are not applicable to their data."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "02:43-03:47",
            "transcript": "All right. That that sounds really exciting discussion about the how we can at least be able to um segment our data is critical steps and and how it can be retrieved down the line in terms of quantification. But then there is this huge problem of looking at different systems that different people are using. Um, and then you want to you have different special scales that you still want to be able to merge together. Um, do you see any challenges in that form of um collecting data at different time points with different systems, yet you want to match them into um a system that data set that's interpretable by different groups.",
            "speaking_duration": 64,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "43:03",
            "end_time": "44:07",
            "annotations": {
                "summarize conversation": "The speaker summarizes the discussion on data segmentation and its importance.",
                "identify gap": "The speaker highlights the challenge of integrating data from different systems and scales.",
                "ask question": "The speaker asks for opinions on the challenges of collecting and merging data from different systems and time points."
            }
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "03:48-04:29",
            "transcript": "Oh yeah, there was this comment that somebody yesterday had said that machine learning will take the place of image J. Uh, and I disagree with that. But when I read this question about like stitching and matching things, I do think there's some opportunities there. Um, for machine learning, uh, just to uh do feature extraction for comparison across some of these. Um, but I'm curious, I mean that that's kind of what we would turn to in our group if we were going to solve this, but I'm curious if other people have ideas or strategies where they would start if they want to cross modalities.",
            "speaking_duration": 41,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "44:08",
            "end_time": "44:49",
            "annotations": {
                "critical response": "The speaker explicitly disagrees with a previously mentioned idea that machine learning will take the place of ImageJ.",
                "ask question": "The speaker is seeking input from others on strategies for cross modalities.",
                "develop idea": "The speaker is expanding on her thoughts regarding machine learning and its applications."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "04:29-05:55",
            "transcript": "I don't know about crossing modalities, but to implement machine learning, um, my student is actively training different softwares to mirror the the in person like they're doing I have two groups counting or quantifying the same images, one teaching the machine like these different softwares elastic and then whatever else he's using. I don't know. I'm sorry. Cell profiler, some other things. Um, and then you have the students manually doing it using image day or other methods and I think that first, it's really necessary to sort of every lab would have to normalize these data within the lab within the same sample before crossing modalities. So I think that that's one of the the hard parts is that this um activation energy is really high even within a single lab to change mechanisms of quantitation and and and quantitative analysis and then to then be able to translate that. So if I'm looking at so this is actually something had had brought up is looking at um going from 2D to 3D, right? And and trying to figure out how you would take information from our 2D system and then sort of thinking about that within 3D and then how would you quantify that and analyze that. Um, even though you're looking at the same cells, same markers, whatever it is, that in and of itself is a really difficult thing to do.",
            "speaking_duration": 86,
            "nods_others": 0,
            "smile_self": 10,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "44:49",
            "end_time": "46:15",
            "annotations": {
                "develop idea": "Crystal is expanding on existing ideas by providing specific examples from her work, such as training software to mimic manual image analysis and the challenges of data normalization.",
                "signal expertise": "She is sharing her expertise based on her and her student's work, indicating her familiarity with image analysis techniques.",
                "identify gap": "She explicitly mentions the gap in current methods, particularly in normalizing data and transitioning from 2D to 3D imaging.",
                "offer feedback": "Crystal is providing feedback based on her experience, suggesting that labs need to normalize data internally before cross-modal analysis."
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "06:15-07:24",
            "transcript": "I have seen some really interesting but probably not science ready machine learning work where they're able to you're able to train a model that quite literally maps your image data from one modality to another. So like if your image data is recorded under fluorescence, here's what it would look like under differential image contrast interference contrast or you know, some other transformation. Um, and even though it's not anywhere near perfect, um, it does provide an interesting way forward at least in terms of how you would stitch together different modalities.",
            "speaking_duration": 69,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "46:35",
            "end_time": "47:44",
            "annotations": {
                "develop idea": "The speaker is developing an idea by providing a concrete example of how machine learning could be used in the context of image analysis to map data across different modalities."
            }
        },
        {
            "speaker": "Dylan McCreedy",
            "timestamp": "07:25-08:01",
            "transcript": "Yeah, I've seen uh labs that are collaborating with Google and they're taking fluorescent images of cells and then taking phase contrast and they've gotten to a point now where they can just give it phase contrast and it will predict the fluorescent labeling. They can then do the fluorescent labeling and compare them and there's a high fidelity between the two. And so it is yeah, I mean even then though you're you're still in an optical, you know, you're you're fairly similar modalities, but I do think it could extend beyond that as long as you have points of registration or some common map that you can put everything on.",
            "speaking_duration": 36,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "47:45",
            "end_time": "48:21",
            "annotations": {
                "develop idea": "Dylan McCreedy is explaining and providing an example of how an existing concept (machine learning) is being developed or applied in a specific context (translating phase contrast images to fluorescent labeling)."
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "08:02-08:29",
            "transcript": "Yeah, right now you would at least need a baseline of here's my I recorded this same data under these two different modalities and I'm going to train this model to learn how to tell the difference between them. Um, part of previous discussions is how we could get to that point without needing to generate those whole data sets on your own.",
            "speaking_duration": 27,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "48:22",
            "end_time": "48:49",
            "annotations": {
                "develop idea": "The speaker is expanding on previous ideas about the need for a baseline in training models for different imaging modalities.",
                "identify gap": "The speaker recognizes a gap in current methods, specifically the desire to avoid generating whole data sets on one's own."
            }
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "08:30-09:59",
            "transcript": "we still we still don't have a a a way of taking the same modality at different magnifications and comparing those data sets. So switching from DIC or phase to fluorescence and back. Now there are some limitations in there that I can go on for a while because we've been thinking about this for a while too and we're interested in in nanoscale kind of protein protein interactions at this point and phase isn't going to do that for us. Uh so there are there are some limitations but even if you are doing a a low mag image of the same of of obvious structure like say mitochondria or the ER at different magnifications, you have to use different algorithms to quantify those. So not just switching from modalities but the actual magnifications of the same modality are not standardized even in the same lab. So if if if if we're switching to a new modality before we fix the the magnification problem in the same modality. I think Crystal really laid out well that if you have the exact same equipment, you should be able to reproduce the data. But none of us have the exact same equipment. And so that only applies if I have the exact same camera you have. And then you tell me exactly what you did and I could probably get close to reproducing it. But I bought the camera that was the cheapest that did the what I wanted it to do. So I don't know some of you probably have my cheap camera too, but most of you have better cameras um for for for imaging and so we have this magnification problem before before we switch modalities. magnification is key.",
            "speaking_duration": 89,
            "nods_others": 0,
            "smile_self": 0,
            "smile_other": 0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "48:50",
            "end_time": "50:19",
            "annotations": {
                "identify gap": "The speaker identifies gaps in current methodologies for comparing data across different magnifications and modalities.",
                "develop idea": "The speaker is elaborating on existing ideas and challenges in the field of image analysis and reproducibility.",
                "critical response": "The speaker is discussing limitations and challenges in current practices, providing a critical perspective on the status quo."
            }
        },
        {
            "speaker": "Dylan Burnette",
            "timestamp": "00:00-00:11",
            "transcript": "has to be dealt with because that is one of the killers as far as lab to lab reproducibility. I take my 40x objective, I don't see my that thing you see with 100x objective, you must be wrong. This happens a lot.",
            "speaking_duration": 11,
            "nods_others": 1,
            "smile_self": 27.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Pointing",
            "interuption": "No",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "50:20",
            "end_time": "50:31",
            "annotations": {
                "identify gap": "The speaker explicitly recognizes a gap in current practices related to lab-to-lab reproducibility in microscopy, specifically concerning the use of different objectives.",
                "critical response": "The speaker provides a critical perspective on the current state of lab-to-lab reproducibility, emphasizing the issue of comparing images taken with different microscopy objectives."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "00:11-00:53",
            "transcript": "It's and I actually have after you're done, I have a quick addition to that. I was going to actually just address something Dylan Dylan mentioned and Ferdinand mentioned earlier that does it matter whether or not there's lab reproducibility and the answer is when we start thinking about the session I was just in, which is transitioning animal models to then human therapies, yes it matters, right? So you have to be able to see the same thing across models, across cell types that you expect to see to be able to ultimately get to where most of us are interested in getting is curing human disease or human disorders. And so I think that it does matter that we have to address that and I agree that it starts even within a lab, within a system.",
            "speaking_duration": 42,
            "nods_others": 1,
            "smile_self": 12.0,
            "smile_other": 12.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "50:31",
            "end_time": "51:13",
            "annotations": {
                "develop idea": "Crystal is expanding on the idea that reproducibility matters, especially for transitioning animal models to human therapies.",
                "clarify goal": "She is clarifying the goal of achieving reproducibility for the purpose of transitioning to human therapies.",
                "supportive response": "Crystal is expressing agreement with the importance of reproducibility in the context of her discussion."
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "00:54-01:29",
            "transcript": "Well, the the point the elaboration I was just going to make is even if you have the same magnification, if your numerical aperture is different, completely different data because I've done imaging with two different 10x objectives looking at axons and a high NA objective gives you beautiful axons, high signal to noise ratio and a low um NA objective results in blurring across it to the point where you almost don't even see the axons. And so literally same objective, two different numerical apertures has a uh you know, I'm sure super resolution is very susceptible to that.",
            "speaking_duration": 35,
            "nods_others": 1,
            "smile_self": 5.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "51:14",
            "end_time": "51:49",
            "annotations": {
                "develop idea": "Elaborating on how different numerical apertures affect imaging, building on previous discussions about imaging challenges.",
                "signal expertise": "Explicitly stating his experience and knowledge in imaging, specifically with different objectives and their effects on image quality.",
                "identify gap": "Highlighting a challenge in imaging related to the numerical aperture of objectives and its impact on data quality and comparability."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "01:29-01:32",
            "transcript": "So let me throw out an idea here.",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "51:49",
            "end_time": "51:52",
            "annotations": {
                "propose new idea": "The speaker is introducing a new idea or suggestion for discussion, as indicated by the phrase 'So let me throw out an idea here.'"
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "01:32-01:33",
            "transcript": "Oh sorry.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "Yes",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "51:52",
            "end_time": "51:53",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "01:35-02:58",
            "transcript": "So I was thinking about this because we in the MRI field we have the same problem that even if you have uh in the same side, you have scanners of different vendor, uh stuff is not especially quantitative MRI is not reproducible, nothing is reproducible virtually across sites or in very limited. So how about having a neural network that transforms? So you have images from one side say and from another side, uh everything's the same, maybe different cell cultures but same idea, same data basically. Uh so now you have a neural network that converts the data from one side to to tries to predict how the data acquired at site A would look like at site B if it were acquired at site B. And now you have uh it's basically what's it called? Is it a GAN? Uh it would be yeah it would be some kind like no it's not again, but you you have a discriminator, right? So you have a discriminator that now decides if uh something is a real image or not. So you basically uh train a neural network that can do this transformation between the two sides. And if at some point it works perfectly, you have a network that can transform the data really to the same type of artifacts and and same types of things and if it doesn't work perfectly, then you know that the information is just not there. So it's just not comparable data.",
            "speaking_duration": 83,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "51:55",
            "end_time": "53:18",
            "annotations": {
                "propose new idea": "Ferdinand proposes using a neural network to transform imaging data from one site to another to address the issue of non-reproducibility.",
                "develop idea": "He elaborates on the concept, suggesting the use of a discriminator to validate the transformed data.",
                "identify gap": "Ferdinand highlights the challenge of non-reproducibility of quantitative MRI data across different sites or vendors."
            }
        },
        {
            "speaker": "Domenico",
            "timestamp": "02:59-03:00",
            "transcript": "Can I can I jump in?",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "53:19",
            "end_time": "53:20",
            "annotations": {
                "process management": "The speaker is managing the meeting flow by asking to contribute to the conversation."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "03:00-03:00",
            "transcript": "Oh sorry.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "Yes",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "53:20",
            "end_time": "53:20",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Domenico",
            "timestamp": "03:00-03:57",
            "transcript": "Can I jump in really quickly? I I had very similar idea but I don't understand how it would work, but the idea is that like you have two different groups exactly. I was thinking the same thing where the we assume it's true that the biological specimen is the same. Like but like Mimi was saying, maybe this group slices a little bit differently, this group doesn't, this group uses a green dye, this group uses a red dye and and we assume though that they are the underlying truth, they are the same. And then we know point spread functions, we know we can we can map those pretty reliably and if we use point spread functions to make sure that we can then align those two and we can make them equivalent using something like a point spread function or something like that. I was a very similar idea where we have to assume that with all the variabilities that that are are true between these two groups, they're both mice, it's both the same section, the same region anatomical region and and yeah, I'm intrigued by that idea. I don't know how it works.",
            "speaking_duration": 57,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "53:20",
            "end_time": "54:17",
            "annotations": {
                "develop idea": "The speaker is expanding on an idea by suggesting a method involving point spread functions for data alignment.",
                "ask question": "The speaker is seeking clarification on how his idea would work."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "03:57-04:51",
            "transcript": "So so the thing is that the neural network would basically figure out the point spread function for you. Uh you don't have to encode it uh physically or mathematically. Uh but then there are also I mean it's possible with neural networks to have a discriminator that can really uh tell very well if uh an image that it sees is from a certain population of images. So if it sees an image generated or transformed from side A, it can tell you if it's uh if it may be from side B.",
            "speaking_duration": 54,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "54:17",
            "end_time": "55:11",
            "annotations": {
                "develop idea": "The speaker is expanding on how neural networks can be used to transform data from one site to another, making it comparable, by suggesting that the network could figure out the point spread function and potentially include a discriminator."
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "04:51-04:52",
            "transcript": "But then wouldn't you need to train?",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "55:11",
            "end_time": "55:12",
            "annotations": {
                "ask question": "The speaker is requesting information or clarification on the need for training in the proposed neural network approach."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "04:52-05:01",
            "transcript": "Yeah, you would basically have to get it yeah. yeah.",
            "speaking_duration": 9,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "55:12",
            "end_time": "55:21",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "05:01-05:02",
            "transcript": "Oh, I see. I see what you're saying. Okay.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "55:21",
            "end_time": "55:22",
            "annotations": {
                "Supportive response": "The speaker is expressing agreement and understanding of a previous point made by another participant."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "05:02-05:11",
            "transcript": "So if you want to reproduce an experiment. So if you do an experiment, you just upload your images and make them publicly available. If I do the same experiment to to reproduce it.",
            "speaking_duration": 9,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "55:22",
            "end_time": "55:31",
            "annotations": {
                "propose new idea": "The speaker introduces a new approach for enhancing experiment reproducibility by suggesting that researchers upload and make their images publicly available.",
                "develop idea": "The speaker elaborates on the idea by explaining that if someone does an experiment, they should upload their images and make them publicly available so others can reproduce it."
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "05:11-05:11",
            "transcript": "Oh, I see.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "Yes",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "55:31",
            "end_time": "55:31",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "05:11-05:27",
            "transcript": "I just have to take your images and put them together with mine and see if if they uh transform into uh each other or not. If they don't.",
            "speaking_duration": 16,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "55:31",
            "end_time": "55:47",
            "annotations": {
                "develop idea": "He is elaborating on a potential method for transforming images from one site to another."
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "05:27-05:27",
            "transcript": "I see what you're saying.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "55:47",
            "end_time": "55:47",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "05:27-05:35",
            "transcript": "So that comes back to the open source system you mentioned earlier on then.",
            "speaking_duration": 8,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "55:47",
            "end_time": "55:55",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "05:35-05:35",
            "transcript": "Yeah, yeah, yeah.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "55:55",
            "end_time": "55:55",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "05:35-05:35",
            "transcript": "That could be used as a standard.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "55:55",
            "end_time": "55:55",
            "annotations": {
                "None": "No relevant code perfectly applies to this utterance as it seems to be a suggestion or a comment on a previous idea rather than a clear example of any given code."
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "05:36-06:17",
            "transcript": "So we've got about seven minutes left, I think until the report out and um we've talked about so many different things that it would be really helpful if we could kind of maybe highlight three or four main areas and the main takeaways for that in the reporting. And so kind of one main area that I, you know, I think that we point out is is is reproducibility on the imaging side even feasible.",
            "speaking_duration": 41,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "55:56",
            "end_time": "56:37",
            "annotations": {
                "summarize conversation": "Dylan suggests summarizing their discussion into main areas and takeaways for reporting.",
                "process management": "Dylan is managing the meeting flow by suggesting they focus on key areas for the report."
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:17-06:17",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:37",
            "end_time": "56:37",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:17-06:27",
            "transcript": "You know, is is there we've mentioned different cameras, different objectives, different conditions, different systems.",
            "speaking_duration": 10,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:37",
            "end_time": "56:47",
            "annotations": {
                "ask question": "The speaker is asking a question about the existence of solutions or approaches to address the variations in imaging techniques.",
                "identify gap": "The speaker is highlighting the challenges or gaps in current imaging practices, specifically regarding reproducibility across different cameras, objectives, conditions, and systems."
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:27-06:27",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:47",
            "end_time": "56:47",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:27-06:30",
            "transcript": "You know, is reproducibility feasible.",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:47",
            "end_time": "56:50",
            "annotations": {
                "ask question": "The speaker is questioning the feasibility of reproducibility in imaging, seeking information or clarification on this topic."
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "06:30-06:30",
            "transcript": "[noise]",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "56:50",
            "end_time": "56:50",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "00:00-00:18",
            "transcript": "comparing CT to MRI and a person who's more knowledgeable about machine learning than I am suggested a cycle again as the structure so that you are feeding back and then you end up with four loss functions essentially.",
            "speaking_duration": 18,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "60:20",
            "end_time": "60:38",
            "annotations": {
                "develop idea": "The speaker is discussing and building upon an existing idea or suggestion made by someone else about using a cycle structure with feedback for comparing CT and MRI imaging techniques.",
                "clarify goal": "The speaker is also clarifying a concept or goal related to machine learning application in imaging."
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "00:20-00:23",
            "transcript": "Sorry, can you repeat that again?",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "60:40",
            "end_time": "60:43",
            "annotations": {
                "None": "The speaker is simply asking for a repetition of a previous statement, which does not fit into any of the provided codes."
            }
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "00:24-00:27",
            "transcript": "Yes, uh, so a cycle, I'll just put it in the chat.",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "60:44",
            "end_time": "60:47",
            "annotations": {
                "supportive response": "Katy Keenan is providing a cooperative response by suggesting she will put something in the chat, indicating a willingness to share information or contribute to the discussion."
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "00:27-00:29",
            "transcript": "Okay, perfect. Thank you.",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "60:47",
            "end_time": "60:49",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "00:31-00:39",
            "transcript": "Yeah, but we have to I don't see the screen update, but I I see there's neural networks for converting between sites, so we have it on the",
            "speaking_duration": 8,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "60:51",
            "end_time": "60:59",
            "annotations": {
                "supportive response": "The speaker is expressing agreement or acknowledgment of a previous point regarding the use of neural networks for converting between sites."
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "00:40-00:44",
            "transcript": "Uh, you see this right here. I was just going to copy and paste that.",
            "speaking_duration": 4,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "Yes",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "61:00",
            "end_time": "61:04",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "00:44-00:46",
            "transcript": "Is it not updating at all?",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "Yes",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "61:04",
            "end_time": "61:06",
            "annotations": {
                "ask question": "The speaker is requesting information or clarification about a technical issue with the screen update."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "00:46-00:46",
            "transcript": "No, I don't see anything.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "61:06",
            "end_time": "61:06",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Unknown speaker",
            "timestamp": "00:47-00:47",
            "transcript": "I don't see anything new.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "61:07",
            "end_time": "61:07",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Unknown speaker",
            "timestamp": "00:48-00:50",
            "transcript": "on your word doc or",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "61:08",
            "end_time": "61:10",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "00:50-00:52",
            "transcript": "Oh, one second. Okay, there we go.",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "61:10",
            "end_time": "61:12",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "00:54-01:27",
            "transcript": "So in bold is kind of what I was planning on putting. So I have conversion neural network to compare between sites and you know, under kind of the point is is reproducibility and image acquisition feasible. And so that kind of, you know, an answer to that would be conversion or neural network between sites. So if you're looking at, you know, similar sets of data that are collected slightly different ways, can you convert them between the sites and that would show you if you're getting the same features of the data. Is that kind of touch upon what you were thinking?",
            "speaking_duration": 33,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "61:14",
            "end_time": "61:47",
            "annotations": {
                "propose new idea": "Dylan suggests using a conversion neural network to compare data between sites.",
                "develop idea": "He elaborates on how this approach could work for assessing reproducibility.",
                "ask question": "Dylan seeks confirmation if his suggestion aligns with others' thoughts."
            }
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "01:28-01:28",
            "transcript": "That's good.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "61:48",
            "end_time": "61:48",
            "annotations": {
                "supportive response": "The speaker is expressing agreement or validation with a previous statement."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "01:29-01:30",
            "transcript": "Yeah, I think so, yeah.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "61:49",
            "end_time": "61:50",
            "annotations": {
                "supportive response": "The speaker is expressing agreement with a previous statement."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "01:31-01:35",
            "transcript": "Between sites and modalities, uh, yeah, experiments could be everything.",
            "speaking_duration": 4,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "61:51",
            "end_time": "61:55",
            "annotations": {
                "supportive response": "The speaker is expressing agreement or validation of the discussion on experiments across sites and modalities."
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "01:36-01:40",
            "transcript": "Yeah, and so that I was going to kind of make a separate point for the modalities down here.",
            "speaking_duration": 4,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "61:56",
            "end_time": "62:00",
            "annotations": {
                "None": "The utterance is a transitional phrase and does not explicitly fit into any of the provided categories."
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "01:41-01:49",
            "transcript": "And then build on what Katy was saying, I need to look in the chat, um, you know, basically can you",
            "speaking_duration": 8,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "62:01",
            "end_time": "62:09",
            "annotations": {
                "ask question": "Dylan McCreedy is requesting information or clarification from others, indicating an 'ask question' behavior.",
                "acknowledge contribution": "Dylan McCreedy acknowledges Katy's previous contribution, showing awareness and respect for her input."
            }
        },
        {
            "speaker": "Katy Keenan",
            "timestamp": "01:50-01:51",
            "transcript": "Uh",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "62:10",
            "end_time": "62:11",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "01:52-01:56",
            "transcript": "And then is there a fourth point that we want to make, um",
            "speaking_duration": 4,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "62:12",
            "end_time": "62:16",
            "annotations": {
                "ask question": "The speaker is asking if there is a fourth point to be made.",
                "process management": "The speaker is managing the discussion flow by questioning what points to include."
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "01:56-01:59",
            "transcript": "Oh crap.",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "62:16",
            "end_time": "62:19",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "02:00-02:03",
            "transcript": "Oh, we are ready to get out of.",
            "speaking_duration": 3,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "62:20",
            "end_time": "62:23",
            "annotations": {
                "process management": "The speaker is managing the meeting flow, suggesting it's time to conclude their discussion."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "02:03-02:12",
            "transcript": "Not muted. I do think it's important to note that it's important that we need to know what we're looking for or what we're what are we putting in and what do we expect to get out.",
            "speaking_duration": 9,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "62:23",
            "end_time": "62:32",
            "annotations": {
                "clarify goal": "The speaker emphasizes the importance of knowing what they are looking for and what they expect to get out, which relates to defining objectives and expectations for their analysis or discussion."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "02:12-02:21",
            "transcript": "Um, and make sure that if we're going to be thinking about comparing data across whatever that we're putting in the same thing and getting trying to get the same thing out.",
            "speaking_duration": 9,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "Open Palms",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "62:32",
            "end_time": "62:41",
            "annotations": {
                "clarify goal": "Crystal Rogers is emphasizing the need for clarity in goals when comparing data, specifically that the inputs and outputs must be equivalent for valid comparisons."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "02:21-02:26",
            "transcript": "So there's probably a better way to say that, but input output.",
            "speaking_duration": 5,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "62:41",
            "end_time": "62:46",
            "annotations": {
                "None": "No relevant code directly applies to this utterance as it is more of a reflective comment than a contribution to the discussion that fits neatly into the provided categories."
            }
        },
        {
            "speaker": "Dylan McCreedy | Texas A&M",
            "timestamp": "02:38-02:39",
            "transcript": "Does that work?",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "62:58",
            "end_time": "62:59",
            "annotations": {
                "ask question": "The speaker is seeking confirmation or agreement on a proposal or idea previously discussed."
            }
        },
        {
            "speaker": "Crystal Rogers",
            "timestamp": "02:40-02:41",
            "transcript": "Sure.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "63:00",
            "end_time": "63:01",
            "annotations": {
                "supportive response": "The utterance 'Sure.' expresses agreement or confirmation with a previous statement without adding new content."
            }
        },
        {
            "speaker": "Unknown speaker",
            "timestamp": "02:44-02:45",
            "transcript": "That's great.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "63:04",
            "end_time": "63:05",
            "annotations": {
                "supportive response": "The speaker is expressing agreement or approval with a previous statement."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "02:47-02:57",
            "transcript": "Thank you, Dylan for putting this together. This is really good discussion and we look forward to continuing the discussion later.",
            "speaking_duration": 10,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "Yes",
            "screenshare_content": "A word document is being shared. The document contains notes and ideas related to image analysis and machine learning. The content is not being actively edited during this segment.",
            "start_time": "63:07",
            "end_time": "63:17",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "00:00-00:02",
            "transcript": "level from Washio.",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "62:57",
            "end_time": "62:59",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "00:07-00:45",
            "transcript": "All right. Um I don't know if there are more facilitators here. All right, maybe we kick off um can you see the program? Might just I just lost it.",
            "speaking_duration": 38,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "63:04",
            "end_time": "63:42",
            "annotations": {
                "process management": "The speaker suggests moving forward with the meeting or a task, indicating an attempt to manage the meeting flow.",
                "ask question": "The speaker requests information about the presence of other facilitators and the visibility of the program."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "00:38-00:40",
            "transcript": "Can you all see the program?",
            "speaking_duration": 2,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "63:35",
            "end_time": "63:37",
            "annotations": {
                "ask question": "The speaker is requesting information or confirmation from the group about whether they can see the program."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "00:41-01:21",
            "transcript": "Okay. All right. Um let's start by first of all introducing ourselves briefly. Um I'm Sam Achilefu, Washington University in St. Louis. I work in the area of molecular imaging uh especially in cancer imaging. I also um look at interventional processes such as um image guided surgery um um imaging of all sorts, okay?",
            "speaking_duration": 40,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "63:38",
            "end_time": "64:18",
            "annotations": {
                "signal expertise": "The speaker explicitly states their own expertise and work area.",
                "encourage participation": "The speaker invites others to introduce themselves, encouraging participation.",
                "process management": "The speaker is managing the meeting flow by suggesting that they introduce themselves."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "01:21-01:22",
            "transcript": "I'm",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "64:18",
            "end_time": "64:19",
            "annotations": {
                "None": "No relevant code applies to this utterance as it appears to be incomplete and not fully expressing a codable thought."
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "01:22-01:43",
            "transcript": "I'm Shannon Quinn. I'm in uh departments of computer science and cell biology at University of Georgia. Um and I use computer vision and machine learning to build models uh of cellular systems, studying their change in both spatial and temporal dimensions.",
            "speaking_duration": 21,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "64:19",
            "end_time": "64:40",
            "annotations": {
                "signal expertise": "The speaker explicitly states their own expertise or qualifications related to the task by mentioning their departments, university, and areas of work."
            }
        },
        {
            "speaker": "Samuel Achilefu",
            "timestamp": "01:46-01:47",
            "transcript": "You can call on the next person.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "64:43",
            "end_time": "64:44",
            "annotations": {
                "encourage participation": "The speaker is inviting another team member to introduce themselves, thereby encouraging participation.",
                "process management": "The speaker is managing the meeting flow by ensuring introductions proceed in an orderly manner."
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "01:51-01:56",
            "transcript": "Uh looks like uh Melike, am I spot pronouncing your name correctly?",
            "speaking_duration": 5,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "64:48",
            "end_time": "64:53",
            "annotations": {
                "ask question": "The speaker is requesting information about the pronunciation of another participant's name, making this an explicit question."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "01:56-01:57",
            "transcript": "Melika, correct.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "64:53",
            "end_time": "64:54",
            "annotations": {
                "None": "No relevant code applies to this utterance"
            }
        },
        {
            "speaker": "Shannon Quinn",
            "timestamp": "01:57-01:57",
            "transcript": "Melika?",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "64:54",
            "end_time": "64:54",
            "annotations": {
                "ask question": "The speaker is requesting information or clarification on the pronunciation of a name."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "01:57-01:58",
            "transcript": "Yeah.",
            "speaking_duration": 1,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "64:54",
            "end_time": "64:55",
            "annotations": {
                "None": "No relevant code applies to this utterance as it is a minimal response."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "02:00-02:36",
            "transcript": "Hi. I'm Melika. I am um um at the University of Pennsylvania. Uh my lab uses single molecule based approaches and super resolution microscopy um to visualize um spatial and temporal organization of cells. Um one uh particular question of interest we have is how genome folding in 3D space influences gene activity and how genome may be misfolded in disease.",
            "speaking_duration": 36,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "64:57",
            "end_time": "65:33",
            "annotations": {
                "signal expertise": "The speaker explicitly states their background and area of expertise related to the task."
            }
        },
        {
            "speaker": "Melike Lakadamyali",
            "timestamp": "02:36-02:36",
            "transcript": "Um um and and that's it. Yeah. And the next person is uh Ferdinand.",
            "speaking_duration": 0,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "65:33",
            "end_time": "65:33",
            "annotations": {
                "process management": "The speaker is managing the flow of introductions in the meeting.",
                "encourage participation": "The speaker is inviting the next person to introduce themselves."
            }
        },
        {
            "speaker": "Ferdinand Schweser",
            "timestamp": "02:37-02:58",
            "transcript": "Uh yeah. I'm Ferdinand, I work at the University at Buffalo uh in the Department of Neurology but my training is in physics and uh um I'm pretty much doing most of the time computational imaging um and uh what we do is we develop um quantitative MRI techniques based on physical models.",
            "speaking_duration": 21,
            "nods_others": 0,
            "smile_self": 0.0,
            "smile_other": 0.0,
            "distracted_others": 0,
            "hand_gesture": "None",
            "interuption": "No",
            "overlap": "No",
            "screenshare": "No",
            "screenshare_content": "None",
            "start_time": "65:34",
            "end_time": "65:55",
            "annotations": {
                "signal expertise": "Ferdinand explicitly states his own expertise in physics and his work in computational imaging and quantitative MRI techniques."
            }
        }
    ]
}